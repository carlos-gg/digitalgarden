<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AIs on</title><link>https://carlgogo.github.io/AIDigitalGarden/ai/</link><description>Recent content in AIs on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://carlgogo.github.io/AIDigitalGarden/ai/index.xml" rel="self" type="application/rss+xml"/><item><title/><link>https://carlgogo.github.io/AIDigitalGarden/AI/Bayesian-modelling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Bayesian-modelling/</guid><description>Bayesian modelling See [[Bayesian neural networks]]
https://en.wikipedia.org/wiki/Bayesian_statistics https://en.wikipedia.org/wiki/Bayesian_inference http://brohrer.github.io/how_bayesian_inference_works.html http://willwolf.io/en/2017/02/07/bayesian-inference-via-simulated-annealing/ Bayesian vs frequentist discussion:
http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/ http://www.fharrell.com/2017/02/my-journey-from-frequentist-to-bayesian.html https://mchankins.wordpress.com/2013/04/21/still-not-significant-2/ https://aeon.</description></item><item><title/><link>https://carlgogo.github.io/AIDigitalGarden/AI/Data-Science-Data-Engineering/Dask/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Data-Science-Data-Engineering/Dask/</guid><description>Dask #CODE Dask - flexible parallel computing library for analytics: https://github.com/dask/dask
http://docs.dask.org/en/latest/cheatsheet.html #CODE Dask-Jobqueue - Easily deploy Dask on job queuing systems like PBS, Slurm, MOAB, SGE, LSF, and HTCondor: https://github.</description></item><item><title/><link>https://carlgogo.github.io/AIDigitalGarden/AI/Data-Science-Data-Engineering/Distributed-DL/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Data-Science-Data-Engineering/Distributed-DL/</guid><description>Distributed Deep learning https://d2l.ai/chapter_computational-performance/multiple-gpus.html
https://jhui.github.io/2017/03/07/TensorFlow-GPU/
https://www.logicalclocks.com/blog/goodbye-horovod-hello-collectiveallreduce
Twelve ways to fool the masses when reporting performance of deep learning workloads: https://htor.</description></item><item><title/><link>https://carlgogo.github.io/AIDigitalGarden/AI/Data-Science-Data-Engineering/Horovod/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Data-Science-Data-Engineering/Horovod/</guid><description>Horovod #PAPER Horovod: fast and easy distributed deep learning in TensorFlow (Sergeev 2018): http://arxiv.org/abs/1802.05799 #CODE https://github.com/horovod/horovod https://horovod.readthedocs.io/en/latest/keras.html https://horovod.readthedocs.io/en/stable/tensorflow.html https://eng.</description></item><item><title/><link>https://carlgogo.github.io/AIDigitalGarden/AI/Data-Science-Data-Engineering/MLops/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Data-Science-Data-Engineering/MLops/</guid><description>ML ops Experiment tracking https://neptune.ai/blog/best-ml-experiment-tracking-tools Weights and biases: https://wandb.ai/site #CODE Aim: https://github.com/aimhubio/aim The open-source tool for ML experiment comparison https://aimstack.</description></item><item><title/><link>https://carlgogo.github.io/AIDigitalGarden/AI/Data-Science-Data-Engineering/Tensorflow-keras/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Data-Science-Data-Engineering/Tensorflow-keras/</guid><description>Tensorflow, Keras #CODE Tensorflow (Google): https://github.com/tensorflow/tensorflow http://playground.tensorflow.org/ https://cloud.google.com/blog/big-data/2016/07/understanding-neural-networks-with-tensorflow-playground https://blog.metaflow.fr/tensorflow-how-to-freeze-a-model-and-serve-it-with-a-python-api-d4f3596b3adc#.dg41ldof5 https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/#0 Tensorflow 2.0: models migration and new design: https://pgaleone.eu/tensorflow/gan/2018/11/04/tensorflow-2-models-migration-and-new-design/ http://planspace.</description></item><item><title/><link>https://carlgogo.github.io/AIDigitalGarden/AI/Data-Science-Data-Engineering/Visualization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Data-Science-Data-Engineering/Visualization/</guid><description>Visualization https://github.com/fasouto/awesome-dataviz http://keshif.me/demo/VisTools The visualization universe http://visualizationuniverse.com/ http://visualizationuniverse.com/charts/ Dataviz project: https://datavizproject.com/ UW Interactive Data Lab: http://idl.cs.washington.edu/ Flowing Data Tutorials: https://flowingdata.</description></item><item><title/><link>https://carlgogo.github.io/AIDigitalGarden/AI/Data-Science-Data-Engineering/Xarray/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Data-Science-Data-Engineering/Xarray/</guid><description>Xarray #code Xarray - N-D labeled arrays and datasets in Python: https://github.com/pydata/xarray #PAPER Xarray - N-D labeled Arrays and Datasets in Python (Hoyer 2017): https://openresearchsoftware.</description></item><item><title/><link>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/Bayesian-neural-networks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/Bayesian-neural-networks/</guid><description>Bayesian Neural Networks - BNNs Bayesian Neural Network tutorial: http://edwardlib.org/tutorials/bayesian-neural-network Bayesian Deep Learning - NeurIPS Workshop: http://bayesiandeeplearning.org/ Deep Learning Is Not Good Enough, We Need Bayesian Deep Learning for Safe [[AI]].</description></item><item><title/><link>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/GNNs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/GNNs/</guid><description>Graph neural networks (GNNs) Graph Neural networks (GNNs) are being widely adopted for diverse applications and domains. This is in part due to their effectiveness on complex data structures, improved performance and scalability, and availability of approaches A Gentle Introduction to Graph Neural Networks: https://distill.</description></item><item><title/><link>https://carlgogo.github.io/AIDigitalGarden/AI/Math-and-stats/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Math-and-stats/</guid><description>Maths and Statistics Statistics cheatsheet: https://stanford.edu/~shervine/teaching/cme-106/cheatsheet-statistics https://github.com/rouseguy/intro2stats Stanford-cs-229 ML, probability and stats refresher: https://github.com/afshinea/stanford-cs-229-machine-learning/blob/master/en/refresher-probabilities-statistics.pdf https://www.khanacademy.org/math/statistics-probability http://christopherroach.com/articles/statistics-for-hackers/ http://students.brown.edu/seeing-theory/ https://scipy-latinamerica.github.io/revista.io/blog/2018/10/22/probabilidad-y-estadistica-con-python/ Trigonometry refresher: https://stanford.</description></item><item><title/><link>https://carlgogo.github.io/AIDigitalGarden/AI/Supervised-Learning/Gaussian-Process/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Supervised-Learning/Gaussian-Process/</guid><description>Gaussian Process https://en.wikipedia.org/wiki/Gaussian_process In probability theory and statistics, a Gaussian process is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution, i.</description></item><item><title/><link>https://carlgogo.github.io/AIDigitalGarden/AI/Supervised-Learning/Gradient-boosting/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Supervised-Learning/Gradient-boosting/</guid><description>Gradient boosting See [[Ensemble learning]]
Outperforms Random Forests and AdaBoost. RF is easier to tune and less prone to overfitting http://arogozhnikov.</description></item><item><title/><link>https://carlgogo.github.io/AIDigitalGarden/AI/Supervised-Learning/Random-forest/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Supervised-Learning/Random-forest/</guid><description>Random forest See [[Ensemble learning]]
https://github.com/kjw0612/awesome-random-forest
https://sebastianraschka.com/faq/docs/bagging-boosting-rf.html
Bagging and random forests are “bagging” algorithms that aim to reduce the complexity of models that overfit the training data.</description></item><item><title/><link>https://carlgogo.github.io/AIDigitalGarden/AI/Unsupervised-learning/Clustering/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Unsupervised-learning/Clustering/</guid><description>Clustering Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters).</description></item><item><title>Active learning</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Active-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Active-learning/</guid><description>https://en.wikipedia.org/wiki/Active_learning_(machine_learning) Active learning refers to algorithms that take an active role in the selection of which ex-amples are labeled. Active learning assumes that there is an ‘oracle’, such as a human expert, that can be queried to get ground-truth labels for selected unlabeled instances.</description></item><item><title>Anomaly and Outlier Detection</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Anomaly-and-Outlier-Detection/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Anomaly-and-Outlier-Detection/</guid><description>See Active learning for anomaly discovery
Resources Most of the outlier detection approaches belong to [[Unsupervised learning]] although it might be framed as a [[Semi-supervised learning]] problem.</description></item><item><title>Artificial Intelligence</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/AI/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/AI/</guid><description>Resources The expression artificial intelligence is an umbrella term encompassing a suite of technologies that can perform complex tasks when acting in conditions of uncertainty, including visual perception, speech recognition, natural language processing, reasoning, learning from data, and a range of optimisation problems.</description></item><item><title>Autoencoders</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/Autoencoders/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/Autoencoders/</guid><description>Dimensionality Reduction: https://www.cs.toronto.edu/~hinton/science.pdf
The classical approach for unsupervised learning using neural networks. The basic version consists of a Multilayer Perceptron (MLP) where the input and output layer have the same size and a smaller hidden layer is trained to recover the input.</description></item><item><title>Automated planning</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Automated-planning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Automated-planning/</guid><description>AI Planning is a branch of artificial intelligence that concerns the realization of strategies or action sequences, typically for execution by intelligent agents, autonomous robots and unmanned vehicles.</description></item><item><title>AutoML</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/AutoML/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/AutoML/</guid><description>See [[Model selection and tuning]]
https://en.wikipedia.org/wiki/Automated_machine_learning
Automated machine learning (AutoML) is the process of automating the process of applying machine learning to real-world problems.</description></item><item><title>Background subtraction</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Computer-Vision/Background-subtraction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Computer-Vision/Background-subtraction/</guid><description>https://en.wikipedia.org/wiki/Foreground_detection
https://github.com/murari023/awesome-background-subtraction
Foreground detection is one of the major tasks in the field of computer vision and image processing whose aim is to detect changes in image sequences.</description></item><item><title>Capsule Neural networks (CapsNets)</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/CapsNets/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/CapsNets/</guid><description>https://en.wikipedia.org/wiki/Capsule_neural_network
A Capsule Neural Network (CapsNet) is a machine learning system that is a type of artificial neural network (ANN) that can be used to better model hierarchical relationships.</description></item><item><title>Causality</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Causality/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Causality/</guid><description>To Build Truly Intelligent Machines, Teach Them Cause and Effect: https://www.quantamagazine.org/to-build-truly-intelligent-machines-teach-them-cause-and-effect-20180515/
Representing uncertain knowledge: https://glouppe.github.io/info8006-introduction-to-ai/?p=lecture5.md
Reasoning over time: https://glouppe.</description></item><item><title>Class imbalance</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Supervised-Learning/Class-imbalance/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Supervised-Learning/Class-imbalance/</guid><description>https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis http://www.marcoaltini.com/blog/dealing-with-imbalanced-data-undersampling-oversampling-and-proper-cross-validation http://www.alfredo.motta.name/cross-validation-done-wrong/ http://www.win-vector.com/blog/2015/02/does-balancing-classes-improve-classifier-performance/ http://www.chioka.in/class-imbalance-problem/ http://www.kdnuggets.com/2017/06/7-techniques-handle-imbalanced-data.html https://svds.com/learning-imbalanced-classes/ Conventional algorithms are often biased towards the majority class because their loss functions attempt to optimize quantities such as error rate, not taking the data distribution into consideration.</description></item><item><title>Classification</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Supervised-Learning/Classification/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Supervised-Learning/Classification/</guid><description>https://github.com/jmportilla/Udemy---Machine-Learning/blob/master/Multi-Class%20Classification.ipynb Comparison of classifiers https://www.quora.com/What-are-the-advantages-of-different-classification-algorithms http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html https://medium.com/@maheshkkumar/implementing-a-binary-classifier-in-python-b69d08d8da21#.goynlh7ah Metrics https://www.neuraldesigner.com/blog/methods-binary-classification http://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/ http://docs.aws.amazon.com/machine-learning/latest/dg/binary-model-insights.html Naive Bayes https://blancosilva.</description></item><item><title>Computer Vision</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Computer-Vision/Computer-vision/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Computer-Vision/Computer-vision/</guid><description>See [[CNNs]] [[MLPs#MLPs for vision and language]] [[Transformers#For Computer Vision]] [[Generative modelling#Generative models for Image data]] [[GANs]]
https://github.com/jbhuang0604/awesome-computer-vision Papers with code - computer vision: https://paperswithcode.</description></item><item><title>Convolutional Neural Networks (CNNs)</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/CNNs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/CNNs/</guid><description>https://github.com/kjw0612/awesome-deep-vision https://en.wikipedia.org/wiki/Convolutional_neural_network In [[deep learning]], a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery.</description></item><item><title>Data Engineering and Computer Science</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Data-Science-Data-Engineering/Data-engineering-and-computer-science/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Data-Science-Data-Engineering/Data-engineering-and-computer-science/</guid><description>https://github.com/ossu/computer-science Data engineering role is ensuring uninterrupted flow of data between servers and applications. https://www.datacamp.com/community/blog/data-engineering-vs-data-science-infographic#gs.pvMeguY Interaction between ML and CS teams: https://labs.</description></item><item><title>Data Science</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Data-Science-Data-Engineering/Data-Science/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Data-Science-Data-Engineering/Data-Science/</guid><description>Resources https://en.wikipedia.org/wiki/Data_science https://github.com/bulutyazilim/awesome-datascience A curated collection of useful resources for Data science ,Python and R: https://datapyr.zeef.com/ Reproducible Data Analysis in Jupyter (Vanderplas): https://jakevdp.</description></item><item><title>Deep belief network</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/Deep-belief-network/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/Deep-belief-network/</guid><description>In machine learning, a deep belief network (DBN) is a generative graphical model, or alternatively a type of deep neural network, composed of multiple layers of latent variables(&amp;ldquo;hidden units&amp;rdquo;), with connections between the layers but not between units within each layer.</description></item><item><title>Deep Learning</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/Deep-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/Deep-learning/</guid><description>Resources DL is a branch of [[Machine Learning]] and [[AI]] based on a set of algorithms that attempt to model high level abstractions in data by using a deep graph with multiple processing layers, composed of multiple linear and non-linear transformations.</description></item><item><title>Dimensionality reduction and low-rank modeling</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Unsupervised-learning/Dimensionality-reduction-and-low-rank-modeling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Unsupervised-learning/Dimensionality-reduction-and-low-rank-modeling/</guid><description>The Beginner&amp;rsquo;s Guide to Dimensionality Reduction: https://idyll.pub/post/visxai-dimensionality-reduction-1dbad0a67a092b007c526a45/ Distances, Neighborhoods, or Dimensions? Projection Literacy for the Analysis of Multivariate Data: https://visxprojections.</description></item><item><title>Encoder-decoder networks</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/Encoder-decoder-networks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/Encoder-decoder-networks/</guid><description>Very common models for semantic segmentation tasks.
[[Deep learning]] architectures composed of two paths, an encoding and a decoding one.</description></item><item><title>Ensemble learning</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Supervised-Learning/Ensemble-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Supervised-Learning/Ensemble-learning/</guid><description>https://en.wikipedia.org/wiki/Ensemble_learning
In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.</description></item><item><title>Explainable AI (XAI)</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/XAI/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/XAI/</guid><description>https://github.com/anguyen8/XAI-papers https://en.wikipedia.org/wiki/Explainable_artificial_intelligence Ideas on interpreting [[machine learning]]: https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning Explainable AI demos: https://lrpserver.hhi.fraunhofer.de/ Why you need to care about Explainable Machine Learning Interpreting machine learning models I.</description></item><item><title>Feature learning</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Feature-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Feature-learning/</guid><description>In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data.</description></item><item><title>Feature selection</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Supervised-Learning/Feature-selection/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Supervised-Learning/Feature-selection/</guid><description>See [[Regression#Regularized regression]]
https://en.wikipedia.org/wiki/Feature_selection
http://machinelearningmastery.com/an-introduction-to-feature-selection/
http://scikit-learn.org/stable/modules/feature_selection.html
Removing features with low variance: http://scikit-learn.org/stable/modules/feature_selection.html#removing-features-with-low-variance</description></item><item><title>Forecasting</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Forecasting/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Forecasting/</guid><description>See: [[Time Series analysis]] [[Regression]] [[RNNs]] [[CNNs#Sequence time series modelling]] [[Transformers#For NLP]]
https://en.wikipedia.org/wiki/Forecasting Microsoft - Time Series Forecasting Best Practices &amp;amp; Examples: https://github.</description></item><item><title>Fourier Neural Operator</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/Fourier-Neural-Operators/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/Fourier-Neural-Operators/</guid><description>#PAPER Fourier Neural Operator for Parametric Partial Differential Equations (Li 2020): https://arxiv.org/abs/2010.08895 #CODE https://github.com/zongyi-li/fourier_neural_operator https://zongyi-li.github.io/blog/2020/fourier-pde/ https://www.technologyreview.com/2020/10/30/1011435/ai-fourier-neural-network-cracks-navier-stokes-and-partial-differential-equations/ Function approximation in Fourier space instead of a the Euclidian (with conventional convolutions) Paper explained: https://www.</description></item><item><title>Generative Adversarial Networks</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/GANs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/GANs/</guid><description>In brief, a GAN consists of two networks; a generator (G) and a discriminator (D), given a set of training examples, G will generate outputs and D will classify them as either being from the same distribution as the training examples or not.</description></item><item><title>Generative modeling</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/Generative-modelling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/Generative-modelling/</guid><description>Generative models: https://openai.com/blog/generative-models/
Deep Generative Models: https://www.cs.toronto.edu/~slwang/generative_model.pdf
Taxonomy of Generative Models: https://christineai.blog/taxonomy/
#COURSE Deep Generative Modeling: VAEs and GANs (MIT 6.</description></item><item><title>Geometric deep learning</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/Geometric-deep-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/Geometric-deep-learning/</guid><description>Talks and courses #TALK Geometric Deep Learning: The Erlangen Programme of ML (M Bronstein, ICLR 2021 Keynote): https://www.youtube.com/watch?v=w6Pw4MOzMuo #COURSE Geometric Deep Learning: https://geometricdeeplearning.</description></item><item><title>Image and video captioning</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Computer-Vision/Image-and-video-captioning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Computer-Vision/Image-and-video-captioning/</guid><description>Review papers:
#PAPER A Systematic Literature Review on Image Captioning (Staniute 2019): https://www.mdpi.com/2076-3417/9/10/2024/htm
#PAPER Survey of convolutional neural networks for image captioning (Kalra 2020): https://www.</description></item><item><title>Image-to-image translation</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Computer-Vision/Image-to-image-translation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Computer-Vision/Image-to-image-translation/</guid><description>The task of Image-to-image translation is to learn the mapping from a given image (X) to a specific target image (Y), e.</description></item><item><title>Inpainting</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Computer-Vision/Inpainting/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Computer-Vision/Inpainting/</guid><description>https://en.wikipedia.org/wiki/Inpainting Inpainting is a conservation process where damaged, deteriorating, or missing parts of an artwork are filled in to present a complete image https://www.</description></item><item><title>Jupyter</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Data-Science-Data-Engineering/Jupyter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Data-Science-Data-Engineering/Jupyter/</guid><description>#CODE Jupyter: https://github.com/jupyter #CODE Jupyterlab: https://github.com/jupyterlab/jupyterlab #CODE Jupyter(hub): https://jupyter.org/hub #CODE Jupyterlite: https://github.com/jupyterlite https://blog.jupyter.org/jupyterlite-jupyter-%EF%B8%8F-webassembly-%EF%B8%8F-python-f6e2e41ab3fa #CODE Papermill - Parameterize, execute, and analyze notebooks: https://github.</description></item><item><title>Learning to rank</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Learning-to-rank/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Learning-to-rank/</guid><description>https://en.wikipedia.org/wiki/Learning_to_rank
Learning to rank or machine-learned ranking (MLR) is the application of machine learning, typically supervised, semi-supervised or [[reinforcement learning]], in the construction of ranking models for information retrieval systems.</description></item><item><title>Machine Learning</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Machine-Learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Machine-Learning/</guid><description>Resources Machine learning identifies patterns using statistical learning and computers by unearthing boundaries in data sets. Awesome ML: https://github.com/josephmisiti/awesome-machine-learning Machine Learning Research Articles: https://deepai.</description></item><item><title>Model selection and tuning</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Supervised-Learning/Model-selection-and-tuning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Supervised-Learning/Model-selection-and-tuning/</guid><description>See [[AutoML]] See [[Data engineering and computer science#Workflow managers distributed ML]]
Model selection and evaluation: https://scikit-learn.org/stable/model_selection.html Code #CODE Weights &amp;amp; Biases: https://docs.</description></item><item><title>Multilayer perceptrons (MLPs)</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/MLPs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/MLPs/</guid><description>A multilayer perceptron (MLP) is a feed forward artificial neural network model that maps sets of input data onto a set of appropriate outputs.</description></item><item><title>Multimodal learning</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/Multimodal-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/Multimodal-learning/</guid><description>General-purpose neural networks capable of handling diverse inputs and output tasks Code #CODE Pykale (in pytorch): https://github.com/pykale/pykale [[#^pykale]] References #PAPER Multi-modal Transformer for Video Retrieval (Gabeur 2020): https://arxiv.</description></item><item><title>Natural Language Processing (NLP)</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/NLP/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/NLP/</guid><description>https://en.wikipedia.org/wiki/Natural_language_processing A Computer Science field connected to Artificial Intelligence and Computational Linguistics which focuses on interactions between computers and human language and a machine’s ability to understand, or mimic the understanding of human language.</description></item><item><title>Neural Ordinary Differential Equations</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/Neural-ODEs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/Neural-ODEs/</guid><description>https://github.com/Zymrael/awesome-neural-ode Understanding Neural ODE&amp;rsquo;s: https://jontysinai.github.io/jekyll/update/2019/01/18/understanding-neural-odes.html Neural Ordinary Differential Equations and Dynamics Models: https://medium.com/@ml.at.berkeley/neural-ordinary-differential-equations-and-dynamics-models-1a4277fbb80 ODEs are often used to describe the time derivatives of a physical situation, referred to as the dynamics.</description></item><item><title>Normalizing flows</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/Normalizing-flows/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/Normalizing-flows/</guid><description>Normalizing flow models are generative models, i.e. they infer the underlying probability distribution of an observed dataset. With that distribution we can do a number of interesting things, namely sample new realistic points and query probability densities.</description></item><item><title>Object classification, image recognition</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Computer-Vision/Object-classification-image-recognition/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Computer-Vision/Object-classification-image-recognition/</guid><description>See: [[CNNs]] [[Object detection]] [[Semantic segmentation]] [[Residual and dense neural networks]]
https://cv-tricks.com/cnn/understand-resnet-alexnet-vgg-inception/
https://blog.paralleldots.com/data-science/must-read-path-breaking-papers-about-image-classification/
#PAPER AlexNet: ImageNet Classification with Deep Convolutional Neural Networks (2012): https://papers.</description></item><item><title>Object detection</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Computer-Vision/Object-detection/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Computer-Vision/Object-detection/</guid><description>See: [[EO]] for applications [[Semantic segmentation]]
Code https://github.com/microsoft/computervision-recipes/tree/master/scenarios/detection #CODE MMdetection: https://github.com/open-mmlab/mmdetection OpenMMLab Detection Toolbox and Benchmark (pytorch) https://mmdetection.readthedocs.io/ #CODE TensorFlow object detection API Repository: https://github.</description></item><item><title>One, few-shot learning</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/One-few-shot-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/One-few-shot-learning/</guid><description>https://en.wikipedia.org/wiki/One-shot_learning One-shot learning is an object categorization problem, found mostly in computer vision. Whereas most machine learning based object categorization algorithms require training on hundreds or thousands of samples/images and very large datasets, one-shot learning aims to learn information about object categories from one, or only a few, training samples/images.</description></item><item><title>Probabilistic machine learning</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Probabilistic-machine-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Probabilistic-machine-learning/</guid><description>See: [[Bayesian neural networks]] [[Bayesian modelling]]
#BOOK Probabilistic Graphical Models: Principles and Techniques (Koller, 2009 MIT): http://pgm.stanford.edu/ #PAPER Probabilistic machine learning and artificial intelligence (Ghahramani 2015): https://www.</description></item><item><title>Problem Solving and Search</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Problem-Solving-and-Search/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Problem-Solving-and-Search/</guid><description>Solving problems by searching: https://glouppe.github.io/info8006-introduction-to-ai/?p=lecture2.md Problem solving and search: https://www.emse.fr/~picard/cours/ai/chapter03.pdf Constraint satisfaction problems: Constraint satisfaction problems (CSPs) are mathematical questions defined as a set of objects whose state must satisfy a number of constraints or limitations.</description></item><item><title>Recurrent Neural Networks (RNNs)</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/RNNs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/RNNs/</guid><description>https://en.wikipedia.org/wiki/Recurrent_neural_network
https://github.com/kjw0612/awesome-rnn
Recurrent Neural Networks cheatsheet: https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks
Tensorflow, DL and RNNs without a PhD: https://docs.</description></item><item><title>Regression</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Supervised-Learning/Regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Supervised-Learning/Regression/</guid><description>See: [[Time Series analysis]] [[RNNs]] [[CNNs#Sequence time series modelling]]
https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/ https://towardsdatascience.com/a-beginners-guide-to-regression-analysis-in-machine-learning-8a828b491bbf http://www.datasciencecentral.com/profiles/blogs/10-types-of-regressions-which-one-to-use http://www.datasciencecentral.com/profiles/blogs/23-types-of-regression Curve fitting vs regression: https://blog.datazar.com/curve-fitting-vs-regression-752ce295b0b1 Goodness of fit: Coefficient of determination (The R-squared measure of goodness of fit): http://blog.</description></item><item><title>Reinforcement learning</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Reinforcement-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Reinforcement-learning/</guid><description>Reinforcement learning is the task of learning what actions to take, given a certain situation/environment, so as to maximize a reward signal.</description></item><item><title>Residual and dense neural networks</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/Residual-and-dense-neural-networks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/Residual-and-dense-neural-networks/</guid><description>https://en.wikipedia.org/wiki/Residual_neural_network
Training and investigating Residual Nets: http://torch.ch/blog/2016/02/04/resnets.html
#PAPER Deep Residual Learning for Image Recognition, Resnet-50 (He 2015): http://arxiv.</description></item><item><title>Self-supervised learning</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Self-supervised-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Self-supervised-learning/</guid><description>https://github.com/jason718/awesome-self-supervised-learning https://hackernoon.com/self-supervised-learning-gets-us-closer-to-autonomous-learning-be77e6c86b5a Self-Supervised Representation Learning: https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html Self-Supervised Vision Models (2021, Dr. Ishan Misra - FAIR): https://www.youtube.com/watch?v=EXJmodhu4_4 Self-supervised learning: The dark matter of intelligence: https://ai.</description></item><item><title>Semantic segmentation</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Computer-Vision/Semantic-segmentation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Computer-Vision/Semantic-segmentation/</guid><description>See: [[Encoder-decoder networks]] for image segmentation [[Object detection]]
https://en.wikipedia.org/wiki/Image_segmentation https://github.com/mrgloom/awesome-semantic-segmentation https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4 Overview of semantic image segmentation: https://www.jeremyjordan.me/semantic-segmentation/ Code #CODE DeepLab2: https://github.</description></item><item><title>Semi-supervised learning</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Semi-supervised-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Semi-supervised-learning/</guid><description>https://en.wikipedia.org/wiki/Semi-supervised_learning Semi-supervised learning is an approach to machine learning that combines a small amount of labeled data with a large amount of unlabeled data during training.</description></item><item><title>Super-resolution</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Computer-Vision/Super-resolution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Computer-Vision/Super-resolution/</guid><description>See [[Image-to-image translation]]
https://github.com/ptkin/Awesome-Super-Resolution https://github.com/ChaofWang/Awesome-Super-Resolution https://keras.io/examples/vision/super_resolution_sub_pixel/ Image Super-Resolution: A Comprehensive Review (2020): https://blog.paperspace.com/image-super-resolution/ #TALK How Super Resolution Works (2019): https://www.</description></item><item><title>Supervised Learning</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Supervised-Learning/Supervised-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Supervised-Learning/Supervised-learning/</guid><description>https://en.wikipedia.org/wiki/Supervised_learning Supervised Learning cheatsheet: https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-supervised-learning http://scikit-learn.org/stable/supervised_learning.html Metrics: http://docs.aws.amazon.com/machine-learning/latest/dg/binary-model-insights.html ROC curves, AUC http://www.dataschool.io/roc-curves-and-auc-explained/ http://corysimon.github.io/articles/what-is-an-roc-curve/ [[Classification]] [[Regression]] Structured learning https://en.</description></item><item><title>Time series analysis</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Time-Series-analysis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Time-Series-analysis/</guid><description>See: [[Forecasting]] [[RNNs]] [[CNNs#Sequence time series modelling]] [[Deep learning#Deep learning for tabular data]]
https://en.wikipedia.org/wiki/Time_series https://github.com/MaxBenChrist/awesome_time_series_in_python https://github.com/frutik/awesome-timeseries https://github.com/cuge1995/awesome-time-series http://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/ Python to work with time series data: https://github.</description></item><item><title>Transfer learning</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Transfer-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Transfer-learning/</guid><description>https://en.wikipedia.org/wiki/Transfer_learning Transfer learning (TL) is a research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem https://github.</description></item><item><title>Transformers</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/Transformers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/Transformers/</guid><description>https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/ (from RNNs with attention to Transformers) https://analyticsindiamag.com/a-complete-learning-path-to-transformers/ https://analyticsindiamag.com/transformers-for-vision-7-works-that-indicate-fusion-is-the-future-of-ai/ Code #CODE Transformers: https://github.com/huggingface/transformers #CODE Xformers: https://github.com/facebookresearch/xformers For NLP #PAPER Attention is all you need (Vaswani 2017): https://arxiv.</description></item><item><title>Unsupervised learning</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Unsupervised-learning/Unsupervised-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Unsupervised-learning/Unsupervised-learning/</guid><description>https://en.wikipedia.org/wiki/Unsupervised_learning Unsupervised Learning cheatsheet: https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-unsupervised-learning Unsupervised machine learning is the machine learning task of inferring a function to describe hidden structure from &amp;ldquo;unlabeled&amp;rdquo; data (a classification or categorization is not included in the observations).</description></item><item><title>Video segmentation and prediction</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Computer-Vision/Video-segmentation-and-prediction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Computer-Vision/Video-segmentation-and-prediction/</guid><description>See: [[Encoder-decoder networks]] [[Deep learning#Deep learning for multi-dimensional data]] [[RNNs]]
Spatiotemporal classification and regression
Hybrid convolutional and recurrent networks, 3dconv and related approaches</description></item><item><title>Weakly supervised learning</title><link>https://carlgogo.github.io/AIDigitalGarden/AI/Weakly-supervised-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlgogo.github.io/AIDigitalGarden/AI/Weakly-supervised-learning/</guid><description>Weak supervision is a branch of machine learning where noisy, limited, or imprecise sources are used to provide supervision signal for labeling large amounts of training data in a supervised learning setting.</description></item></channel></rss>