<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="See:
 [[AI4ES/Emulators]] [[AI/Computer Vision/Video segmentation and prediction]] [[AI/Deep learning/CNNs]] [[AI/Deep learning/RNNs]] [[AI/Deep learning/Fourier Neural Operators]]   Code  #CODE PySTEPS - Python framework for short-term ensemble prediction systems   https://pysteps."><title>Weather forecasting, nowcasting</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://carlos-gg.github.io/digitalgarden//icon.png><link href=https://carlos-gg.github.io/digitalgarden/styles.b3e1e36b0403ac565c9392b3e23ef3b6.min.css rel=stylesheet><link href=https://carlos-gg.github.io/digitalgarden/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://carlos-gg.github.io/digitalgarden/js/darkmode.18b7c6dfe67ae3bf2317338cf4189144.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/util.9825137f5e7825e8553c68ce39ac9e44.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/popover.abe6a51cc7138c5dff00f151dd627ad1.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/code-title.b35124ad8db0ba37162b886afb711cbc.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/clipboard.c20857734e53a3fb733b7443879efa61.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const BASE_URL="https://carlos-gg.github.io/digitalgarden/",fetchData=Promise.all([fetch("https://carlos-gg.github.io/digitalgarden/indices/linkIndex.15aea4010da4689ebfb0d0bc0de36185.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://carlos-gg.github.io/digitalgarden/indices/contentIndex.bfd8ee348e31547f8eba56b56185a534.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://carlos-gg.github.io/digitalgarden",!0,!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!0;drawGraph("https://carlos-gg.github.io/digitalgarden",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/carlos-gg.github.io\/digitalgarden\/js\/router.9d4974281069e9ebb189f642ae1e3ca2.min.js"
    attachSPARouting(init, render)
  </script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-S103D50NQ0"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S103D50NQ0",{anonymize_ip:!1})}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://carlos-gg.github.io/digitalgarden/js/full-text-search.24827f874defbbc6d529926cbfcfb493.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://carlos-gg.github.io/digitalgarden/>CarlosGG's Knowledge Garden 🪴</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Weather forecasting, nowcasting</h1><p class=meta>Last updated
Sep 9, 2022
<a href=https://github.com/jackyzha0/quartz/tree/hugo/content/AI4ES/Weather%20forecasting,%20nowcasting.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details open><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#code>Code</a></li><li><a href=#courses>Courses</a></li><li><a href=#references>References</a></li></ol></nav></details></aside><blockquote><p>See:</p><ul><li><a href=/digitalgarden/AI4ES/Emulators rel=noopener class=internal-link data-src=/digitalgarden/AI4ES/Emulators>AI4ES/Emulators</a></li><li><a href=/digitalgarden/AI/Computer-Vision/Video-segmentation-and-prediction rel=noopener class=internal-link data-src=/digitalgarden/AI/Computer-Vision/Video-segmentation-and-prediction>AI/Computer Vision/Video segmentation and prediction</a></li><li><a href=/digitalgarden/AI/Deep-learning/CNNs rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/CNNs>AI/Deep learning/CNNs</a></li><li><a href=/digitalgarden/AI/Deep-learning/RNNs rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/RNNs>AI/Deep learning/RNNs</a></li><li><a href=/digitalgarden/AI/Deep-learning/Fourier-Neural-Operators rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Fourier-Neural-Operators>AI/Deep learning/Fourier Neural Operators</a></li></ul></blockquote><a href=#code><h2 id=code><span class=hanchor arialabel=Anchor># </span>Code</h2></a><ul><li>#CODE
<a href=https://github.com/pySTEPS/pysteps rel=noopener>PySTEPS - Python framework for short-term ensemble prediction systems</a><ul><li><a href=https://pysteps.github.io/ rel=noopener>https://pysteps.github.io/</a></li></ul></li></ul><a href=#courses><h2 id=courses><span class=hanchor arialabel=Anchor># </span>Courses</h2></a><ul><li>#COURSE
<a href=https://www.ecmwf.int/en/elibrary/19274-sources-uncertainty rel=noopener>Sources of uncertainty (Buizza, ECMWF)</a></li><li>#COURSE
<a href=https://www.ecmwf.int/assets/elearning/stochphysics/stochphysics1/story_html5.html rel=noopener>Using stochastic physics to represent model uncertainty</a></li></ul><a href=#references><h2 id=references><span class=hanchor arialabel=Anchor># </span>References</h2></a><ul><li>#PAPER
<a href=https://arxiv.org/abs/1506.04214 rel=noopener>ConvLSTM: Convolutional LSTM Network- A Machine Learning Approach for Precipitation Nowcasting (Shi 2015)</a><ul><li>#CODE
<a href=https://keras.io/examples/conv_lstm/ rel=noopener>https://keras.io/examples/conv_lstm/</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1706.03458 rel=noopener>Deep Learning for Precipitation Nowcasting: A Benchmark and A New Model, trajGRU (Shi 2017)</a><ul><li><a href=https://github.com/CNALeon007/TrajGRU rel=noopener>https://github.com/CNALeon007/TrajGRU</a></li></ul></li><li>#PAPER
<a href=https://deepstruct.github.io/ICML17/1stDeepStructWS_paper_2.pdf rel=noopener>Automating weather forecasts based on CNNs (Rozas Larraondo 2017)</a></li><li><a href=https://addi.ehu.es/handle/10810/32532 rel=noopener>#THESIS/PHD Application of machine learning techniques to weather forecasting (Rozas Larraondo 2019)</a></li><li>#PAPER
<a href=https://ieeexplore.ieee.org/document/8777193 rel=noopener>A Generative Adversarial Gated Recurrent Unit Model for Precipitation Nowcasting (Tian, 2019)</a></li><li>#PAPER
<a href=https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2019MS001705 rel=noopener>Can machines learn to predict weather? Using deep learning to predict gridded 500‐hPa geopotential height from historical weather data (Weyn 2019)</a></li><li>#PAPER
<a href=https://sailinglab.github.io/pgm-spring-2019/assets/project/final-reports/project7.pdf rel=noopener>Reversible Deep Generative Models for Climate Informatics (Rosenfeld 2019)</a></li><li>#PAPER
<a href=https://www.mdpi.com/2073-4433/10/11/668 rel=noopener>Prediction of Rainfall Using Intensified LSTM Based Recurrent Neural Network with Weighted Linear Units (Poornima 2019)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1906.08829 rel=noopener>Data-driven predictions of a multiscale Lorenz 96 chaotic system using machine-learning methods: reservoir computing, artificial neural network, and LSTM (Chattopadhyay 2019)</a><ul><li>#CODE
<a href=https://github.com/ashesh6810/RCESN_spatio_temporal rel=noopener>https://github.com/ashesh6810/RCESN_spatio_temporal</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1812.09467 rel=noopener>Deep Uncertainty Quantification: A Machine Learning Approach for Weather forecasting (Wang 2019)</a><ul><li>Proposed data-driven method augmented by an effective information fusion mechanism to learn from historical data that incorporates prior knowledge from NWP</li><li>Weather forecasting problem posed as an end-to-end deep learning problem and solve it by proposing a novel negative log-likelihood error (NLE) loss function</li><li>A notable advantage of our proposed method is that it simultaneously implements single-value forecasting and uncertainty quantification, which we refer to as deep uncertainty quantification (DUQ)</li><li>The proposed DUQ is based on sequence-to-sequence (seq2seq, a.k.a Encoder-Decoder)</li><li>DUQ predicts two values at each timestep corresponding to the predicted mean and variance to parameterize the Gaussian distributions</li></ul></li><li>#PAPER
<a href=https://www.mdpi.com/2073-4433/10/5/244/htm rel=noopener>Computer Vision in Precipitation Nowcasting: Applying Image Quality Assessment Metrics for Training Deep Neural Networks (Tran 2019)</a></li><li>#PAPER
<a href=https://www.atmos-chem-phys.net/20/2303/2020/ rel=noopener>Technical note: DL for creating surrogate models of precipitation in Earth system models (Weber 2020)</a><ul><li>Precipitation forecasting using resnets</li><li>#CODE
<a href=https://github.com/hutchresearch/deep_climate_emulator rel=noopener>https://github.com/hutchresearch/deep_climate_emulator</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2003.11927 rel=noopener>Improving data-driven global weather prediction using deep CNNs on a cubed sphere (Weyn 2020)</a> ^bd4b0a<ul><li><a href=https://github.com/jweyn/DLWP-CS rel=noopener>https://github.com/jweyn/DLWP-CS</a></li><li>New developments in this framework include an offline volume-conservative mapping to a cubed-sphere grid, improvements to the CNN architecture (U-NET), and the minimization of the loss function over multiple steps in a prediction sequence.</li><li>The cubed-sphere remapping minimizes the distortion on the cube faces on which convolution operations are performed and provides natural boundary conditions for padding in the CNN.</li><li>Our improved model produces weather forecasts that are indefinitely stable and produce realistic weather patterns at lead times of several weeks and longer</li><li>#TALK
<a href="https://www.youtube.com/watch?v=Yk0eSNs7nbs" rel=noopener>S2S forecasting using large ensembles of data-driven global weather prediction models</a></li></ul></li><li>#PAPER
<a href=https://www.essoar.org/doi/10.1002/essoar.10502527.2 rel=noopener>A Machine-Learning-Based Global Atmospheric Forecast Model (Szunyogh 2020)</a><ul><li>The paper investigates the applicability of machine learning (ML) to weather prediction by building a reservoir-computing-based, low-resolution, global prediction model.</li><li>The model is designed to take advantage of the massively parallel architecture of a modern supercomputer.</li><li>The forecast performance of the model is assessed by comparing it to that of daily climatology, persistence, and a numerical (physics-based) model of identical prognostic state variables and resolution</li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2003.12140 rel=noopener>MetNet: A Neural Weather Model for Precipitation Forecasting (Sonderby 2020)</a><ul><li>#TALK
<a href=https://vimeo.com/417618472 rel=noopener>https://vimeo.com/417618472</a><ul><li><a href=https://www.ecmwf.int/sites/default/files/medialibrary/2020-05/12_May.pdf rel=noopener>https://www.ecmwf.int/sites/default/files/medialibrary/2020-05/12_May.pdf</a></li></ul></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2008.09090v1 rel=noopener>TRU-NET: A Deep Learning Approach to High Resolution Prediction of Rainfall (Adewoyin 2020)</a><ul><li>#CODE
<a href=https://github.com/Akanni96/TRUNET rel=noopener>https://github.com/Akanni96/TRUNET</a></li></ul></li><li>#PAPER
<a href=https://www.nature.com/articles/s41598-020-57897-9 rel=noopener>Predicting clustered weather patterns: A test case for applications of convolutional neural networks to spatio-temporal climate data (Chattopadhyay, 2020)</a> ^e83f4e<ul><li>Introduced an unsupervised auto-labeling strategy that can facilitate exploring the capabilities of supervised deep learning techniques such as CNNs in studying problems in climate and environmental sciences</li><li>Applied this strategy to clustered daily large-scale weather patterns over North America<ul><li>focused on re-identifying and predicting the daily weather patterns over North America in summer and winter</li><li>focus on daily averaged geopotential height at 500 hPa (Z500 hereafter), whose isolines are approximately the streamlines of the large-scale circulation at mid-troposphere and are often used to represent weather patterns</li><li>used data from the Large Ensemble (LENS) Community Project, which consists of a 40-member ensemble of fully-coupled Community Earth System Model version 1 (CESM1) simulations with historical radiative forcing from 1920 to 2005</li><li>daily Z500 from 1980 to 2005 provides ~95000 samples for summer months and for winter months over North America</li><li>the K-means algorithm is used to classify the winter days and summer days (separately) into n = 4 clusters</li><li>the clustering analysis is performed on zonal-mean-removed daily Z500 anomalies projected on 22 EOFS that retain approximately 95% of the variance</li><li>the computed cluster index for each day is used to label the full Z500 pattern of that day and 5 days earlier</li><li>full Z500 fields are used, rather than the computed anomalies (or any other type of anomalies), because one hopes to use CNN with minimally pre-processed data</li><li>task: re-identifying and predicting the clusters in the full Z500 fields, which include complex temporal variabilities such as the seasonal cycle and non-stationarity resulting from the low-frequency coupled ocean-atmosphere modes and changes in the radiative forcing between 1980 and 2005</li><li>network: trivial CNN for classification with a final output of 4 values (categorical cross-entropy)</li></ul></li><li>Showed the outstanding performance of CNNs in re-identifying and predicting patterns in chaotic, multi-scale, non-stationary, spatio-temporal data with minimal pre-processing</li><li>CNNs are not used as a clustering technique, as clusters are already found using an unsupervised method (the K-means algorithm). Rather, CNNs are used to predict which cluster index a Z500 pattern will belong to in 1–5 days in the future</li><li>The cluster-based forecasting of circulation patterns that is presented here, again if performed using a CNN trained on reanalysis data and using more input variables, might help with prediction of low-frequency variability in the S2S timescales (see <a href=/digitalgarden/AI4ES/S2S rel=noopener class=internal-link data-src=/digitalgarden/AI4ES/S2S>AI4ES/S2S</a>)</li><li>The scaling (of the prediction accuracy of the CNNs) that is found here shows a nonlinear relation between accuracy and N, and suggests that the amount of data currently available from reanalysis since 1979 can be enough to successfully train an accurate CNN for applications involving daily large-scale weather patterns</li><li>Follow up paper using CapsNets and extreme temperature clusters <a href=/digitalgarden/AI4ES/Extremes-events#d42267 rel=noopener class=internal-link data-src=/digitalgarden/AI4ES/Extremes-events>AI4ES/Extremes events</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2005.04988 rel=noopener>A review of radar-based nowcasting of precipitation and applicable machine learning techniques (Prudden, 2020)</a></li><li>#PAPER
<a href=https://npg.copernicus.org/preprints/npg-2020-39/ rel=noopener>Boosting performance in machine learning of geophysical flows via scale separation (Faranda 2020)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/2011.11705 rel=noopener>DeepClimGAN: A High-Resolution Climate Data Generator (Puchko 2020)</a><ul><li><a href=/digitalgarden/AI/Deep-learning/GANs rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/GANs>AI/Deep learning/GANs</a> for spatio-temporal forecasting</li><li><a href=https://www.climatechange.ai/CameraReadySubmissions-2-119/52/CameraReadySubmission/nips2019_icml.pdf rel=noopener>https://www.climatechange.ai/CameraReadySubmissions%202-119/52/CameraReadySubmission/nips2019_icml.pdf</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2006.07718 rel=noopener>A generative adversarial network approach to (ensemble) weather prediction (Bihlo 2020)</a><ul><li>Implemented a deep convolutional <a href=/digitalgarden/AI/Deep-learning/GANs rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/GANs>AI/Deep learning/GANs</a> to predict the geopotential height of the 500 hPa pressure level, the two-meter temperature and the total precipitation for the next 24 hours over Europe</li><li>The proposed models are trained on 4 years of ERA5 reanalysis data from 2015–2018 with the goal to predict the associated meteorological fields in 2019</li><li>The forecasts show a good qualitative and quantitative agreement with the true reanalysis data for the geopotential height and two-meter temperature, while failing for total precipitation, thus indicating that weather forecasts based on data alone may be possible for specific meteorological parameters</li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2002.05398v2 rel=noopener>Ensemble methods for neural network-based weather forecasts (Scher 2020)</a><ul><li>#CODE
<a href=https://github.com/sipposip/ensemble-neural-network-weather-forecasts rel=noopener>https://github.com/sipposip/ensemble-neural-network-weather-forecasts</a></li><li>Aimed to transform a deterministic neural network weather forecasting system into an ensemble forecasting system</li><li>Tested four methods to generate the ensemble: random initial perturbations, retraining of the neural network, use of random dropout in the network, and the creation of initial perturbations with singular vector decomposition</li><li>The latter method is widely used in numerical weather prediction models, but is yet to be tested on neural networks</li><li>The ensemble mean forecasts obtained from these four approaches all beat the unperturbed neural network forecasts, with the retraining method yielding the highest improvement</li><li>The skill of the neural network fore-casts is systematically lower than that of state-of-the-art numerical weather prediction models</li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2008.13524 rel=noopener>Spherical convolution and other forms of informed machine learning for deep neural network based weather forecasts (Scher 2020)</a><ul><li>CNN-based weather forecasting solutions are are usually trained on atmospheric data represented as regular latitude-longitude grids, neglecting the curvature of the Earth</li><li>Showed the benefit of replacing the convolution operations with a spherical convolution operation, which takes into account the geometry of the underlying data, including correct representations near the poles</li><li>Additionally, studied the effect of including the information that the two hemispheres of the Earth have “flipped” properties - for example cyclones circulating in opposite directions - into the structure of the network</li><li>Using spherical convolution leads to an additional improvement in forecast skill, especially close to the poles in the first days of the forecast</li><li>The spherical convolution is implemented flexibly and scales well to high resolution datasets, but is still significantly more expensive than a standard convolution operation</li></ul></li><li>#THESIS/PHD [Artificial intelligence in weather and climate prediction (Scher 2020)](
<a href=https://www.diva-portal.org/smash/get/diva2:1425352/FULLTEXT01.pdf rel=noopener>https://www.diva-portal.org/smash/get/diva2:1425352/FULLTEXT01.pdf</a> ^2e6f0f)</li><li>#THESIS/MSC
<a href=https://infoscience.epfl.ch/record/278138 rel=noopener>Geometric deep learning for medium-range weather prediction (Llorens 2020)</a><ul><li>#CODE
<a href=https://github.com/illorens/weather_prediction rel=noopener>https://github.com/illorens/weather_prediction</a></li><li>Spherical CNNs benchmarking</li></ul></li><li>#PAPER
<a href=https://link.springer.com/article/10.1007/s00500-020-04954-0 rel=noopener>Temporal convolutional neural (TCN) network for an effective weather forecasting using time-series data from the local weather station (Hewage 2020)</a></li><li>#PAPER
<a href=https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2019MS001909 rel=noopener>Optimization of Deep Learning Precipitation Models Using Categorical Binary Metrics (Rozas Larraondo 2020)</a><ul><li>#CODE
<a href=https://github.com/prl900/weather_encoders rel=noopener>https://github.com/prl900/weather_encoders</a></li><li>This work introduces a methodology for optimizing neural network models using a combination of continuous and categorical binary indices in the context of precipitation forecasting</li><li>Proposed an alternative formulation for these categorical indices that are differentiable and we demonstrate how they can be used to optimize the skill of precipitation neural network models defined as a multiobjective optimization problem</li></ul></li><li>#PAPER
<a href=https://www.mdpi.com/1996-1073/13/13/3440/htm rel=noopener>Localized Convolutional Neural Networks for Geospatial Wind Forecasting (Uselis 2020)</a> ^uselis20<ul><li>#CODE
<a href=https://github.com/oshapio/Localized-CNNs-for-Geospatial-Wind-Forecasting rel=noopener>https://github.com/oshapio/Localized-CNNs-for-Geospatial-Wind-Forecasting</a></li><li>In a convolutional layer, each neuron has a fixed local receptive field of the layer input and shares its weights with all the other (repeated) neurons arranged in a lattice corresponding to the dimensions of the input</li><li>Typically, an element-wise nonlinear function is applied to the results of the convolution which gives a lattice of identical weighted-sum-and-non linearity neurons each looking at a different k×k size patch of the input image</li><li>Convolutions give CNNs some unique benefits, compared to regular MLPs: less trainable weights (faster and with less overfitting), every filter is trained on every k×k patch of every input image g(·,·), which utilizes the training data well, the architecture and learned weights of the convolutional layer do not depend on the size of the input image (easier to reuse), convolutions give translation invariance (the features are detected the same way, no matter where they are in the image)</li><li>Translation invariance is very important for good generalization when objects being detected are randomly framed in the images. Translation invariance, however, is absolute in convolutional layers</li><li>This location agnosticism might not always be optimal even for images as not all objects or features are equally likely to appear in every part of them. This is especially evident when the images have a constant static framing, eg., geospatial ad meteorological data</li><li>While the same laws of atmospheric physics apply at every location, each location typically also has its own unique features like altitude, terrain, land use, large objects, sun absorption, the heat capacity of the ground,heat sources, etc. that affect the dynamics</li><li>The complete location agnosticism in CNNs can be remedied in several ways by supplying different additional static location-dependent features:<ul><li>The explicit coordinates of each location like in CoordConv</li><li>A combination of local random static location-dependent inputs, that could potentially allow us to uniquely “identify” each location as well</li><li>The above mentioned real-world relevant unique location-specific features if they are explicitly available (typically not all of them are)</li></ul></li><li>Additionally, location-based differences could be learned more directly by introducing additional (at every input lattice location):<ul><li>Learnable local inputs/latent variables</li><li>Learnable local transformations of the inputs in the form of local weights</li></ul></li><li>“Localized CNNs” - models that learn to treat the different locations/pixels in the input similarly, but not identically</li></ul></li><li>#PAPER
<a href=https://dl.acm.org/doi/10.1145/3429309.3429325 rel=noopener>Deep spatial transformers for autoregressive data-driven forecasting of geophysical turbulence (Chattopadhyay 2020)</a><ul><li><a href=https://eartharxiv.org/repository/view/118/ rel=noopener>https://eartharxiv.org/repository/view/118/</a></li></ul></li><li>#PAPER
<a href=https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2020MS002405 rel=noopener>Data-driven medium-range weather prediction with a Resnet pretrained on climate simulations: A new model for WeatherBench (Rasp 2021)</a><ul><li>There are three fundamental techniques for creating data‐driven forecasts: direct, continuous and iterative. For direct forecasts, a separate model is trained directly for each desired forecast time. In continuous models, time is an additional input and a single model is trained to predict all forecast lead times (as in MetNet). Finally, iterative forecasts are created by training a direct model for a short forecast time (e.g., 6 h) and then running the model several times using its own output from the previous iteration. As mentioned above, this is the approach taken by Weyn et al. 2020</li><li>First train our model using the 150 years of CMIP data described above. We then take the pretrained model and fine‐tune it using the ERA data</li></ul></li><li>#PAPER
<a href=https://hess.copernicus.org/articles/26/795/2022/hess-26-795-2022.html rel=noopener>Evaluation and interpretation of convolutional long short-term memory networks for regional hydrological modelling (Anderson 2022)</a><ul><li>#CODE
<a href=https://github.com/andersonsam/cnn_lstm_era rel=noopener>https://github.com/andersonsam/cnn_lstm_era</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2202.11214 rel=noopener>FourCastNet: A Global Data-driven High-resolution Weather Model using Adaptive Fourier Neural Operators (Pathak 2022)</a><ul><li><a href="https://www.youtube.com/watch?v=nuT_U1AQz3g" rel=noopener>Accelerating Extreme Weather Prediction with FourCastNet</a></li><li>#TALK
<a href="https://www.youtube.com/watch?v=IBTVAC82xtQ" rel=noopener>Building Digital Twins of the Earth for NVIDIA&rsquo;s Earth-2 Initiative</a></li><li>#CODE
<a href=https://github.com/NVlabs/FourCastNet rel=noopener>https://github.com/NVlabs/FourCastNet</a></li><li>#CODE
<a href=https://github.com/HFAiLab/FourCastNet rel=noopener>https://github.com/HFAiLab/FourCastNet</a> (NOT OFFICIAL)</li><li>ERA5 0.25 deg, 20 variables, from 1979 (~50k training samples)</li><li>Unparalleled accuracy at forecast lead times of up to one week, challenging variables such as surface winds and precipitation</li><li>FourCastNet has eight times greater resolution than state-of-the-art DL-based global weather models</li><li>FourCastNet’s predictions are comparable to the IFS model on metrics of RMSE and Anomaly Correlation Coefficient (ACC) at lead times of up to three days</li><li>FourCastNet’s reliable, rapid, and computationally inexpensive forecasts facilitate the generation of very large ensembles, thus enabling estimation of well-calibrated and constrained uncertainties in extremes with higher confidence than current NWP ensembles</li><li>The AFNO model is unique in that it frames the mixing operation as continuous global convolution, implemented efficiently in the Fourier domain with FFTs, which allows modeling dependencies across spatial and channel dimensions flexibly and scalably</li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2202.04964 rel=noopener>Forecasting large-scale circulation regimes using deformable convolutional neural networks and global spatiotemporal climate data (Holm Nielsen 2022)</a><ul><li>supervised machine learning approach based on deformable convolutional neural networks (deCNNs) and transfer learning to forecast the North Atlantic-European weather regimes during extended boreal winter for 1 to 15 days into the future</li><li>authors could extract and learn transformation-invariant spatial patterns across large geographical areas using deformable convolutions, which is not possible with regular CNNs</li><li>used pre-training (training on the large 20CRv3 reanalysis dataset spanning from 1836 to 1980, then transfer learning)</li><li>using an interpretation technique called integrated gradients, we could attribute each variable’s contributions for a particular observation on a grid-point basis. This is especially important if we want to understand global climate processes better and explain drivers behind specific weather regimes that account for major uncertainty in NWP models days to weeks ahead</li></ul></li><li>#PAPER
<a href=https://gmd.copernicus.org/articles/15/2221/2022/ rel=noopener>Towards physics-inspired data-driven weather forecasting: integrating data assimilation with a deep spatial-transformer-based U-NET in a case study with ERA5 (Chattopadhyay 2022)</a></li><li>#PAPER
<a href=https://arxiv.org/pdf/2207.05833 rel=noopener>Earthformer: Exploring Space-Time Transformers for Earth System Forecasting (Gao 2022)</a><ul><li>Earthformer is a space-time Transformer for Earth system forecasting. Earthformer is based on a generic, flexible and efficient space-time attention block, named Cuboid Attention</li><li>The idea is to decompose the data into cuboids and apply cuboid-level self-attention in parallel. These cuboids are further connected with a collection of global vectors</li></ul></li></ul></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/digitalgarden/AI/Deep-learning/Fourier-Neural-Operators/ data-ctx="AI4ES/Weather forecasting, nowcasting" data-src=/AI/Deep-learning/Fourier-Neural-Operators class=internal-link>Fourier Neural Operator (FNO)</a></li><li><a href=/digitalgarden/AI4ES/AI4ES/ data-ctx="AI4ES/Weather forecasting, nowcasting" data-src=/AI4ES/AI4ES class=internal-link>AI4ES</a></li><li><a href=/digitalgarden/AI4ES/Emulators/ data-ctx="AI4ES/Weather forecasting, nowcasting" data-src=/AI4ES/Emulators class=internal-link>(Earth system model) Emulators and surrogates</a></li><li><a href=/digitalgarden/AI4ES/Ensembles-multi-models/ data-ctx="AI4ES/Weather forecasting, nowcasting" data-src=/AI4ES/Ensembles-multi-models class=internal-link>Ensembles, multi-models</a></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://carlos-gg.github.io/digitalgarden/js/graph.abd4bc2af3869a96524d7d23b76152c7.js></script></div></div><div id=contact_buttons><footer><p>Made by Carlos Alberto Gomez Gonzalez using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2022</p><ul><li><a href=https://carlos-gg.github.io/digitalgarden/>Home</a></li><li><a href=https://carlos-gg.github.io>Carlos'Homepage</a></li></ul></footer></div></div></body></html>