<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Super-resolution See [[Image-to-image translation]]
 https://github.com/ptkin/Awesome-Super-Resolution https://github.com/ChaofWang/Awesome-Super-Resolution https://keras.io/examples/vision/super_resolution_sub_pixel/ Image Super-Resolution: A Comprehensive Review (2020): https://blog.paperspace.com/image-super-resolution/ #TALK How Super Resolution Works (2019): https://www."><title>ðŸª´ AI and DS Digital Garden</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=/icon.png><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Source+Sans+Pro:wght@400;600;700&family=Fira+Code:wght@400;700&display=swap" rel=stylesheet><style>:root{--lt-colours-light:var(--light) !important;--lt-colours-lightgray:var(--lightgray) !important;--lt-colours-dark:var(--secondary) !important;--lt-colours-secondary:var(--tertiary) !important;--lt-colours-gray:var(--outlinegray) !important}h1,h2,h3,h4,h5,h6,ol,ul,thead{font-family:Inter;color:var(--dark);font-weight:revert;margin:revert;padding:revert}p,ul,text{font-family:source sans pro,sans-serif;color:var(--gray);fill:var(--gray);font-weight:revert;margin:revert;padding:revert}#TableOfContents>ol{counter-reset:section;margin-left:0;padding-left:1.5em}#TableOfContents>ol>li{counter-increment:section}#TableOfContents>ol>li>ol{counter-reset:subsection}#TableOfContents>ol>li>ol>li{counter-increment:subsection}#TableOfContents>ol>li>ol>li::marker{content:counter(section)"." counter(subsection)"  "}#TableOfContents>ol>li::marker{content:counter(section)"  "}#TableOfContents>ol>li::marker,#TableOfContents>ol>li>ol>li::marker{font-family:Source Sans Pro;font-weight:700}footer{margin-top:4em;text-align:center}table{width:100%}img{width:100%;border-radius:3px;margin:1em 0}p>img+em{display:block;transform:translateY(-1em)}sup{line-height:0}p,tbody,li{font-family:Source Sans Pro;color:var(--gray);line-height:1.5em}blockquote{margin-left:0;border-left:3px solid var(--secondary);padding-left:1em;transition:border-color .2s ease}blockquote:hover{border-color:var(--tertiary)}table{padding:1.5em}td,th{padding:.1em .5em}.footnotes p{margin:.5em 0}.pagination{list-style:none;padding-left:0;display:flex;margin-top:2em;gap:1.5em;justify-content:center}.pagination>li{text-align:center;display:inline-block}.pagination>li a{background-color:initial!important}.pagination>li a[href$="#"]{opacity:.2}.section h3>a{font-weight:700;font-family:Inter;margin:0}.section p{margin-top:0}article>.meta{margin:-1.5em 0 1em;opacity:.7}article>.tags{list-style:none;padding-left:0}article>.tags .meta>h1{margin:0}article>.tags .meta>p{margin:0}article>.tags>li{display:inline-block}article>.tags>li>a{border-radius:8px;border:var(--outlinegray)1px solid;padding:.2em .5em}article>.tags>li>a::before{content:"#";margin-right:.3em;color:var(--outlinegray)}article a{font-family:Source Sans Pro;font-weight:600}article a.internal-link{text-decoration:none;background-color:rgba(143,159,169,.15);padding:0 .1em;margin:auto -.1em;border-radius:3px}.backlinks a{font-weight:600;font-size:.9rem}sup>a{text-decoration:none;padding:0 .1em 0 .2em}a{font-family:Inter,sans-serif;font-size:1em;font-weight:700;text-decoration:none;transition:all .2s ease;color:var(--secondary)}a:hover{color:var(--tertiary)!important}pre{font-family:fira code;padding:.75em;border-radius:3px;overflow-x:scroll}code{font-family:fira code;font-size:.85em;padding:.15em .3em;border-radius:5px;background:var(--lightgray)}html{scroll-behavior:smooth}html:lang(ar) p,html:lang(ar) h1,html:lang(ar) h2,html:lang(ar) h3,html:lang(ar) article{direction:rtl;text-align:right}body{margin:0;height:100vh;width:100vw;overflow-x:hidden;background-color:var(--light)}@keyframes fadeIn{0%{opacity:0}100%{opacity:1}}footer{margin-top:4em}footer>a{font-size:1em;color:var(--secondary);padding:0 .5em 3em}hr{width:25%;margin:4em auto;height:2px;border-radius:1px;border-width:0;color:var(--dark);background-color:var(--dark)}.singlePage{margin:4em 30vw}@media all and (max-width:1200px){.singlePage{margin:25px 5vw}}.page-end{display:flex;flex-direction:row;gap:2em}@media all and (max-width:780px){.page-end{flex-direction:column}}.page-end>*{flex:1 0}.page-end>.backlinks-container>ul{list-style:none;padding-left:0}.page-end>.backlinks-container>ul>li{margin:.5em 0;padding:.25em 1em;border:var(--outlinegray)1px solid;border-radius:5px}.page-end #graph-container{border:var(--outlinegray)1px solid;border-radius:5px}.centered{margin-top:30vh}article>h1{font-size:2em}header{display:flex;flex-direction:row;align-items:center}header>h1{font-size:2em}@media all and (max-width:600px){header>nav{display:none}}header>.spacer{flex:auto}header>svg{cursor:pointer;width:18px;min-width:18px;margin:0 1em}header>svg:hover .search-path{stroke:var(--tertiary)}header>svg .search-path{stroke:var(--gray);stroke-width:2px;transition:stroke .5s ease}#search-container{position:fixed;z-index:9999;left:0;top:0;width:100vw;height:100%;overflow:scroll;display:none;backdrop-filter:blur(4px);-webkit-backdrop-filter:blur(4px)}#search-container>div{width:50%;margin-top:15vh;margin-left:auto;margin-right:auto}@media all and (max-width:1200px){#search-container>div{width:90%}}#search-container>div>*{width:100%;border-radius:4px;background:var(--light);box-shadow:0 14px 50px rgba(27,33,48,.12),0 10px 30px rgba(27,33,48,.16);margin-bottom:2em}#search-container>div>input{box-sizing:border-box;padding:.5em 1em;font-family:Inter,sans-serif;color:var(--dark);font-size:1.1em;border:1px solid var(--outlinegray)}#search-container>div>input:focus{outline:none}#search-container>div>#results-container>.result-card{padding:1em;cursor:pointer;transition:background .2s ease;border:1px solid var(--outlinegray);border-bottom:none;width:100%;font-family:inherit;font-size:100%;line-height:1.15;margin:0;overflow:visible;text-transform:none;text-align:left;background:var(--light);outline:none}#search-container>div>#results-container>.result-card:hover,#search-container>div>#results-container>.result-card:focus{background:rgba(180,180,180,.15)}#search-container>div>#results-container>.result-card:first-of-type{border-top-left-radius:5px;border-top-right-radius:5px}#search-container>div>#results-container>.result-card:last-of-type{border-bottom-left-radius:5px;border-bottom-right-radius:5px;border-bottom:1px solid var(--outlinegray)}#search-container>div>#results-container>.result-card>h3,#search-container>div>#results-container>.result-card>p{margin:0}#search-container>div>#results-container>.result-card .search-highlight{background-color:#afbfc966;padding:.05em .2em;border-radius:3px}.section-ul{list-style:none;padding-left:0}.section-ul>li{border:1px solid var(--outlinegray);border-radius:5px;padding:0 1em;margin-bottom:1em}.section-ul>li h3{opacity:1;font-weight:700;margin-bottom:0}.section-ul>li .meta{opacity:.6}.popover{z-index:999;position:absolute;width:20em;display:inline-block;visibility:hidden;background-color:var(--light);padding:1em;border:1px solid var(--outlinegray);border-radius:5px;transform:translate(-50%,40%);opacity:0;pointer-events:none;transition:opacity .2s ease,transform .2s ease;transition-delay:.3s;user-select:none}@media all and (max-width:600px){.popover{display:none}}.popover.visible{opacity:1;visibility:visible;transform:translate(-50%,20%)}.popover>h3{font-size:1rem;margin:.25em 0}.popover>.meta{margin-top:.25em;opacity:.5}.popover>p{margin:0;font-weight:400;user-select:none}</style><style>.darkmode{float:right;padding:1em;min-width:30px;position:relative}@media all and (max-width:450px){.darkmode{padding:1em}}.darkmode>.toggle{display:none;box-sizing:border-box}.darkmode svg{opacity:0;position:absolute;width:20px;height:20px;top:calc(50% - 10px);margin:0 7px;fill:var(--gray);transition:opacity .1s ease}.toggle:checked~label>#dayIcon{opacity:0}.toggle:checked~label>#nightIcon{opacity:1}.toggle:not(:checked)~label>#dayIcon{opacity:1}.toggle:not(:checked)~label>#nightIcon{opacity:0}</style><style>.chroma{color:#f8f8f2;background-color:#282a36;overflow:hidden}.chroma .lntd{vertical-align:top;padding:0;margin:0;border:0}.chroma .lntable{border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block}.chroma .hl{display:block;width:100%;background-color:#ffc}.chroma .lnt{margin-right:.4em;padding:0 .4em;color:#7f7f7f}.chroma .ln{margin-right:.4em;padding:0 .4em;color:#7f7f7f}.chroma .k{color:#ff79c6}.chroma .kc{color:#ff79c6}.chroma .kd{color:#8be9fd;font-style:italic}.chroma .kn{color:#ff79c6}.chroma .kp{color:#ff79c6}.chroma .kr{color:#ff79c6}.chroma .kt{color:#8be9fd}.chroma .na{color:#50fa7b}.chroma .nb{color:#8be9fd;font-style:italic}.chroma .nc{color:#50fa7b}.chroma .nf{color:#50fa7b}.chroma .nl{color:#8be9fd;font-style:italic}.chroma .nt{color:#ff79c6}.chroma .nv{color:#8be9fd;font-style:italic}.chroma .vc{color:#8be9fd;font-style:italic}.chroma .vg{color:#8be9fd;font-style:italic}.chroma .vi{color:#8be9fd;font-style:italic}.chroma .s{color:#f1fa8c}.chroma .sa{color:#f1fa8c}.chroma .sb{color:#f1fa8c}.chroma .sc{color:#f1fa8c}.chroma .dl{color:#f1fa8c}.chroma .sd{color:#f1fa8c}.chroma .s2{color:#f1fa8c}.chroma .se{color:#f1fa8c}.chroma .sh{color:#f1fa8c}.chroma .si{color:#f1fa8c}.chroma .sx{color:#f1fa8c}.chroma .sr{color:#f1fa8c}.chroma .s1{color:#f1fa8c}.chroma .ss{color:#f1fa8c}.chroma .m{color:#bd93f9}.chroma .mb{color:#bd93f9}.chroma .mf{color:#bd93f9}.chroma .mh{color:#bd93f9}.chroma .mi{color:#bd93f9}.chroma .il{color:#bd93f9}.chroma .mo{color:#bd93f9}.chroma .o{color:#ff79c6}.chroma .ow{color:#ff79c6}.chroma .c{color:#6272a4}.chroma .ch{color:#6272a4}.chroma .cm{color:#6272a4}.chroma .c1{color:#6272a4}.chroma .cs{color:#6272a4}.chroma .cp{color:#ff79c6}.chroma .cpf{color:#ff79c6}.chroma .gd{color:#8b080b}.chroma .ge{text-decoration:underline}.chroma .gh{font-weight:700}.chroma .gi{font-weight:700}.chroma .go{color:#44475a}.chroma .gu{font-weight:700}.chroma .gl{text-decoration:underline}.lntd:first-of-type>.chroma{padding-right:0}.chroma code{font-family:fira code!important;font-size:.85em;line-height:1em;background:0 0;padding:0}.chroma{border-radius:3px;margin:0}</style><style>:root{--light:#faf8f8;--dark:#141021;--secondary:#284b63;--tertiary:#84a59d;--visited:#afbfc9;--primary:#f28482;--gray:#4e4e4e;--lightgray:#f0f0f0;--outlinegray:#dadada}[saved-theme=dark]{--light:#1e1e21 !important;--dark:#fbfffe !important;--secondary:#6b879a !important;--visited:#4a575e !important;--tertiary:#84a59d !important;--primary:#f58382 !important;--gray:#d4d4d4 !important;--lightgray:#292633 !important;--outlinegray:#343434 !important}</style><script>const userPref=window.matchMedia('(prefers-color-scheme: light)').matches?'light':'dark',currentTheme=localStorage.getItem('theme')??userPref;currentTheme&&document.documentElement.setAttribute('saved-theme',currentTheme);const switchTheme=a=>{a.target.checked?(document.documentElement.setAttribute('saved-theme','dark'),localStorage.setItem('theme','dark')):(document.documentElement.setAttribute('saved-theme','light'),localStorage.setItem('theme','light'))};window.addEventListener('DOMContentLoaded',()=>{const a=document.querySelector('#darkmode-toggle');a.addEventListener('change',switchTheme,!1),currentTheme==='dark'&&(a.checked=!0)})</script><script>let saved=!1;const fetchData=async()=>{if(saved)return saved;const promises=[fetch("https://carlgogo.github.io/AIDigitalGarden/linkIndex.json").then(a=>a.json()).then(a=>({index:a.index,links:a.links})),fetch("https://carlgogo.github.io/AIDigitalGarden/contentIndex.json").then(a=>a.json())],[{index,links},content]=await Promise.all(promises),res={index,links,content};return saved=res,res};fetchData()</script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-XYFD95KB4J"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-XYFD95KB4J',{anonymize_ip:!1})}</script><script>function htmlToElement(a){const b=document.createElement('template');return a=a.trim(),b.innerHTML=a,b.content.firstChild}const baseUrl="https://carlgogo.github.io/AIDigitalGarden".replace(window.location.origin,"");document.addEventListener("DOMContentLoaded",()=>{fetchData().then(({content:a})=>{const b=[...document.getElementsByClassName("internal-link")];b.forEach(b=>{const c=a[b.dataset.src.replace(baseUrl,"")];if(c){const d=`<div class="popover">
    <h3>${c.title}</h3>
    <p>${removeMarkdown(c.content).split(" ",20).join(" ")}...</p>
    <p class="meta">${new Date(c.lastmodified).toLocaleDateString()}</p>
</div>`,a=htmlToElement(d);b.appendChild(a),b.addEventListener("mouseover",()=>{a.classList.add("visible")}),b.addEventListener("mouseout",()=>{a.classList.remove("visible")})}})})})</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/gh/nextapps-de/flexsearch@0.7.2/dist/flexsearch.bundle.js></script><script>const removeMarkdown=(c,b={listUnicodeChar:!1,stripListLeaders:!0,gfm:!0,useImgAltText:!1,preserveLinks:!1})=>{let a=c||"";a=a.replace(/^(-\s*?|\*\s*?|_\s*?){3,}\s*$/gm,"");try{b.stripListLeaders&&(b.listUnicodeChar?a=a.replace(/^([\s\t]*)([\*\-\+]|\d+\.)\s+/gm,b.listUnicodeChar+" $1"):a=a.replace(/^([\s\t]*)([\*\-\+]|\d+\.)\s+/gm,"$1")),b.gfm&&(a=a.replace(/\n={2,}/g,"\n").replace(/~{3}.*\n/g,"").replace(/~~/g,"").replace(/`{3}.*\n/g,"")),b.preserveLinks&&(a=a.replace(/\[(.*?)\][\[\(](.*?)[\]\)]/g,"$1 ($2)")),a=a.replace(/<[^>]*>/g,"").replace(/^[=\-]{2,}\s*$/g,"").replace(/\[\^.+?\](\: .*?$)?/g,"").replace(/\s{0,2}\[.*?\]: .*?$/g,"").replace(/\!\[(.*?)\][\[\(].*?[\]\)]/g,b.useImgAltText?"$1":"").replace(/\[(.*?)\][\[\(].*?[\]\)]/g,"$1").replace(/^\s{0,3}>\s?/g,"").replace(/(^|\n)\s{0,3}>\s?/g,"\n\n").replace(/^\s{1,2}\[(.*?)\]: (\S+)( ".*?")?\s*$/g,"").replace(/^(\n)?\s{0,}#{1,6}\s+| {0,}(\n)?\s{0,}#{0,} {0,}(\n)?\s{0,}$/gm,"$1$2$3").replace(/([\*_]{1,3})(\S.*?\S{0,1})\1/g,"$2").replace(/([\*_]{1,3})(\S.*?\S{0,1})\1/g,"$2").replace(/(`{3,})(.*?)\1/gm,"$2").replace(/`(.+?)`/g,"$1").replace(/\n{2,}/g,"\n\n")}catch(a){return console.error(a),c}return a}</script><script>async function run(){const g=new FlexSearch.Document({cache:!0,charset:"latin:extra",optimize:!0,worker:!0,document:{index:[{field:"content",tokenize:"strict",context:{resolution:5,depth:3,bidirectional:!0},suggest:!0},{field:"title",tokenize:"forward"}]}}),{content:d}=await fetchData();for(const[b,a]of Object.entries(d))g.add({id:b,title:a.title,content:removeMarkdown(a.content)});const j=(i,j)=>{const a=20,k=j.split(/\s+/).filter(a=>a!==""),b=i.split(/\s+/).filter(a=>a!==""),f=a=>k.some(b=>a.toLowerCase().startsWith(b.toLowerCase())),d=b.map(f);let e=0,g=0;for(let b=0;b<Math.max(d.length-a,0);b++){const f=d.slice(b,b+a),c=f.reduce((a,b)=>a+b,0);c>=e&&(e=c,g=b)}const c=Math.max(g-a,0),h=Math.min(c+2*a,b.length),l=b.slice(c,h).map(a=>{return f(a)?`<span class="search-highlight">${a}</span>`:a}).join(" ").replaceAll('</span> <span class="search-highlight">'," ");return`${c===0?"":"..."}${l}${h===b.length?"":"..."}`},m=({url:b,title:c,content:d,term:a})=>{const e=removeMarkdown(d),f=j(c,a),g=j(e,a);return`<button class="result-card" id="${b}">
        <h3>${f}</h3>
        <p>${g}</p>
    </button>`},h=(a,b)=>{window.location.href="https://carlgogo.github.io/AIDigitalGarden"+`${a}#:~:text=${encodeURIComponent(b)}`},l=a=>({id:a,url:a,title:d[a].title,content:d[a].content}),c=document.getElementById('search-bar'),e=document.getElementById("results-container");let b;c.addEventListener("keyup",a=>{if(a.key==="Enter"){const a=document.getElementsByClassName("result-card")[0];h(a.id,b)}}),c.addEventListener('input',a=>{b=a.target.value,g.search(b,[{field:"content",limit:10,suggest:!0},{field:"title",limit:5}]).then(d=>{const a=b=>{const a=d.filter(a=>a.field===b);return a.length===0?[]:[...a[0].result]},f=new Set([...a('title'),...a('content')]),c=[...f].map(l);if(c.length===0)e.innerHTML=`<button class="result-card">
                    <h3>No results.</h3>
                    <p>Try another search term?</p>
                </button>`;else{e.innerHTML=c.map(a=>m({...a,term:b})).join("\n");const a=document.getElementsByClassName("result-card");[...a].forEach(a=>{a.onclick=()=>h(a.id,b)})}})});const a=document.getElementById("search-container");function f(){a.style.display==="none"||a.style.display===""?(c.value="",e.innerHTML="",a.style.display="block",c.focus()):a.style.display="none"}function i(){a.style.display="none"}document.addEventListener('keydown',a=>{a.key==="/"&&(a.preventDefault(),f()),a.key==="Escape"&&(a.preventDefault(),i())});const k=document.getElementById("search-icon");k.addEventListener('click',a=>{f()}),k.addEventListener('keydown',a=>{f()}),a.addEventListener('click',a=>{i()}),document.getElementById("search-space").addEventListener('click',a=>{a.stopPropagation()})}run()</script><div class=singlePage><header><h1 id=page-title><a href=https://carlgogo.github.io/AIDigitalGarden>ðŸª´ AI and DS Digital Garden</a></h1><svg tabindex="0" id="search-icon" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg><div class=spacer></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><p class=meta>Last updated March 7, 2022</p><ul class=tags></ul><aside class=mainTOC><h3>Table of Contents</h3><nav id=TableOfContents><ol><li><a href=#code>Code</a></li><li><a href=#review-papers>Review Papers</a></li><li><a href=#cnn-based>CNN-based</a></li><li><a href=#gan-based>GAN-based</a></li><li><a href=#transformer-based>Transformer-based</a></li></ol></nav></aside><h1 id=super-resolution>Super-resolution</h1><p>See [[Image-to-image translation]]</p><ul><li><a href=https://github.com/ptkin/Awesome-Super-Resolution>https://github.com/ptkin/Awesome-Super-Resolution</a></li><li><a href=https://github.com/ChaofWang/Awesome-Super-Resolution>https://github.com/ChaofWang/Awesome-Super-Resolution</a></li><li><a href=https://keras.io/examples/vision/super_resolution_sub_pixel/>https://keras.io/examples/vision/super_resolution_sub_pixel/</a></li><li>Image Super-Resolution: A Comprehensive Review (2020): <a href=https://blog.paperspace.com/image-super-resolution/>https://blog.paperspace.com/image-super-resolution/</a></li><li>#TALK How Super Resolution Works (2019): <a href="https://www.youtube.com/watch?v=KULkSwLk62I">https://www.youtube.com/watch?v=KULkSwLk62I</a></li><li>#TALK Can you enhance that? Single Image Super Resolution (Pydata 2019): <a href="https://www.youtube.com/watch?v=lmUxbRY7H2I">https://www.youtube.com/watch?v=lmUxbRY7H2I</a></li></ul><h2 id=code>Code</h2><ul><li>#CODE BasicSR: Open Source Image and Video Restoration Toolbox for Super-resolution, Denoise, Deblurring (Pytorch). <a href=https://github.com/xinntao/BasicSR>https://github.com/xinntao/BasicSR</a><ul><li>It includes EDSR, RCAN, SRResNet, SRGAN, ESRGAN, EDVR, etc</li></ul></li><li>#CODE Single Image Super Resolution benchmark (Keras): <a href=https://github.com/hieubkset/Keras-Image-Super-Resolution>https://github.com/hieubkset/Keras-Image-Super-Resolution</a><ul><li>EDSR, SRGAN, SRFeat, RCAN, ESRGAN and ERCA (not published)</li></ul></li><li>#CODE Single Image Super-Resolution with EDSR, WDSR and SRGAN (Keras): <a href=https://github.com/krasserm/super-resolution>https://github.com/krasserm/super-resolution</a><ul><li><a href=http://krasserm.github.io/2019/09/04/super-resolution/>http://krasserm.github.io/2019/09/04/super-resolution/</a></li></ul></li></ul><h2 id=review-papers>Review Papers</h2><ul><li>#PAPER Deep Learning for Single Image Super-Resolution: A Brief Review (2018): <a href=https://arxiv.org/abs/1808.03344>https://arxiv.org/abs/1808.03344</a></li><li>#PAPER A Deep Journey into Super-resolution: A survey (Anwar 2020): <a href=https://arxiv.org/abs/1904.07523>https://arxiv.org/abs/1904.07523</a><ul><li><a href=https://github.com/saeed-anwar/SRsurvey>https://github.com/saeed-anwar/SRsurvey</a></li></ul></li><li>#PAPER Deep Learning for Image Super-resolution: A Survey (Wang 2020): <a href=https://arxiv.org/abs/1902.06068>https://arxiv.org/abs/1902.06068</a></li><li>#PAPER NTIRE 2020 Challenge on Perceptual Extreme Super-Resolution: Methods and Results (Zhang 2020): <a href=https://arxiv.org/abs/2005.01056>https://arxiv.org/abs/2005.01056</a><ul><li><a href=https://data.vision.ee.ethz.ch/cvl/ntire20/>https://data.vision.ee.ethz.ch/cvl/ntire20/</a></li><li>Jointly with NTIRE 2020 workshop we have an NTIRE challenge on perceptual extreme super-resolution, that is,the task of super-resolving an LR image to a perceptually pleasant HR image with a magnification factor x16</li></ul></li><li>#PAPER A Comprehensive Review of Deep Learning-based Single Image Super-resolution (Bashir 2021): <a href=https://arxiv.org/abs/2102.09351>https://arxiv.org/abs/2102.09351</a></li></ul><h2 id=cnn-based>CNN-based</h2><ul><li>#PAPER Image Super-Resolution Using Deep Convolutional Networks, SRCNN (Dong 2015): <a href=https://arxiv.org/abs/1501.00092>https://arxiv.org/abs/1501.00092</a><ul><li>#CODE <a href=https://github.com/MarkPrecursor/SRCNN-keras>https://github.com/MarkPrecursor/SRCNN-keras</a></li><li>#CODE <a href=https://github.com/yukia18/srcnn-keras>https://github.com/yukia18/srcnn-keras</a></li></ul></li><li>#PAPER Accurate Image Super-Resolution Using Very Deep Convolutional Networks (2015): <a href=http://arxiv.org/abs/1511.04587>http://arxiv.org/abs/1511.04587</a></li><li>#PAPER Deep Networks for Image Super-Resolution with Sparse Prior (Wang 2015): <a href=http://www.ifp.illinois.edu/~dingliu2/iccv15/>http://www.ifp.illinois.edu/~dingliu2/iccv15/</a><ul><li><a href=http://www.ifp.illinois.edu/~dingliu2/iccv15/iccv15.pdf>http://www.ifp.illinois.edu/~dingliu2/iccv15/iccv15.pdf</a></li></ul></li><li>#PAPER FSRCNN - Accelerating the Super-Resolution Convolutional Neural Network (Dong 2016): <a href=https://arxiv.org/abs/1608.00367>https://arxiv.org/abs/1608.00367</a><ul><li><a href=http://mmlab.ie.cuhk.edu.hk/projects/FSRCNN.html>http://mmlab.ie.cuhk.edu.hk/projects/FSRCNN.html</a></li><li>Uses deconvolution layers (transposed convolution)</li><li>#CODE <a href=https://github.com/GeorgeSeif/FSRCNN-Keras>https://github.com/GeorgeSeif/FSRCNN-Keras</a></li></ul></li><li>#PAPER Deconvolution and Checkerboard Artifacts (Odena 2016): <a href=https://distill.pub/2016/deconv-checkerboard/>https://distill.pub/2016/deconv-checkerboard/</a><ul><li>Identifies the learned upsample operation (often called deconvolutions) in generative networks as a source of noise</li><li>Overall lesson here is that if you use transposed convolutions, be careful that your kernel size is a multiple of your stride</li><li>However if you use a nearest neighbor or bilinear upsample approach followed by a convolution (termed the â€˜resize convolutionâ€™) checkerboard artifacts should not appear</li><li>They have more succes with nearest neighbor than with bilinear, possibly because bilinear upsampling smooths away important high frequency signals</li></ul></li><li>#PAPER Perceptual Losses for Real-Time Style Transfer and Super-Resolution (Johnson 2016): <a href=http://arxiv.org/abs/1603.08155>http://arxiv.org/abs/1603.08155</a><ul><li><a href=http://cs.stanford.edu/people/jcjohns/papers/fast-style/fast-style-supp.pdf>http://cs.stanford.edu/people/jcjohns/papers/fast-style/fast-style-supp.pdf</a></li></ul></li><li>#PAPER ESPCN - Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network (Shi 2016): <a href=https://arxiv.org/abs/1609.05158>https://arxiv.org/abs/1609.05158</a><ul><li><a href=https://medium.datadriveninvestor.com/review-espcn-real-time-sr-super-resolution-8dceca249350>https://medium.datadriveninvestor.com/review-espcn-real-time-sr-super-resolution-8dceca249350</a></li><li>#CODE <a href=https://keras.io/examples/vision/super_resolution_sub_pixel/>https://keras.io/examples/vision/super_resolution_sub_pixel/</a></li><li>SubPixelUpscaling implementation here: <a href=https://github.com/pavitrakumar78/Anime-Face-GAN-Keras/blob/master/misc_layers.py>https://github.com/pavitrakumar78/Anime-Face-GAN-Keras/blob/master/misc_layers.py</a></li><li>Subpixel convolution is the same as pixel-shuffle: <a href=https://nico-curti.github.io/NumPyNet/NumPyNet/layers/pixelshuffle_layer.html>https://nico-curti.github.io/NumPyNet/NumPyNet/layers/pixelshuffle_layer.html</a></li><li>A drawback of the interpolation upsampling is that upsampling errors are introduced that can be hard to correct sub-sequently</li><li>The idea of pixel shuffling is to rearrange the pixels of multiple low-resolution images, or in this case feature activations, to one high-resolution out-put image by periodic shuffling of the image points. It thus represents a learnable upsampling operation</li><li>Through the constant periodicity, the previous operations of the neural network can learn to distribute content across the feature dimension which is then shuffled to yield the high-resolution output</li><li>This allows to process the image entirely in low-resolution space</li></ul></li><li>#PAPER Checkerboard artifact free sub-pixel convolution: A note on sub-pixel convolution, resize convolution and convolution resize (Aitken 2017): <a href=https://arxiv.org/abs/1707.02937>https://arxiv.org/abs/1707.02937</a><ul><li>#CODE <a href=https://github.com/Golbstein/EDSR-Keras/blob/master/subpixel.py>https://github.com/Golbstein/EDSR-Keras/blob/master/subpixel.py</a></li></ul></li><li>#PAPER EDSR - Enhanced Deep Residual Networks for Single Image Super-Resolution (Lim 2017): <a href=https://arxiv.org/abs/1707.02921>https://arxiv.org/abs/1707.02921</a><ul><li>#CODE <a href=https://github.com/Golbstein/EDSR-Keras>https://github.com/Golbstein/EDSR-Keras</a></li><li>#CODE <a href=https://github.com/hieubkset/Keras-Image-Super-Resolution>https://github.com/hieubkset/Keras-Image-Super-Resolution</a></li></ul></li><li>#PAPER Pixel Deconvolutional Networks (Gao 2017): <a href=https://arxiv.org/abs/1705.06820>https://arxiv.org/abs/1705.06820</a></li><li>#PAPER RDN - Residual Dense Network for Image Super-Resolution (Zhang 2018): <a href=https://arxiv.org/abs/1802.08797>https://arxiv.org/abs/1802.08797</a><ul><li>#CODE <a href=https://github.com/idealo/image-super-resolution>https://github.com/idealo/image-super-resolution</a></li><li>#CODE <a href=https://github.com/hengchuan/RDN-TensorFlow>https://github.com/hengchuan/RDN-TensorFlow</a></li></ul></li><li>#PAPER WDSR - Wide Activation for Efficient and Accurate ImageSuper-Resolution (Yu 2018): <a href=https://arxiv.org/abs/1808.08718>https://arxiv.org/abs/1808.08718</a></li><li>#PAPER RecResNet: A Recurrent Residual CNN Architecture for Disparity Map Enhancement (Batsos 2018): <a href=https://ieeexplore.ieee.org/document/8490974>https://ieeexplore.ieee.org/document/8490974</a><ul><li><a href=https://mordohai.github.io/public/Batsos_RecResNet18.pdf>https://mordohai.github.io/public/Batsos_RecResNet18.pdf</a></li><li>#CODE <a href=https://github.com/kbatsos/RecResNet>https://github.com/kbatsos/RecResNet</a></li></ul></li><li>#PAPER RCAN - Image Super-Resolution Using Very Deep Residual Channel Attention Networks (Zhang 2018): <a href=https://arxiv.org/abs/1807.02758>https://arxiv.org/abs/1807.02758</a><ul><li>#CODE <a href=https://github.com/yulunzhang/RCAN>https://github.com/yulunzhang/RCAN</a></li></ul></li><li>#PAPER Fast and Accurate Image Super-Resolution with Deep Laplacian Pyramid Networks (Lai 2018): <a href=http://vllab.ucmerced.edu/wlai24/LapSRN/>http://vllab.ucmerced.edu/wlai24/LapSRN/</a></li><li>#PAPER Super-Resolution using Convolutional Neural Networks without Any Checkerboard Artifacts (Sugawara 2018): <a href=https://arxiv.org/abs/1806.02658v1>https://arxiv.org/abs/1806.02658v1</a></li><li>#PAPER Single Image Super Resolution based on a Modified U-net with Mixed Gradient Loss (Lu, 2019): <a href=https://arxiv.org/abs/1911.09428>https://arxiv.org/abs/1911.09428</a></li><li>#PAPER Densely Residual Laplacian Super-Resolution (Anwar 2019): <a href=https://arxiv.org/abs/1906.12021>https://arxiv.org/abs/1906.12021</a><ul><li>#CODE <a href=https://github.com/saeed-anwar/DRLN>https://github.com/saeed-anwar/DRLN</a></li></ul></li><li>#PAPER Hyperspectral Image Super-Resolution with 1Dâ€“2D Attentional Convolutional Neural Network (Li 2019): <a href=https://www.mdpi.com/2072-4292/11/23/2859/htm>https://www.mdpi.com/2072-4292/11/23/2859/htm</a></li><li>#PAPER Deep Learning for Multiple-Image Super-Resolution (Kawulok 2019): <a href=https://arxiv.org/abs/1903.00440>https://arxiv.org/abs/1903.00440</a></li><li>#PAPER RUNet: A Robust UNet Architecture for Image Super-Resolution (Hu 2019): <a href=https://openaccess.thecvf.com/content_CVPRW_2019/papers/WiCV/Hu_RUNet_A_Robust_UNet_Architecture_for_Image_Super-Resolution_CVPRW_2019_paper.pdf>https://openaccess.thecvf.com/content_CVPRW_2019/papers/WiCV/Hu_RUNet_A_Robust_UNet_Architecture_for_Image_Super-Resolution_CVPRW_2019_paper.pdf</a></li><li>#PAPER Learned Image Downscaling for Upscaling using Content Adaptive Resampler (Sun 2019): <a href=https://arxiv.org/abs/1907.12904>https://arxiv.org/abs/1907.12904</a><ul><li>#CODE <a href=https://github.com/sunwj/CAR>https://github.com/sunwj/CAR</a></li><li><a href=https://paperswithcode.com/paper/learned-image-downscaling-for-upscaling-using>https://paperswithcode.com/paper/learned-image-downscaling-for-upscaling-using</a></li><li>The proposed resampler network generates content adaptive image resampling kernels that are applied to the original HR input to generate pixels on the downscaled image</li><li>Moreover, a differentiable upscaling (SR) module is employed to upscale the LR result into its underlying HR counterpart</li><li>By back-propagating the reconstruction error down to the original HR input across the entire framework to adjust model parameters, the proposed framework achieves a new state-of-the-art SR performance through upscaling guided image resamplers which adaptively preserve detailed information that is essential to the upscaling</li></ul></li><li>#PAPER Image Super-Resolution Using Attention Based DenseNet with Residual Deconvolution (Li 2019): <a href=https://arxiv.org/abs/1907.05282>https://arxiv.org/abs/1907.05282</a></li><li>#PAPER Meta-SR: A Magnification-Arbitrary Network for Super-Resolution (Hu 2019): <a href=https://arxiv.org/abs/1903.00875>https://arxiv.org/abs/1903.00875</a><ul><li>#CODE <a href=https://github.com/XuecaiHu/Meta-SR-Pytorch>https://github.com/XuecaiHu/Meta-SR-Pytorch</a></li><li>#CODE <a href=https://github.com/smallsunsun1/Meta-SR/>https://github.com/smallsunsun1/Meta-SR/</a></li><li>#CODE <a href=https://github.com/jason71995/meta_sr/>https://github.com/jason71995/meta_sr/</a></li><li>Continuous, arbitrary scaling</li></ul></li><li>#PAPER Pixel Transposed Convolutional Networks (Gao 2019): <a href=https://ieeexplore.ieee.org/document/8618415>https://ieeexplore.ieee.org/document/8618415</a><ul><li>The pixel transposed convolutional layer (PixelTCL) is proposed to establish direct relationships among adjacent pixels on the up-sampled feature map</li><li>PixelTCL can largely overcome the checkerboard problem suffered by regular transposed convolutional operations</li></ul></li><li>#PAPER A Very Deep Spatial Transformer Towards Robust Single Image Super-Resolution (Jiang 2019): <a href=https://ieeexplore.ieee.org/abstract/document/8679959>https://ieeexplore.ieee.org/abstract/document/8679959</a></li><li>#PAPER ASDN: A Deep Convolutional Network for Arbitrary Scale Image Super-Resolution (Shen 2020): <a href=https://arxiv.org/abs/2010.02414v1>https://arxiv.org/abs/2010.02414v1</a></li><li>#PAPER LIIF - Learning Continuous Image Representation with Local Implicit Image Function (Chen 2020): <a href=https://arxiv.org/abs/2012.09161>https://arxiv.org/abs/2012.09161</a><ul><li><a href=https://yinboc.github.io/liif/>https://yinboc.github.io/liif/</a></li><li>#CODE <a href=https://github.com/yinboc/liif>https://github.com/yinboc/liif</a></li><li>Continuous, arbitrary scaling</li></ul></li><li>#PAPER Fixed smooth convolutional layer for avoiding checkerboard artifacts in CNNs (Kinoshita 2020): <a href=https://arxiv.org/abs/2002.02117v1>https://arxiv.org/abs/2002.02117v1</a></li><li>#PAPER Efficient Image Super-Resolution Using Pixel Attention (Zhao 2020): <a href=https://arxiv.org/abs/2010.01073>https://arxiv.org/abs/2010.01073</a> ^srwithpixelattention<ul><li>#CODE [[CNNs#^tfvisualattention]]</li><li>#CODE <a href=https://github.com/zhaohengyuan1/PAN>https://github.com/zhaohengyuan1/PAN</a></li></ul></li><li>#PAPER Dense U-net for super-resolution with shuffle pooling layer (Lu 2021): <a href=https://arxiv.org/abs/2011.05490>https://arxiv.org/abs/2011.05490</a></li><li>#PAPER OverNet: Lightweight Multi-Scale Super-Resolution with Overscaling Network (Behjati 2021): <a href=https://arxiv.org/abs/2008.02382>https://arxiv.org/abs/2008.02382</a><ul><li><a href="https://www.youtube.com/watch?v=_YAn5TaIJfM">https://www.youtube.com/watch?v=_YAn5TaIJfM</a></li></ul></li><li>#PAPER Image Super-Resolution via Iterative Refinement (Saharia 2021): <a href=https://arxiv.org/abs/2104.07636>https://arxiv.org/abs/2104.07636</a><ul><li><a href=https://iterative-refinement.github.io/>https://iterative-refinement.github.io/</a></li><li>SR3 is inspired by recent work on Denoising Diffusion Probabilistic Models (DDPM) and denoising score matching</li><li>SR3 adapts denoising diffusion probabilistic models to conditional image generation and performs super-resolution through a stochastic denoising process</li><li>Inference starts with pure Gaussian noise and iteratively refines the noisy output using a U-Net model trained on denoising at various noise levels</li><li>#CODE <a href=https://github.com/Janspiry/Image-Super-Resolution-via-Iterative-Refinement>https://github.com/Janspiry/Image-Super-Resolution-via-Iterative-Refinement</a></li></ul></li></ul><h2 id=gan-based>GAN-based</h2><ul><li>#PAPER SRGAN: Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network (Ledig 2016): <a href=https://arxiv.org/abs/1609.04802>https://arxiv.org/abs/1609.04802</a><ul><li>#CODE <a href=https://github.com/idealo/image-super-resolution>https://github.com/idealo/image-super-resolution</a></li><li>#CODE <a href=https://github.com/tensorlayer/srgan>https://github.com/tensorlayer/srgan</a></li><li>#CODE <a href=https://github.com/leftthomas/SRGAN>https://github.com/leftthomas/SRGAN</a></li><li>#TALK <a href="https://www.youtube.com/watch?v=BXIR_SVCrsE">https://www.youtube.com/watch?v=BXIR_SVCrsE</a></li><li>First proposed the perceptual loss: content loss + adversarial loss<ul><li>content loss ensures high-level content is preserved by computing the MSE in the VGG feature-space (instead of pixel image space)</li><li>adversarial loss ensures the reconstructed images look real (textures detail)</li></ul></li><li>Model based on VGG architecture and DCGAN</li></ul></li><li>#PAPER Class-Conditional Superresolution with GANs (Chen 2017): <a href=http://cs231n.stanford.edu/reports/2017/pdfs/314.pdf>http://cs231n.stanford.edu/reports/2017/pdfs/314.pdf</a><ul><li>#CODE <a href=https://github.com/vincentschen/cgan-superres>https://github.com/vincentschen/cgan-superres</a></li></ul></li><li>#PAPER ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks (Wang 2018): <a href=https://arxiv.org/abs/1809.00219>https://arxiv.org/abs/1809.00219</a>
- #CODE <a href=https://github.com/xinntao/ESRGAN>https://github.com/xinntao/ESRGAN</a></li><li>#PAPER tempoGAN: A temporally coherent, volumetric GAN for super-resolution fluid flow (Xie 2018): <a href=https://arxiv.org/abs/1801.09710>https://arxiv.org/abs/1801.09710</a></li><li>#PAPER Unsupervised Single-Image Super-Resolution with Multi-Gram Loss (Shi 2019): <a href=https://www.mdpi.com/2079-9292/8/8/833/htm>https://www.mdpi.com/2079-9292/8/8/833/htm</a></li><li>#PAPER TecoGAN: Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation (Chu 2020): <a href=https://ge.in.tum.de/publications/2019-tecogan-chu/>https://ge.in.tum.de/publications/2019-tecogan-chu/</a><ul><li>#CODE <a href=https://github.com/thunil/TecoGAN>https://github.com/thunil/TecoGAN</a></li><li>Paper explained: <a href="https://www.youtube.com/watch?v=MwCgvYtOLS0">https://www.youtube.com/watch?v=MwCgvYtOLS0</a></li></ul></li><li>#PAPER TSRGAN: Generative Adversarial Network for Image Super-Resolution Combining Texture Loss (Jiang 2020): <a href=https://www.mdpi.com/2076-3417/10/5/1729/htm>https://www.mdpi.com/2076-3417/10/5/1729/htm</a></li><li>#PAPER Residual Channel Attention Generative Adversarial Network for Image Super-Resolution and Noise Reduction (Cai 2020): <a href=https://arxiv.org/abs/2004.13674>https://arxiv.org/abs/2004.13674</a></li><li>#PAPER Meta-SRGAN - Arbitrary Scale Super-Resolution for Brain MRI Images (Tan 2020): <a href=https://arxiv.org/abs/2004.02086>https://arxiv.org/abs/2004.02086</a><ul><li>#CODE <a href=https://github.com/pancakewaffles/metasrgan-tutorial/>https://github.com/pancakewaffles/metasrgan-tutorial/</a></li></ul></li><li>#PAPER MSG-GAN: Multi-Scale Gradients for Generative Adversarial Networks (Karnewar 2020): <a href=https://arxiv.org/abs/1903.06048>https://arxiv.org/abs/1903.06048</a></li><li>#PAPER MRI Super-Resolution with GAN and 3D Multi-Level DenseNet: Smaller, Faster, and Better (Chen 2020): <a href=https://arxiv.org/abs/2003.01217>https://arxiv.org/abs/2003.01217</a></li><li>#PAPER Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data (Wnag 2021): <a href=https://arxiv.org/abs/2107.10833v1>https://arxiv.org/abs/2107.10833v1</a> ^real-esrgan<ul><li>#CODE <a href=https://github.com/xinntao/Real-ESRGAN>https://github.com/xinntao/Real-ESRGAN</a></li><li>Super-resolution with a hint of image restoration</li><li>Proposed a high-order degradation modeling process to better simulate complex real-world degradations (blur, downsampling, noise, etc and combinations)</li></ul></li></ul><h2 id=transformer-based>Transformer-based</h2><ul><li>#PAPER Learning Texture Transformer Network for Image Super-Resolution (Yang 2020): <a href=https://arxiv.org/abs/2006.04139>https://arxiv.org/abs/2006.04139</a> ^ttsr<ul><li>#CODE <a href=https://github.com/researchmm/TTSR>https://github.com/researchmm/TTSR</a></li><li>Texture Transformer Network for Image Super-Resolution (TTSR)</li><li>LR and Ref images are formulated as queries and keys in a transformer, respectively</li><li>The proposed texture transformer consists of a learnable texture extractor which learns a jointly feature embedding for further attention computation and two attention based modules which transfer HR textures from the Ref image.</li><li>Furthermore, the proposed texture transformer can be stacked in a cross-scale way with the proposed CSFI module to learn a more powerful feature representation</li></ul></li></ul></article><hr><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script>async function run(){const{index:a,links:p,content:u}=await fetchData(),j="https://carlgogo.github.io/AIDigitalGarden/AI/Computer-Vision/Super-resolution".replace("https://carlgogo.github.io/AIDigitalGarden",""),n=[{"/moc":"#4388cc"}];let h=-1;const m=a=>[...new Set(a.flatMap(a=>[a.source,a.target]))],e=new Set,f=[j||"/","__SENTINEL"];if(h>=0)while(h>=0&&f.length>0){const b=f.shift();if(b==="__SENTINEL")h--,f.push("__SENTINEL");else{e.add(b);const c=a.links[b]||[],d=a.backlinks[b]||[];f.push(...c.map(a=>a.target),...d.map(a=>a.source))}}else m(p).forEach(a=>e.add(a));const g={nodes:[...e].map(a=>({id:a})),links:p.filter(a=>e.has(a.source)&&e.has(a.target))},k=a=>{if(a.id===j||a.id==="/"&&j==="")return"var(--g-node-active)";for(const b of n){const c=Object.keys(b)[0],d=b[c];if(a.id.startsWith(c))return d}return"var(--g-node)"},l=c=>{function d(b,a){b.active||c.alphaTarget(1).restart(),a.fx=a.x,a.fy=a.y}function e(a,b){b.fx=a.x,b.fy=a.y}function f(b,a){b.active||c.alphaTarget(0),a.fx=null,a.fy=null}const a=!0,b=()=>{};return d3.drag().on("start",a?d:b).on("drag",a?e:b).on("end",a?f:b)},c=250,b=document.getElementById("graph-container").offsetWidth,i=d3.forceSimulation(g.nodes).force("charge",d3.forceManyBody().strength(-30)).force("link",d3.forceLink(g.links).id(a=>a.id)).force("center",d3.forceCenter()),d=d3.select('#graph-container').append('svg').attr('width',b).attr('height',c).attr("viewBox",[-b/2,-c/2,b,c]),t=!1;if(t){const a=[{Current:"var(--g-node-active)"},{Note:"var(--g-node)"},...n];a.forEach((a,e)=>{const f=Object.keys(a)[0],g=a[f];d.append("circle").attr("cx",-b/2+20).attr("cy",c/2-30*(e+1)).attr("r",6).style("fill",g),d.append("text").attr("x",-b/2+40).attr("y",c/2-30*(e+1)).text(f).style("font-size","15px").attr("alignment-baseline","middle")})}const r=d.append("g").selectAll("line").data(g.links).join("line").attr("class","link").attr("stroke","var(--g-link)").attr("stroke-width",2).attr("data-source",a=>a.source.id).attr("data-target",a=>a.target.id),s=d.append("g").selectAll("g").data(g.nodes).enter().append("g"),q=s.append("circle").attr("class","node").attr("id",a=>a.id).attr("r",b=>{const c=a.links[b.id]?.length||0,d=a.backlinks[b.id]?.length||0;return 3+(c+d)/4}).attr("fill",k).style("cursor","pointer").on("click",(b,a)=>{window.location.href="https://carlgogo.github.io/AIDigitalGarden"+decodeURI(a.id).replace(/\s+/g,'-')}).on("mouseover",function(g,b){d3.selectAll(".node").transition().duration(100).attr("fill","var(--g-node-inactive)");const d=m([...a.links[b.id]||[],...a.backlinks[b.id]||[]]),e=d3.selectAll(".node").filter(a=>d.includes(a.id)),c=b.id,f=d3.selectAll(".link").filter(a=>a.source.id===c||a.target.id===c);e.transition().duration(200).attr("fill",k),f.transition().duration(200).attr("stroke","var(--g-link-active)"),d3.select(this.parentNode).select("text").raise().transition().duration(200).style("opacity",1)}).on("mouseleave",function(d,b){d3.selectAll(".node").transition().duration(200).attr("fill",k);const a=b.id,c=d3.selectAll(".link").filter(b=>b.source.id===a||b.target.id===a);c.transition().duration(200).attr("stroke","var(--g-link)"),d3.select(this.parentNode).select("text").transition().duration(200).style("opacity",0)}).call(l(i)),o=s.append("text").attr("dx",12).attr("dy",".35em").text(a=>u[decodeURI(a.id).replace(/\s+/g,'-')]?.title||"Untitled").style("opacity",0).style("pointer-events","none").call(l(i)),v=!0;v&&d.call(d3.zoom().extent([[0,0],[b,c]]).scaleExtent([.25,4]).on("zoom",({transform:a})=>{r.attr("transform",a),q.attr("transform",a),o.attr("transform",a)})),i.on("tick",()=>{r.attr("x1",a=>a.source.x).attr("y1",a=>a.source.y).attr("x2",a=>a.target.x).attr("y2",a=>a.target.y),q.attr("cx",a=>a.x).attr("cy",a=>a.y),o.attr("x",a=>a.x).attr("y",a=>a.y)})}run()</script></div></div><div id=contact_buttons><footer><p>Made by Carlos Alberto Gomez Gonzalez using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2022</p><a href=/>Home</a>
<a href=https://carlgogo.github.io>Website</a><a href=https://github.com/carlgogo>Github</a></footer></div></div></body></html>