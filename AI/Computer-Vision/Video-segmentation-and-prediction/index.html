<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="See:
 [[AI/Deep learning/Encoder-decoder networks]] &ldquo;Deep learning for multi-dimensional data&rdquo; section in [[AI/Deep learning/DL]] [[AI/Deep learning/RNNs]]   Resources  Spatiotemporal classification and regression Hybrid convolutional and recurrent networks, 3dconv and related approaches  https://github."><title>Video segmentation and prediction</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://carlos-gg.github.io/digitalgarden//icon.png><link href=https://carlos-gg.github.io/digitalgarden/styles.b3e1e36b0403ac565c9392b3e23ef3b6.min.css rel=stylesheet><link href=https://carlos-gg.github.io/digitalgarden/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://carlos-gg.github.io/digitalgarden/js/darkmode.18b7c6dfe67ae3bf2317338cf4189144.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/util.9825137f5e7825e8553c68ce39ac9e44.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/popover.abe6a51cc7138c5dff00f151dd627ad1.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/code-title.b35124ad8db0ba37162b886afb711cbc.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/clipboard.c20857734e53a3fb733b7443879efa61.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const BASE_URL="https://carlos-gg.github.io/digitalgarden/",fetchData=Promise.all([fetch("https://carlos-gg.github.io/digitalgarden/indices/linkIndex.15aea4010da4689ebfb0d0bc0de36185.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://carlos-gg.github.io/digitalgarden/indices/contentIndex.bfd8ee348e31547f8eba56b56185a534.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://carlos-gg.github.io/digitalgarden",!0,!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!0;drawGraph("https://carlos-gg.github.io/digitalgarden",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/carlos-gg.github.io\/digitalgarden\/js\/router.9d4974281069e9ebb189f642ae1e3ca2.min.js"
    attachSPARouting(init, render)
  </script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-S103D50NQ0"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S103D50NQ0",{anonymize_ip:!1})}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://carlos-gg.github.io/digitalgarden/js/full-text-search.24827f874defbbc6d529926cbfcfb493.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://carlos-gg.github.io/digitalgarden/>CarlosGG's Knowledge Garden ðŸª´</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Video segmentation and prediction</h1><p class=meta>Last updated
Aug 21, 2022
<a href=https://github.com/jackyzha0/quartz/tree/hugo/content/AI/Computer%20Vision/Video%20segmentation%20and%20prediction.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details open><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#resources>Resources</a></li><li><a href=#courses>Courses</a></li><li><a href=#references>References</a></li></ol></nav></details></aside><blockquote><p>See:</p><ul><li><a href=/digitalgarden/AI/Deep-learning/Encoder-decoder-networks rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Encoder-decoder-networks>AI/Deep learning/Encoder-decoder networks</a></li><li>&ldquo;Deep learning for multi-dimensional data&rdquo; section in <a href=/digitalgarden/AI/Deep-learning/DL rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/DL>AI/Deep learning/DL</a></li><li><a href=/digitalgarden/AI/Deep-learning/RNNs rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/RNNs>AI/Deep learning/RNNs</a></li></ul></blockquote><a href=#resources><h2 id=resources><span class=hanchor arialabel=Anchor># </span>Resources</h2></a><ul><li>Spatiotemporal classification and regression</li><li>Hybrid convolutional and recurrent networks, 3dconv and related approaches</li><li><a href=https://github.com/jinwchoi/awesome-action-recognition rel=noopener>https://github.com/jinwchoi/awesome-action-recognition</a></li><li><a href=http://blog.qure.ai/notes/deep-learning-for-videos-action-recognition-review rel=noopener>http://blog.qure.ai/notes/deep-learning-for-videos-action-recognition-review</a></li><li><a href=https://stackoverflow.com/questions/55926841/convolving-across-channels-in-keras-cnn-conv1d-depthwise-separable-conv-cccp rel=noopener>https://stackoverflow.com/questions/55926841/convolving-across-channels-in-keras-cnn-conv1d-depthwise-separable-conv-cccp</a></li><li><a href=https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728 rel=noopener>https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728</a></li></ul><a href=#courses><h2 id=courses><span class=hanchor arialabel=Anchor># </span>Courses</h2></a><ul><li>#COURSE
<a href="https://www.youtube.com/watch?v=_aUq7lmMfxo&list=PLqYmG7hTraZCDxZ44o4p3N5Anz3lLRVZF&index=4" rel=noopener>Advanced Models for Computer Vision (DeepMind x UCL | Deep Learning Lectures)</a><ul><li><a href=https://storage.googleapis.com/deepmind-media/UCLxDeepMind_2020/L4---UCLxDeepMind-DL2020.pdf rel=noopener>https://storage.googleapis.com/deepmind-media/UCLxDeepMind_2020/L4%20-%20UCLxDeepMind%20DL2020.pdf</a></li></ul></li></ul><a href=#references><h2 id=references><span class=hanchor arialabel=Anchor># </span>References</h2></a><ul><li>#PAPER
<a href=https://arxiv.org/abs/1412.0767 rel=noopener>Learning Spatiotemporal Features with 3D Convolutional Networks. C3D, 3D CNNs (Tran 2015)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1502.04681 rel=noopener>Unsupervised Learning of Video Representations using LSTMs (Srivastava 2016)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1611.05435 rel=noopener>Convolutional Gated Recurrent Networks for Video Segmentation (Siam 2016)</a><ul><li>Hybrid convolutional and recurrent networks</li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1411.4389 rel=noopener>LRCN: Long-term Recurrent Convolutional Networks for Visual Recognition and Description (Donahue 2016)</a><ul><li>Hybrid convolutional and recurrent networks</li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1604.06573 rel=noopener>Convolutional Two-Stream Network Fusion for Video Action Recognition (Feichtenhofer 2016)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1605.08104 rel=noopener>Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning (Lotter 2016)</a><ul><li><a href=https://coxlab.github.io/prednet/ rel=noopener>https://coxlab.github.io/prednet/</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1710.08518 rel=noopener>ContextVP: Fully Context-Aware Video Prediction (Byeon 2018)</a><ul><li><a href=http://on-demand.gputechconf.com/gtc/2018/presentation/s8713-fully-context-aware-video-prediction.pdf rel=noopener>http://on-demand.gputechconf.com/gtc/2018/presentation/s8713-fully-context-aware-video-prediction.pdf</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1808.06865 rel=noopener>Machine Learning for Spatiotemporal Sequence Forecasting: A Survey (Shi, 2018)</a></li><li>#PAPER
<a href=https://dl.acm.org/doi/fullHtml/10.1145/3184558.3191571 rel=noopener>Residual Convolutional LSTM for Tweet Count Prediction (Wei 2018)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1711.11248 rel=noopener>A Closer Look at Spatiotemporal Convolutions for Action Recognition (Tran 2018)</a><ul><li>#CODE
<a href=https://github.com/facebookresearch/VMZ rel=noopener>https://github.com/facebookresearch/VMZ</a></li><li>#CODE See Ghadiyaram 2019 below</li><li>#CODE
<a href=https://github.com/juenkhaw/action_recognition_project rel=noopener>https://github.com/juenkhaw/action_recognition_project</a></li><li>demonstrate that 3D ResNets significantly outperform 2D ResNets for the same depth when trained and evaluated on large-scale,challenging action recognition benchmarks</li><li>introduce two new forms of spatio temporal convolution that can be viewed as middle grounds between the extremes of 2D (spatial convolution) and full 3D: mixed convolution (MC) and consists in employing 3D convolutions only in the early layers of the network, with 2D convolutions in the top layers, and the R(2+1)D spatiotemporal conv block which explicitly factorizes 3D convolution into two separate and successive operations, a 2D spatial convolution and a 1D temporal convolution</li><li>the first advantage is an additional nonlinear rectification between these two operations. This effectively doubles the number of non-linearities compared to a network using full 3D convolutions for the same number of parameters, thus rendering the model capable of representing more complex functions.The second potential benefit is that the decomposition facilitates the optimization, yielding in practice both a lower training loss and a lower testing loss</li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1904.02811 rel=noopener>Video Classification with Channel-Separated Convolutional Networks (Tran 2019)</a><ul><li>#CODE
<a href=https://github.com/facebookresearch/VMZ rel=noopener>https://github.com/facebookresearch/VMZ</a></li></ul></li><li>#PAPER
<a href=https://ieeexplore.ieee.org/abstract/document/8840843 rel=noopener>Dilated 3D Convolutional Neural Networks for Brain MRI Data Classification (Wang 2019)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1906.04928 rel=noopener>Deep Learning for Spatio-Temporal Data Mining: A Survey (Wang 2019)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1905.00561 rel=noopener>Large-scale weakly-supervised pre-training for video action recognition (Ghadiyaram 2019)</a><ul><li>#CODE
<a href=https://github.com/microsoft/computervision-recipes/tree/master/scenarios/action_recognition rel=noopener>https://github.com/microsoft/computervision-recipes/tree/master/scenarios/action_recognition</a></li></ul></li><li>#PAPER
<a href="https://openreview.net/forum?id=B1lKS2AqtX" rel=noopener>Eidetic 3D LSTM A Model for Video Prediction and Beyond, E3D-LSTM (Wang 2019)</a><ul><li>#CODE
<a href=https://github.com/google/e3d_lstm rel=noopener>https://github.com/google/e3d_lstm</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2008.01232 rel=noopener>Late Temporal Modeling in 3D CNN Architectures with BERT for Action Recognition (Esat Kalfaoglu 2020)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/2103.13915 rel=noopener>An Image is Worth 16x16 Words, What is a Video Worth? (Sharir 2021)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/2103.10504 rel=noopener>UNETR: Transformers for 3D Medical Image Segmentation (Hatamizadeh 2021)</a><ul><li><a href=https://theaisummer.com/medical-segmentation-transformers/ rel=noopener>https://theaisummer.com/medical-segmentation-transformers/</a></li><li>UNETR is the first successful transformer architecture for 3D medical image segmentation</li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2111.06265 rel=noopener>Dense Unsupervised Learning for Video Segmentation (Araslanov 2021)</a><ul><li><a href=https://github.com/visinf/dense-ulearn-vos rel=noopener>https://github.com/visinf/dense-ulearn-vos</a></li><li>methods that learns spatio-temporal correspondences without any supervision (<a href=/digitalgarden/AI/Unsupervised-learning/Unsupervised-learning rel=noopener class=internal-link data-src=/digitalgarden/AI/Unsupervised-learning/Unsupervised-learning>AI/Unsupervised learning/Unsupervised learning</a>, and achieves state-of-the-art accuracy of video object segmentation</li><li>#TALK
<a href="https://www.youtube.com/watch?v=tSBWZ6nYld0" rel=noopener>https://www.youtube.com/watch?v=tSBWZ6nYld0</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2112.10764v1 rel=noopener>Mask2Former for Video Instance Segmentation (Cheng 2021)</a><ul><li>#CODE
<a href=https://github.com/facebookresearch/Mask2Former rel=noopener>https://github.com/facebookresearch/Mask2Former</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/pdf/2207.07115v2 rel=noopener>XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model (Cheng 2022)</a><ul><li>#CODE
<a href=https://github.com/hkchengrex/XMem rel=noopener>https://github.com/hkchengrex/XMem</a></li><li><a href=https://hkchengrex.github.io/XMem/ rel=noopener>https://hkchengrex.github.io/XMem/</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/pdf/2208.02245v1 rel=noopener>MinVIS: A Minimal Video Instance Segmentation Framework without Video-based Training (Huang 2022)</a><ul><li>#CODE
<a href=https://github.com/NVlabs/MinVIS rel=noopener>https://github.com/NVlabs/MinVIS</a></li></ul></li></ul></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/digitalgarden/AI/Computer-Vision/Deep-CV/ data-ctx="AI/Computer Vision/Video segmentation and prediction" data-src=/AI/Computer-Vision/Deep-CV class=internal-link>Deep CV</a></li><li><a href=/digitalgarden/AI/Deep-learning/CNNs/ data-ctx="AI/Computer Vision/Video segmentation and prediction" data-src=/AI/Deep-learning/CNNs class=internal-link>Convolutional Neural Networks (CNNs)</a></li><li><a href=/digitalgarden/AI/Deep-learning/DL/ data-ctx="AI/Computer Vision/Video segmentation and prediction" data-src=/AI/Deep-learning/DL class=internal-link>Deep Learning (DL)</a></li><li><a href=/digitalgarden/AI4ES/AI4ES/ data-ctx="AI/Computer Vision/Video segmentation and prediction" data-src=/AI4ES/AI4ES class=internal-link>AI4ES</a></li><li><a href=/digitalgarden/AI4ES/Weather-forecasting-nowcasting/ data-ctx="AI/Computer Vision/Video segmentation and prediction" data-src=/AI4ES/Weather-forecasting-nowcasting class=internal-link>Weather forecasting, nowcasting</a></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://carlos-gg.github.io/digitalgarden/js/graph.abd4bc2af3869a96524d7d23b76152c7.js></script></div></div><div id=contact_buttons><footer><p>Made by Carlos Alberto Gomez Gonzalez using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2022</p><ul><li><a href=https://carlos-gg.github.io/digitalgarden/>Home</a></li><li><a href=https://carlos-gg.github.io>Carlos'Homepage</a></li></ul></footer></div></div></body></html>