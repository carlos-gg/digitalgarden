<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="See
  CNNs  MLPs#MLPs for vision and language  Transformers#For Computer Vision  Generative modelling#Generative models for Image data  GANs  Resources  https://github."><title>Computer Vision</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=/icon.png><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Source+Sans+Pro:wght@400;600;700&family=Fira+Code:wght@400;700&display=swap" rel=stylesheet><style>:root{--lt-colours-light:var(--light) !important;--lt-colours-lightgray:var(--lightgray) !important;--lt-colours-dark:var(--secondary) !important;--lt-colours-secondary:var(--tertiary) !important;--lt-colours-gray:var(--outlinegray) !important}h1,h2,h3,h4,h5,h6,ol,ul,thead{font-family:Inter;color:var(--dark);font-weight:revert;margin:revert;padding:revert}p,ul,text{font-family:source sans pro,sans-serif;color:var(--gray);fill:var(--gray);font-weight:revert;margin:revert;padding:revert}#TableOfContents>ol{counter-reset:section;margin-left:0;padding-left:1.5em}#TableOfContents>ol>li{counter-increment:section}#TableOfContents>ol>li>ol{counter-reset:subsection}#TableOfContents>ol>li>ol>li{counter-increment:subsection}#TableOfContents>ol>li>ol>li::marker{content:counter(section)"." counter(subsection)"  "}#TableOfContents>ol>li::marker{content:counter(section)"  "}#TableOfContents>ol>li::marker,#TableOfContents>ol>li>ol>li::marker{font-family:Source Sans Pro;font-weight:700}footer{margin-top:4em;text-align:center}table{width:100%}img{width:100%;border-radius:3px;margin:1em 0}p>img+em{display:block;transform:translateY(-1em)}sup{line-height:0}p,tbody,li{font-family:Source Sans Pro;color:var(--gray);line-height:1.5em}blockquote{margin-left:0;border-left:3px solid var(--secondary);padding-left:1em;transition:border-color .2s ease}blockquote:hover{border-color:var(--tertiary)}table{padding:1.5em}td,th{padding:.1em .5em}.footnotes p{margin:.5em 0}.pagination{list-style:none;padding-left:0;display:flex;margin-top:2em;gap:1.5em;justify-content:center}.pagination>li{text-align:center;display:inline-block}.pagination>li a{background-color:initial!important}.pagination>li a[href$="#"]{opacity:.2}.section h3>a{font-weight:700;font-family:Inter;margin:0}.section p{margin-top:0}article>.meta{margin:-1.5em 0 1em;opacity:.7}article>.tags{list-style:none;padding-left:0}article>.tags .meta>h1{margin:0}article>.tags .meta>p{margin:0}article>.tags>li{display:inline-block}article>.tags>li>a{border-radius:8px;border:var(--outlinegray)1px solid;padding:.2em .5em}article>.tags>li>a::before{content:"#";margin-right:.3em;color:var(--outlinegray)}article a{font-family:Source Sans Pro;font-weight:600}article a.internal-link{text-decoration:none;background-color:rgba(143,159,169,.15);padding:0 .1em;margin:auto -.1em;border-radius:3px}.backlinks a{font-weight:600;font-size:.9rem}sup>a{text-decoration:none;padding:0 .1em 0 .2em}a{font-family:Inter,sans-serif;font-size:1em;font-weight:700;text-decoration:none;transition:all .2s ease;color:var(--secondary)}a:hover{color:var(--tertiary)!important}pre{font-family:fira code;padding:.75em;border-radius:3px;overflow-x:scroll}code{font-family:fira code;font-size:.85em;padding:.15em .3em;border-radius:5px;background:var(--lightgray)}html{scroll-behavior:smooth}html:lang(ar) p,html:lang(ar) h1,html:lang(ar) h2,html:lang(ar) h3,html:lang(ar) article{direction:rtl;text-align:right}body{margin:0;height:100vh;width:100vw;overflow-x:hidden;background-color:var(--light)}@keyframes fadeIn{0%{opacity:0}100%{opacity:1}}footer{margin-top:4em}footer>a{font-size:1em;color:var(--secondary);padding:0 .5em 3em}hr{width:25%;margin:4em auto;height:2px;border-radius:1px;border-width:0;color:var(--dark);background-color:var(--dark)}.singlePage{margin:4em 30vw}@media all and (max-width:1200px){.singlePage{margin:25px 5vw}}.page-end{display:flex;flex-direction:row;gap:2em}@media all and (max-width:780px){.page-end{flex-direction:column}}.page-end>*{flex:1 0}.page-end>.backlinks-container>ul{list-style:none;padding-left:0}.page-end>.backlinks-container>ul>li{margin:.5em 0;padding:.25em 1em;border:var(--outlinegray)1px solid;border-radius:5px}.page-end #graph-container{border:var(--outlinegray)1px solid;border-radius:5px}.centered{margin-top:30vh}article>h1{font-size:2em}header{display:flex;flex-direction:row;align-items:center}header>h1{font-size:2em}@media all and (max-width:600px){header>nav{display:none}}header>.spacer{flex:auto}header>svg{cursor:pointer;width:18px;min-width:18px;margin:0 1em}header>svg:hover .search-path{stroke:var(--tertiary)}header>svg .search-path{stroke:var(--gray);stroke-width:2px;transition:stroke .5s ease}#search-container{position:fixed;z-index:9999;left:0;top:0;width:100vw;height:100%;overflow:scroll;display:none;backdrop-filter:blur(4px);-webkit-backdrop-filter:blur(4px)}#search-container>div{width:50%;margin-top:15vh;margin-left:auto;margin-right:auto}@media all and (max-width:1200px){#search-container>div{width:90%}}#search-container>div>*{width:100%;border-radius:4px;background:var(--light);box-shadow:0 14px 50px rgba(27,33,48,.12),0 10px 30px rgba(27,33,48,.16);margin-bottom:2em}#search-container>div>input{box-sizing:border-box;padding:.5em 1em;font-family:Inter,sans-serif;color:var(--dark);font-size:1.1em;border:1px solid var(--outlinegray)}#search-container>div>input:focus{outline:none}#search-container>div>#results-container>.result-card{padding:1em;cursor:pointer;transition:background .2s ease;border:1px solid var(--outlinegray);border-bottom:none;width:100%;font-family:inherit;font-size:100%;line-height:1.15;margin:0;overflow:visible;text-transform:none;text-align:left;background:var(--light);outline:none}#search-container>div>#results-container>.result-card:hover,#search-container>div>#results-container>.result-card:focus{background:rgba(180,180,180,.15)}#search-container>div>#results-container>.result-card:first-of-type{border-top-left-radius:5px;border-top-right-radius:5px}#search-container>div>#results-container>.result-card:last-of-type{border-bottom-left-radius:5px;border-bottom-right-radius:5px;border-bottom:1px solid var(--outlinegray)}#search-container>div>#results-container>.result-card>h3,#search-container>div>#results-container>.result-card>p{margin:0}#search-container>div>#results-container>.result-card .search-highlight{background-color:#afbfc966;padding:.05em .2em;border-radius:3px}.section-ul{list-style:none;padding-left:0}.section-ul>li{border:1px solid var(--outlinegray);border-radius:5px;padding:0 1em;margin-bottom:1em}.section-ul>li h3{opacity:1;font-weight:700;margin-bottom:0}.section-ul>li .meta{opacity:.6}.popover{z-index:999;position:absolute;width:20em;display:inline-block;visibility:hidden;background-color:var(--light);padding:1em;border:1px solid var(--outlinegray);border-radius:5px;transform:translate(-50%,40%);opacity:0;pointer-events:none;transition:opacity .2s ease,transform .2s ease;transition-delay:.3s;user-select:none}@media all and (max-width:600px){.popover{display:none}}.popover.visible{opacity:1;visibility:visible;transform:translate(-50%,20%)}.popover>h3{font-size:1rem;margin:.25em 0}.popover>.meta{margin-top:.25em;opacity:.5}.popover>p{margin:0;font-weight:400;user-select:none}</style><style>.darkmode{float:right;padding:1em;min-width:30px;position:relative}@media all and (max-width:450px){.darkmode{padding:1em}}.darkmode>.toggle{display:none;box-sizing:border-box}.darkmode svg{opacity:0;position:absolute;width:20px;height:20px;top:calc(50% - 10px);margin:0 7px;fill:var(--gray);transition:opacity .1s ease}.toggle:checked~label>#dayIcon{opacity:0}.toggle:checked~label>#nightIcon{opacity:1}.toggle:not(:checked)~label>#dayIcon{opacity:1}.toggle:not(:checked)~label>#nightIcon{opacity:0}</style><style>.chroma{color:#f8f8f2;background-color:#282a36;overflow:hidden}.chroma .lntd{vertical-align:top;padding:0;margin:0;border:0}.chroma .lntable{border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block}.chroma .hl{display:block;width:100%;background-color:#ffc}.chroma .lnt{margin-right:.4em;padding:0 .4em;color:#7f7f7f}.chroma .ln{margin-right:.4em;padding:0 .4em;color:#7f7f7f}.chroma .k{color:#ff79c6}.chroma .kc{color:#ff79c6}.chroma .kd{color:#8be9fd;font-style:italic}.chroma .kn{color:#ff79c6}.chroma .kp{color:#ff79c6}.chroma .kr{color:#ff79c6}.chroma .kt{color:#8be9fd}.chroma .na{color:#50fa7b}.chroma .nb{color:#8be9fd;font-style:italic}.chroma .nc{color:#50fa7b}.chroma .nf{color:#50fa7b}.chroma .nl{color:#8be9fd;font-style:italic}.chroma .nt{color:#ff79c6}.chroma .nv{color:#8be9fd;font-style:italic}.chroma .vc{color:#8be9fd;font-style:italic}.chroma .vg{color:#8be9fd;font-style:italic}.chroma .vi{color:#8be9fd;font-style:italic}.chroma .s{color:#f1fa8c}.chroma .sa{color:#f1fa8c}.chroma .sb{color:#f1fa8c}.chroma .sc{color:#f1fa8c}.chroma .dl{color:#f1fa8c}.chroma .sd{color:#f1fa8c}.chroma .s2{color:#f1fa8c}.chroma .se{color:#f1fa8c}.chroma .sh{color:#f1fa8c}.chroma .si{color:#f1fa8c}.chroma .sx{color:#f1fa8c}.chroma .sr{color:#f1fa8c}.chroma .s1{color:#f1fa8c}.chroma .ss{color:#f1fa8c}.chroma .m{color:#bd93f9}.chroma .mb{color:#bd93f9}.chroma .mf{color:#bd93f9}.chroma .mh{color:#bd93f9}.chroma .mi{color:#bd93f9}.chroma .il{color:#bd93f9}.chroma .mo{color:#bd93f9}.chroma .o{color:#ff79c6}.chroma .ow{color:#ff79c6}.chroma .c{color:#6272a4}.chroma .ch{color:#6272a4}.chroma .cm{color:#6272a4}.chroma .c1{color:#6272a4}.chroma .cs{color:#6272a4}.chroma .cp{color:#ff79c6}.chroma .cpf{color:#ff79c6}.chroma .gd{color:#8b080b}.chroma .ge{text-decoration:underline}.chroma .gh{font-weight:700}.chroma .gi{font-weight:700}.chroma .go{color:#44475a}.chroma .gu{font-weight:700}.chroma .gl{text-decoration:underline}.lntd:first-of-type>.chroma{padding-right:0}.chroma code{font-family:fira code!important;font-size:.85em;line-height:1em;background:0 0;padding:0}.chroma{border-radius:3px;margin:0}</style><style>:root{--light:#faf8f8;--dark:#141021;--secondary:#284b63;--tertiary:#84a59d;--visited:#afbfc9;--primary:#f28482;--gray:#4e4e4e;--lightgray:#f0f0f0;--outlinegray:#dadada}[saved-theme=dark]{--light:#1e1e21 !important;--dark:#fbfffe !important;--secondary:#6b879a !important;--visited:#4a575e !important;--tertiary:#84a59d !important;--primary:#f58382 !important;--gray:#d4d4d4 !important;--lightgray:#292633 !important;--outlinegray:#343434 !important}</style><script>const userPref=window.matchMedia('(prefers-color-scheme: light)').matches?'light':'dark',currentTheme=localStorage.getItem('theme')??userPref;currentTheme&&document.documentElement.setAttribute('saved-theme',currentTheme);const switchTheme=a=>{a.target.checked?(document.documentElement.setAttribute('saved-theme','dark'),localStorage.setItem('theme','dark')):(document.documentElement.setAttribute('saved-theme','light'),localStorage.setItem('theme','light'))};window.addEventListener('DOMContentLoaded',()=>{const a=document.querySelector('#darkmode-toggle');a.addEventListener('change',switchTheme,!1),currentTheme==='dark'&&(a.checked=!0)})</script><script>let saved=!1;const fetchData=async()=>{if(saved)return saved;const promises=[fetch("https://carlos-gg.github.io/digitalgarden/linkIndex.json").then(a=>a.json()).then(a=>({index:a.index,links:a.links})),fetch("https://carlos-gg.github.io/digitalgarden/contentIndex.json").then(a=>a.json())],[{index,links},content]=await Promise.all(promises),res={index,links,content};return saved=res,res};fetchData()</script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-S103D50NQ0"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-S103D50NQ0',{anonymize_ip:!1})}</script><script>function htmlToElement(a){const b=document.createElement('template');return a=a.trim(),b.innerHTML=a,b.content.firstChild}const baseUrl="https://carlos-gg.github.io/digitalgarden".replace(window.location.origin,"");document.addEventListener("DOMContentLoaded",()=>{fetchData().then(({content:a})=>{const b=[...document.getElementsByClassName("internal-link")];b.forEach(b=>{const c=a[b.dataset.src.replace(baseUrl,"")];if(c){const d=`<div class="popover">
    <h3>${c.title}</h3>
    <p>${removeMarkdown(c.content).split(" ",20).join(" ")}...</p>
    <p class="meta">${new Date(c.lastmodified).toLocaleDateString()}</p>
</div>`,a=htmlToElement(d);b.appendChild(a),b.addEventListener("mouseover",()=>{a.classList.add("visible")}),b.addEventListener("mouseout",()=>{a.classList.remove("visible")})}})})})</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/gh/nextapps-de/flexsearch@0.7.2/dist/flexsearch.bundle.js></script><script>const removeMarkdown=(c,b={listUnicodeChar:!1,stripListLeaders:!0,gfm:!0,useImgAltText:!1,preserveLinks:!1})=>{let a=c||"";a=a.replace(/^(-\s*?|\*\s*?|_\s*?){3,}\s*$/gm,"");try{b.stripListLeaders&&(b.listUnicodeChar?a=a.replace(/^([\s\t]*)([\*\-\+]|\d+\.)\s+/gm,b.listUnicodeChar+" $1"):a=a.replace(/^([\s\t]*)([\*\-\+]|\d+\.)\s+/gm,"$1")),b.gfm&&(a=a.replace(/\n={2,}/g,"\n").replace(/~{3}.*\n/g,"").replace(/~~/g,"").replace(/`{3}.*\n/g,"")),b.preserveLinks&&(a=a.replace(/\[(.*?)\][\[\(](.*?)[\]\)]/g,"$1 ($2)")),a=a.replace(/<[^>]*>/g,"").replace(/^[=\-]{2,}\s*$/g,"").replace(/\[\^.+?\](\: .*?$)?/g,"").replace(/\s{0,2}\[.*?\]: .*?$/g,"").replace(/\!\[(.*?)\][\[\(].*?[\]\)]/g,b.useImgAltText?"$1":"").replace(/\[(.*?)\][\[\(].*?[\]\)]/g,"$1").replace(/^\s{0,3}>\s?/g,"").replace(/(^|\n)\s{0,3}>\s?/g,"\n\n").replace(/^\s{1,2}\[(.*?)\]: (\S+)( ".*?")?\s*$/g,"").replace(/^(\n)?\s{0,}#{1,6}\s+| {0,}(\n)?\s{0,}#{0,} {0,}(\n)?\s{0,}$/gm,"$1$2$3").replace(/([\*_]{1,3})(\S.*?\S{0,1})\1/g,"$2").replace(/([\*_]{1,3})(\S.*?\S{0,1})\1/g,"$2").replace(/(`{3,})(.*?)\1/gm,"$2").replace(/`(.+?)`/g,"$1").replace(/\n{2,}/g,"\n\n")}catch(a){return console.error(a),c}return a}</script><script>async function run(){const g=new FlexSearch.Document({cache:!0,charset:"latin:extra",optimize:!0,worker:!0,document:{index:[{field:"content",tokenize:"strict",context:{resolution:5,depth:3,bidirectional:!0},suggest:!0},{field:"title",tokenize:"forward"}]}}),{content:d}=await fetchData();for(const[b,a]of Object.entries(d))g.add({id:b,title:a.title,content:removeMarkdown(a.content)});const j=(i,j)=>{const a=20,k=j.split(/\s+/).filter(a=>a!==""),b=i.split(/\s+/).filter(a=>a!==""),f=a=>k.some(b=>a.toLowerCase().startsWith(b.toLowerCase())),d=b.map(f);let e=0,g=0;for(let b=0;b<Math.max(d.length-a,0);b++){const f=d.slice(b,b+a),c=f.reduce((a,b)=>a+b,0);c>=e&&(e=c,g=b)}const c=Math.max(g-a,0),h=Math.min(c+2*a,b.length),l=b.slice(c,h).map(a=>{return f(a)?`<span class="search-highlight">${a}</span>`:a}).join(" ").replaceAll('</span> <span class="search-highlight">'," ");return`${c===0?"":"..."}${l}${h===b.length?"":"..."}`},m=({url:b,title:c,content:d,term:a})=>{const e=removeMarkdown(d),f=j(c,a),g=j(e,a);return`<button class="result-card" id="${b}">
        <h3>${f}</h3>
        <p>${g}</p>
    </button>`},h=(a,b)=>{window.location.href="https://carlos-gg.github.io/digitalgarden"+`${a}#:~:text=${encodeURIComponent(b)}`},l=a=>({id:a,url:a,title:d[a].title,content:d[a].content}),c=document.getElementById('search-bar'),e=document.getElementById("results-container");let b;c.addEventListener("keyup",a=>{if(a.key==="Enter"){const a=document.getElementsByClassName("result-card")[0];h(a.id,b)}}),c.addEventListener('input',a=>{b=a.target.value,g.search(b,[{field:"content",limit:10,suggest:!0},{field:"title",limit:5}]).then(d=>{const a=b=>{const a=d.filter(a=>a.field===b);return a.length===0?[]:[...a[0].result]},f=new Set([...a('title'),...a('content')]),c=[...f].map(l);if(c.length===0)e.innerHTML=`<button class="result-card">
                    <h3>No results.</h3>
                    <p>Try another search term?</p>
                </button>`;else{e.innerHTML=c.map(a=>m({...a,term:b})).join("\n");const a=document.getElementsByClassName("result-card");[...a].forEach(a=>{a.onclick=()=>h(a.id,b)})}})});const a=document.getElementById("search-container");function f(){a.style.display==="none"||a.style.display===""?(c.value="",e.innerHTML="",a.style.display="block",c.focus()):a.style.display="none"}function i(){a.style.display="none"}document.addEventListener('keydown',a=>{a.key==="/"&&(a.preventDefault(),f()),a.key==="Escape"&&(a.preventDefault(),i())});const k=document.getElementById("search-icon");k.addEventListener('click',a=>{f()}),k.addEventListener('keydown',a=>{f()}),a.addEventListener('click',a=>{i()}),document.getElementById("search-space").addEventListener('click',a=>{a.stopPropagation()})}run()</script><div class=singlePage><header><h1 id=page-title><a href=https://carlos-gg.github.io/digitalgarden>CarlosGG's Digital Garden ü™¥</a></h1><svg tabindex="0" id="search-icon" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg><div class=spacer></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Computer Vision</h1><p class=meta>Last updated March 11, 2022</p><ul class=tags></ul><aside class=mainTOC><h3>Table of Contents</h3><nav id=TableOfContents><ol><li><a href=#resources>Resources</a></li><li><a href=#books>Books</a></li><li><a href=#courses>Courses</a></li><li><a href=#code>Code</a></li><li><a href=#references>References</a><ol><li><a href=#deep-learning-based-cv>Deep learning-based CV</a></li><li><a href=#traditional-cv-techniques>Traditional CV techniques</a></li></ol></li></ol></nav></aside><p>See</p><ul><li><a href=/digitalgarden/AI/Deep-learning/CNNs rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/CNNs>CNNs</a></li><li><a href=/digitalgarden/AI/Deep-learning/MLPs rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/MLPs>MLPs#MLPs for vision and language</a></li><li><a href=/digitalgarden/AI/Deep-learning/Transformers rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Transformers>Transformers#For Computer Vision</a></li><li><a href=/digitalgarden/AI/Deep-learning/Generative-modelling rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Generative-modelling>Generative modelling#Generative models for Image data</a></li><li><a href=/digitalgarden/AI/Deep-learning/GANs rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/GANs>GANs</a></li></ul><h2 id=resources>Resources</h2><ul><li><a href=https://github.com/jbhuang0604/awesome-computer-vision>https://github.com/jbhuang0604/awesome-computer-vision</a></li><li><a href=https://paperswithcode.com/area/computer-vision rel=noopener>Papers with code - computer vision</a></li></ul><h2 id=books>Books</h2><ul><li>#BOOK
<a href=https://www.crcpress.com/Image-Processing-and-Acquisition-using-Python/Chityala-Pudipeddi/p/book/9781466583757 rel=noopener>Image Processing and Acquisition using Python (Chityala 2014)</a></li><li>#BOOK
<a href=https://www.pearson.com/us/higher-education/program/Forsyth-Computer-Vision-A-Modern-Approach-2nd-Edition/PGM111082.html rel=noopener>Computer Vision: A Modern Approach (Forsyth, 2011 PEARSON)</a><ul><li><a href=https://github.com/yihui-he/computer-vision-tutorial/blob/master/Computer%20Vision%20A%20Modern%20Approach%202nd%20Edition.pdf>https://github.com/yihui-he/computer-vision-tutorial/blob/master/Computer%20Vision%20A%20Modern%20Approach%202nd%20Edition.pdf</a></li></ul></li><li>#BOOK
<a href=http://www.computervisionmodels.com/ rel=noopener>Computer Vision: Models, Learning, and Inference (Prince, 2012 CAMBRIDGE)</a></li><li>#BOOK
<a href=https://d2l.ai/chapter_computer-vision/index.html rel=noopener>Computer vision (chapter)</a></li></ul><h2 id=courses>Courses</h2><ul><li>#COURSE
<a href=https://courses.engr.illinois.edu/cs543/sp2015/ rel=noopener>Computer vision (CS543/ECE549, UIUC)</a></li><li>#COURSE
<a href=http://6.869.csail.mit.edu/fa18/ rel=noopener>Advances in Computer vision (MIT)</a></li><li>#COURSE
<a href=https://www.udacity.com/course/introduction-to-computer-vision--ud810 rel=noopener>Introduction to computer vision (Udacity, Georgia Tech)</a></li><li>#COURSE
<a href=http://imatge-upc.github.io/telecombcn-2016-dlcv/ rel=noopener>Deep Learning for Computer Vision (UPC TelecomBCN 2016)</a></li><li>#COURSE
<a href=http://cs231n.github.io/ rel=noopener>Convolutional Neural Networks for Visual Recognition (CS231n, Stanford)</a><ul><li><a href=http://karpathy.github.io/neuralnets/ rel=noopener>Pre-version of the course</a></li><li><a href=http://cs231n.github.io/ rel=noopener>Notes (Karpathy)</a></li><li><a href="https://www.youtube.com/playlist?list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC" rel=noopener>Videos for each lecture</a></li></ul></li><li>#COURSE
<a href=https://cs.nyu.edu/~fergus/teaching/vision/ rel=noopener>Computer vision (NYU)</a></li><li>#COURSE
<a href=https://sisu.ut.ee/dev/imageprocessing/avaleht rel=noopener>Digital image processing (U Tartu)</a></li><li>#COURSE Convolutional Neural Networks for Image Recognition (DeepMind x UCL | Deep Learning Lectures)<ul><li><a href="https://www.youtube.com/watch?v=shVKhOmT0HE&list=PLqYmG7hTraZCDxZ44o4p3N5Anz3lLRVZF&index=3&t=1s">https://www.youtube.com/watch?v=shVKhOmT0HE&list=PLqYmG7hTraZCDxZ44o4p3N5Anz3lLRVZF&index=3&t=1s</a></li><li><a href=https://storage.googleapis.com/deepmind-media/UCLxDeepMind_2020/L3%20-%20UUCLxDeepMind%20DL2020.pdf>https://storage.googleapis.com/deepmind-media/UCLxDeepMind_2020/L3%20-%20UUCLxDeepMind%20DL2020.pdf</a></li></ul></li><li>#COURSE Advanced Models for Computer Vision (DeepMind x UCL | Deep Learning Lectures)<ul><li><a href="https://www.youtube.com/watch?v=_aUq7lmMfxo&list=PLqYmG7hTraZCDxZ44o4p3N5Anz3lLRVZF&index=4">https://www.youtube.com/watch?v=_aUq7lmMfxo&list=PLqYmG7hTraZCDxZ44o4p3N5Anz3lLRVZF&index=4</a></li><li><a href=https://storage.googleapis.com/deepmind-media/UCLxDeepMind_2020/L4%20-%20UCLxDeepMind%20DL2020.pdf>https://storage.googleapis.com/deepmind-media/UCLxDeepMind_2020/L4%20-%20UCLxDeepMind%20DL2020.pdf</a></li></ul></li></ul><h2 id=code>Code</h2><ul><li>#CODE
<a href=https://github.com/scikit-image/scikit-image rel=noopener>Scikit-image. Image processing in Python</a><ul><li><a href=http://scikit-image.org>http://scikit-image.org</a></li></ul></li><li>#CODE
<a href=https://opencv.org/ rel=noopener>OpenCV (Open Source Computer Vision Library)</a><ul><li>OpenCV is released under a BSD license and hence it‚Äôs free for both academic and commercial use. It has C++, Python and Java interfaces and supports Windows, Linux, Mac OS, iOS and Android. OpenCV was designed for computational efficiency and with a strong focus on real-time applications. Written in optimized C/C++, the library can take advantage of multi-core processing. Enabled with OpenCL, it can take advantage of the hardware acceleration of the underlying heterogeneous compute platform.</li><li><a href=https://github.com/opencv/opencv/wiki/Deep-Learning-in-OpenCV>https://github.com/opencv/opencv/wiki/Deep-Learning-in-OpenCV</a></li><li>#PAPER
<a href=https://dl.acm.org/doi/10.1145/2184319.2184337 rel=noopener>Real-time computer vision with OpenCV (Pulli 2012)</a></li></ul></li><li>#CODE
<a href=https://github.com/sightmachine/SimpleCV rel=noopener>SimpleCV</a><ul><li><a href=http://simplecv.org/>http://simplecv.org/</a></li><li>SimpleCV is an open source framework for building computer vision applications. With it, you get access to several high-powered computer vision libraries such as OpenCV ‚Äì without having to first learn about bit depths, file formats, color spaces, buffer management, eigenvalues, or matrix versus bitmap storage. This is computer vision made easy.</li></ul></li><li>#CODE
<a href=https://github.com/aleju/imgaug rel=noopener>Imgaug. Image augmentation for machine learning experiments</a><ul><li><a href=http://imgaug.readthedocs.io>http://imgaug.readthedocs.io</a></li></ul></li><li>#CODE
<a href=https://github.com/chainer/chainercv rel=noopener>ChainerCV: a Library for Computer Vision in Deep Learning</a>
- <a href=http://chainercv.readthedocs.io/en/stable/>http://chainercv.readthedocs.io/en/stable/</a>
- ChainerCV is a collection of tools to train and run neural networks for computer vision tasks using Chainer</li><li>#CODE
<a href=https://cmusatyalab.github.io/openface/ rel=noopener>Openface. Free and open source face recognition with deep neural networks</a></li><li>#CODE Vision - The torchvision package consists of popular datasets, model architectures, and common image transformations fo CV.<ul><li><a href=https://github.com/pytorch/vision>https://github.com/pytorch/vision</a></li></ul></li><li>#CODE
<a href=https://github.com/google-research/scenic rel=noopener>Scenic</a><ul><li><a href=https://www.marktechpost.com/2021/10/30/google-research-introduces-scenic-an-open-source-jax-library-for-computer-vision-research/>https://www.marktechpost.com/2021/10/30/google-research-introduces-scenic-an-open-source-jax-library-for-computer-vision-research/</a></li><li>A Jax Library for Computer Vision Research and Beyond</li><li>codebase with a focus on research around attention-based models for computer vision</li></ul></li></ul><h2 id=references>References</h2><h3 id=deep-learning-based-cv>Deep learning-based CV</h3><p>See:</p><ul><li><p><a href=/digitalgarden/AI/Deep-learning/CNNs rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/CNNs>CNNs</a></p></li><li><p><a href=/digitalgarden/AI/Deep-learning/GANs rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/GANs>GANs</a></p></li><li><p><a href=/digitalgarden/AI/Deep-learning/Normalizing-flows rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Normalizing-flows>Normalizing flows</a></p></li><li><p><a href=/digitalgarden/AI/Deep-learning/Transformers rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Transformers>Transformers#For Computer Vision</a></p></li><li><p><a href=/digitalgarden/AI/Deep-learning/Deep-learning rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Deep-learning>Deep Learning</a> is used in the domain of digital image processing to solve difficult problems (e.g.image colourization, classification, segmentation and detection). DL methods such as
<a href=/digitalgarden/AI/Deep-learning/CNNs rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/CNNs>CNNs</a> mostly improve prediction performance using big data and plentiful computing resources and have pushed the boundaries of what was possible. Problems which were assumed to be unsolvable are now being solved with super-human accuracy. Image classification is a prime example of this. Since being reignited by Krizhevsky, Sutskever and Hinton in 2012, DL has dominated the domain ever since due to a substantially better performance compared to traditional methods.</p></li><li><p><a href=https://github.com/kjw0612/awesome-deep-vision>https://github.com/kjw0612/awesome-deep-vision</a></p></li><li><p><a href=https://github.com/timzhang642/3D-Machine-Learning>https://github.com/timzhang642/3D-Machine-Learning</a></p></li><li><p><a href=https://medium.com/@taposhdr/medical-image-analysis-with-deep-learning-i-23d518abf531>https://medium.com/@taposhdr/medical-image-analysis-with-deep-learning-i-23d518abf531</a></p></li><li><p><a href=http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/>http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/</a></p></li><li><p>#PAPER
<a href=https://www.hindawi.com/journals/cin/2018/7068349/ rel=noopener>Deep Learning for Computer Vision: A Brief Review (Voulodimos 2017)</a></p></li><li><p>#PAPER
<a href=https://arxiv.org/abs/1910.13796 rel=noopener>Deep Learning vs. Traditional Computer Vision (O&rsquo;Mahony 2019)</a></p></li><li><p>#PAPER
<a href=https://www.nature.com/articles/s41467-020-20655-6 rel=noopener>Deep learning encodes robust discriminative neuroimaging representations to outperform standard machine learning (Abrol 2021)</a></p></li><li><p>#PAPER
<a href=https://www.nature.com/articles/s41746-020-00376-2 rel=noopener>Deep learning-enabled medical computer vision (Esteva 2021)</a></p></li><li><p>#PAPER
<a href=https://arxiv.org/abs/2103.06255 rel=noopener>Involution: Inverting the Inherence of Convolution for Visual Recognition, a brand new neural operator (Li 2021)</a></p><ul><li>#CODE <a href=https://github.com/d-li14/involution>https://github.com/d-li14/involution</a></li><li><a href="https://www.youtube.com/watch?v=pH2jZun8MoY&list=WL&index=27&t=641s" rel=noopener>Paper explained</a></li><li>involution is a general-purpose neural primitive that is versatile for a spectrum of deep learning models on different vision tasks</li><li>involution bridges convolution and self-attention in design, while being more efficient and effective than convolution, simpler than self-attention in form</li><li>the proposed involution operator could be leveraged as fundamental bricks to build the new generation of neural networks for visual recognition, powering different deep learning models on several prevalent benchmarks</li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/abs/2108.02451v3 rel=noopener>Unifying Nonlocal Blocks for Neural Networks (Zhu 2021)</a></p><ul><li>#CODE <a href=https://github.com/zh460045050/SNL_ICCV2021>https://github.com/zh460045050/SNL_ICCV2021</a></li></ul></li></ul><h3 id=traditional-cv-techniques>Traditional CV techniques</h3><h4 id=background-subtraction>Background subtraction</h4><p>See
<a href=/digitalgarden/AI/Computer-Vision/Background-subtraction rel=noopener class=internal-link data-src=/digitalgarden/AI/Computer-Vision/Background-subtraction>Background subtraction</a></p><h4 id=geometric-transformations>Geometric transformations</h4><ul><li><a href=https://en.wikipedia.org/wiki/Geometric_transformation>https://en.wikipedia.org/wiki/Geometric_transformation</a></li><li><a href=https://www.graphicsmill.com/docs/gm/affine-and-projective-transformations.htm>https://www.graphicsmill.com/docs/gm/affine-and-projective-transformations.htm</a></li><li><a href=https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_geometric_transformations/py_geometric_transformations.html>https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_geometric_transformations/py_geometric_transformations.html</a></li><li><a href=http://eeweb.poly.edu/~yao/EL5123/lecture12_ImageWarping.pdf>http://eeweb.poly.edu/~yao/EL5123/lecture12_ImageWarping.pdf</a></li></ul><h5 id=affine-transformations>Affine transformations</h5><ul><li><a href=https://en.wikipedia.org/wiki/Affine_transformation>https://en.wikipedia.org/wiki/Affine_transformation</a></li><li>Affine transformations are combinations of linear transformations and translations. Properties of affine transformations:</li><li>Lines map to lines</li><li>Parallel lines remain parallel</li><li>Ratios are preserved</li><li>Closed under composition</li></ul><h5 id=projective-transformations>Projective transformations</h5><ul><li><a href=https://en.wikipedia.org/wiki/Projective_transformation>https://en.wikipedia.org/wiki/Projective_transformation</a></li><li>Projective transformations are combos of Affine transformations, and projective warps. Properties of projective transformations:</li><li>Lines map to lines</li><li>Parallel lines do not necessarily remain parallel</li><li>Ratios are not preserved</li><li>Closed under composition</li><li>Models change of basis</li><li>Projective matrix is defined up to a scale (8 DOF)</li></ul><h4 id=filtering>Filtering</h4><ul><li>For each pixel we compute a function of local neighborhood and output a new value. Use cases:</li><li>Enhance images: Denoise, smooth, increase contrast, etc.</li><li>Extract information from images: Texture, edges, distinctive points, etc.</li><li>Detect patterns: Template matching</li></ul><p><a href=https://dsp.stackexchange.com/questions/12684/difference-between-correlation-and-convolution-on-an-image>https://dsp.stackexchange.com/questions/12684/difference-between-correlation-and-convolution-on-an-image</a></p><h5 id=spatial-domain>Spatial domain</h5><ul><li>Linear filtering: function is a weighted sum/difference of pixel values (dot products at each position).</li><li><a href=https://en.wikipedia.org/wiki/Kernel_%28image_processing%29 rel=noopener>Convolution and kernels</a></li><li><a href=http://setosa.io/ev/image-kernels/ rel=noopener>Kernels visualization</a></li><li><a href=http://scikit-image.org/docs/dev/api/skimage.filters.html>http://scikit-image.org/docs/dev/api/skimage.filters.html</a></li><li>Examples:<ul><li><a href=https://en.wikipedia.org/wiki/Box_blur rel=noopener>Box filter. Replaces each pixel with an average of its neighborhood (smoothing effect)</a></li><li>Sharpening filter. Accentuates differences with local average.<ul><li><a href=http://northstar-www.dartmouth.edu/doc/idl/html_6.2/Sharpening_an_Image.html>http://northstar-www.dartmouth.edu/doc/idl/html_6.2/Sharpening_an_Image.html</a></li><li><a href=https://en.wikipedia.org/wiki/Box_blur>https://en.wikipedia.org/wiki/Box_blur</a></li></ul></li><li>Sobel filter. This filter is used in image processing and computer vision, particularly within edge detection algorithms where it creates an image emphasising edges.<ul><li><a href=http://aishack.in/tutorials/sobel-laplacian-edge-detectors/>http://aishack.in/tutorials/sobel-laplacian-edge-detectors/</a></li><li><a href=https://en.wikipedia.org/wiki/Sobel_operator>https://en.wikipedia.org/wiki/Sobel_operator</a></li></ul></li><li><a href=https://en.wikipedia.org/wiki/Gaussian_filter rel=noopener>Gaussian filter. Smoothing. Remove ‚Äúhigh-frequency‚Äù components from the image (low-pass filter)</a></li><li><a href=https://en.wikipedia.org/wiki/Median_filter rel=noopener>Median filter. Non linear filter for image smoothing. Robustness to outliers</a></li><li><a href=https://en.wikipedia.org/wiki/Bilateral_filter rel=noopener>Bilateral filter. A bilateral filter is a non-linear, edge-preserving, and noise-reducing smoothing filter for images. It replaces the intensity of each pixel with a weighted average of intensity values from nearby pixels</a></li><li>Laplacian filter. Filtering with a Laplacian operator:<ul><li><a href=http://aishack.in/tutorials/sobel-laplacian-edge-detectors/>http://aishack.in/tutorials/sobel-laplacian-edge-detectors/</a></li><li><a href=https://docs.opencv.org/2.4/doc/tutorials/imgproc/imgtrans/laplace_operator/laplace_operator.html>https://docs.opencv.org/2.4/doc/tutorials/imgproc/imgtrans/laplace_operator/laplace_operator.html</a></li></ul></li></ul></li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-fallback data-lang=fallback>0	 1	  0
1 	-4	  1
0 	 1	  0
</code></pre></td></tr></table></div></div><h5 id=frequency-domain>Frequency domain</h5><ul><li>Fourier transform stores the magnitude and phase at each frequency. The magnitude encodes how much signal there is at a particular frequency whiel the phase encodes spatial information (indirectly).</li><li>The Convolution Theorem:<ul><li>The Fourier transform of the convolution of two functions is the product of their Fourier transforms</li><li>The inverse Fourier transform of the product of two Fourier transforms is the convolution of the two inverse Fourier transforms</li><li>Convolution in spatial domain is equivalent to multiplication in frequency domain</li></ul></li></ul><h4 id=template-matching>Template matching</h4><ul><li><a href=https://en.wikipedia.org/wiki/Template_matching>https://en.wikipedia.org/wiki/Template_matching</a></li></ul><h5 id=template-based>Template based</h5><ul><li><a href=http://aishack.in/tutorials/template-matching/>http://aishack.in/tutorials/template-matching/</a></li><li>For templates without strong features, or for when the bulk of the template image constitutes the matching image, a template-based approach may be effective.</li><li>Cross-correlation. Linear filtering: function is a weighted sum/difference of pixel values (dot products at each position)<ul><li><a href=https://en.wikipedia.org/wiki/Cross-correlation>https://en.wikipedia.org/wiki/Cross-correlation</a></li><li><a href=https://en.wikipedia.org/wiki/Kernel_%28image_processing%29 rel=noopener>Convolution and kernels</a></li><li><a href=http://setosa.io/ev/image-kernels/ rel=noopener>Kernels visualization</a></li></ul></li><li><a href=https://en.wikipedia.org/wiki/Pyramid_%28image_processing%29 rel=noopener>Using image pyramids</a></li></ul><h5 id=feature-based>Feature based</h5><ul><li>Feature-based approach relies on the extraction of image features such, i.e. shapes, textures , colors, to match in the target image or frame. This approach is currently achieved by using Neural Networks and Deep Learning classifiers such as VGG, AlexNet, ResNet. Deep Convolutional Neural Networks process the image by passing it through different hidden layers and at each layer produce a vector with classification information about the image. These vectors are extracted from the network and are used as the features of the image. Feature extraction by using Deep Neural Networks is extremely effective and thus is the standard in state of the art template matching algorithms.</li><li>This method is considered more robust and is state of the art as it can match templates with non-rigid and out of plane transformation, it can match with high background clutter and illumination changes.</li></ul><h4 id=feature-extraction>Feature extraction</h4><ul><li>The Computer Vision Pipeline, Part 4: feature extraction. <a href=https://freecontent.manning.com/the-computer-vision-pipeline-part-4-feature-extraction/>https://freecontent.manning.com/the-computer-vision-pipeline-part-4-feature-extraction/</a><ul><li>Feature extraction is a core component of the computer vision pipeline. In fact, the entire deep learning model works around the idea of extracting useful features which clearly define the objects in the image. We‚Äôre going to spend a little more time here because it‚Äôs important that you understand what a feature is, what a vector of features is, and why we extract features.</li><li>A feature in Machine Learning is an individual measurable property or characteristic of a phenomenon being observed. Features are the input that you feed to your machine learning model to output a prediction or classification. Suppose you want to predict the price of a house, your input features (properties) might include: square_foot, number_of_rooms, bathrooms, etc. and the model will output the predicted price based on the values of your features. Selecting good features that clearly distinguish your objects increases the predictive power of machine learning algorithms.</li></ul></li><li>In image processing, algorithms are used to detect and isolate various desired portions or shapes (features) of a digitized image or video stream. It is particularly important in the area of optical character recognition.<ul><li>Low-level: Edge detection, Corner detection, Blob detection, Ridge detection, Scale-invariant feature transform,</li><li>Curvature: Edge direction, changing intensity, autocorrelation</li><li>Image motion: Motion detection. Area based, differential approach, Optical flow (<a href=https://en.wikipedia.org/wiki/Optical_flow>https://en.wikipedia.org/wiki/Optical_flow</a>)</li><li>Shape based: Thresholding, Blob extraction, Template matching, Hough transform<ul><li>Lines: Circles/ellipses, Arbitrary shapes (generalized Hough transform). Works with any parameterizable feature (class variables, cluster detection, etc..)</li></ul></li><li>Flexible methods: Deformable, parameterized shapesActive contours (snakes)</li></ul></li></ul><h5 id=blob-detection>Blob detection</h5><ul><li><a href=https://en.wikipedia.org/wiki/Blob_detection>https://en.wikipedia.org/wiki/Blob_detection</a></li><li>In computer vision, blob detection methods are aimed at detecting regions in a digital image that differ in properties, such as brightness or color, compared to surrounding regions. Informally, a blob is a region of an image in which some properties are constant or approximately constant; all the points in a blob can be considered in some sense to be similar to each other.</li><li><a href=https://en.wikipedia.org/wiki/Blob_detection#The_Laplacian_of_Gaussian rel=noopener>The Laplacian of Gaussian</a></li><li><a href=https://en.wikipedia.org/wiki/Difference_of_Gaussians rel=noopener>The difference of Gaussians</a></li><li><a href=https://en.wikipedia.org/wiki/Blob_detection#The_determinant_of_the_Hessian rel=noopener>The determinant of the Hessian</a></li><li><a href=https://en.wikipedia.org/wiki/Blob_detection#The_hybrid_Laplacian_and_determinant_of_the_Hessian_operator_%28Hessian-Laplace%29 rel=noopener>The hybrid Laplacian and determinant of the Hessian operator (Hessian-Laplace)</a></li><li><a href=https://en.wikipedia.org/wiki/Maximally_stable_extremal_regions rel=noopener>Maximally stable extremal regions (MSER)</a></li></ul><h5 id=edge-detection>Edge detection</h5><ul><li><a href=https://en.wikipedia.org/wiki/Template_matching>https://en.wikipedia.org/wiki/Template_matching</a></li><li><a href=https://en.wikipedia.org/wiki/Canny_edge_detector rel=noopener>Canny edge detector</a><ul><li>The Canny edge detector is an edge detection operator that uses a multi-stage algorithm to detect a wide range of edges in images.</li><li>The Process of Canny edge detection algorithm can be broken down to 5 different steps:<ol><li>Apply Gaussian filter to smooth the image in order to remove the noise</li><li>Find the intensity gradients of the image</li><li>Apply non-maximum suppression to get rid of spurious response to edge detection</li><li>Apply double threshold to determine potential edges</li><li>Track edge by hysteresis: Finalize the detection of edges by suppressing all the other edges that are weak and not connected to strong edges.</li></ol></li><li><a href=http://aishack.in/tutorials/canny-edge-detector/>http://aishack.in/tutorials/canny-edge-detector/</a></li></ul></li><li>Robert cross<ul><li><a href=https://en.wikipedia.org/wiki/Roberts_cross>https://en.wikipedia.org/wiki/Roberts_cross</a></li><li>The Roberts cross operator is used in image processing and computer vision for edge detection.</li><li>As a differential operator, the idea behind the Roberts cross operator is to approximate the gradient of an image through discrete differentiation which is achieved by computing the sum of the squares of the differences between diagonally adjacent pixels.</li></ul></li><li>Prewitt operator<ul><li><a href=https://en.wikipedia.org/wiki/Prewitt_operator>https://en.wikipedia.org/wiki/Prewitt_operator</a></li><li>The Prewitt operator is used in image processing, particularly within edge detection algorithms. Technically, it is a discrete differentiation operator, computing an approximation of the gradient of the image intensity function. At each point in the image, the result of the Prewitt operator is either the corresponding gradient vector or the norm of this vector. The Prewitt operator is based on convolving the image with a small, separable, and integer valued filter in horizontal and vertical directions and is therefore relatively inexpensive in terms of computations like Sobel and Kayyali operators. On the other hand, the gradient approximation which it produces is relatively crude, in particular for high frequency variations in the image.</li></ul></li><li>Deriche edge detector<ul><li><a href=https://en.wikipedia.org/wiki/Deriche_edge_detector>https://en.wikipedia.org/wiki/Deriche_edge_detector</a></li><li>The Prewitt operator is used in image processing, particularly within edge detection algorithms. Technically, it is a discrete differentiation operator, computing an approximation of the gradient of the image intensity function. At each point in the image, the result of the Prewitt operator is either the corresponding gradient vector or the norm of this vector. The Prewitt operator is based on convolving the image with a small, separable, and integer valued filter in horizontal and vertical directions and is therefore relatively inexpensive in terms of computations like Sobel and Kayyali operators. On the other hand, the gradient approximation which it produces is relatively crude, in particular for high frequency variations in the image.</li></ul></li></ul><h5 id=corner-detection>Corner detection</h5><ul><li><a href=https://en.wikipedia.org/wiki/Corner_detection>https://en.wikipedia.org/wiki/Corner_detection</a></li><li><a href=http://aishack.in/tutorials/corner-detection-opencv/>http://aishack.in/tutorials/corner-detection-opencv/</a></li><li>Corner detection is an approach used within computer vision systems to extract certain kinds of features and infer the contents of an image.</li><li><a href=https://en.wikipedia.org/wiki/Harris_Corner_Detector rel=noopener>Harris operator: detects corners (patches that have strong gradients in two orthogonal directions)</a></li><li><a href=https://en.wikipedia.org/wiki/Corner_detection#The_F%C3%B6rstner_corner_detector rel=noopener>F√∂rstner corner detector</a></li><li><a href=https://en.wikipedia.org/wiki/Corner_detection#The_Wang_and_Brady_corner_detection_algorithm rel=noopener>The Wang and Brady corner detection algorithm</a></li><li><a href=https://en.wikipedia.org/wiki/Corner_detection#The_SUSAN_corner_detector rel=noopener>The SUSAN corner detector</a></li><li><a href=https://en.wikipedia.org/wiki/Corner_detection#The_Trajkovic_and_Hedley_corner_detector rel=noopener>The Trajkovic and Hedley corner detector</a></li></ul><h5 id=feature-descriptors>Feature descriptors</h5><ul><li>Scale-invariant feature transform (SIFT)<ul><li><a href=http://aishack.in/tutorials/sift-scale-invariant-feature-transform-introduction/>http://aishack.in/tutorials/sift-scale-invariant-feature-transform-introduction/</a></li><li>SIFT keypoints of objects are first extracted from a set of reference images and stored in a database. An object is recognized in a new image by individually comparing each feature from the new image to this database and finding candidate matching features based on Euclidean distance of their feature vectors. From the full set of matches, subsets of keypoints that agree on the object and its location, scale, and orientation in the new image are identified to filter out good matches. The determination of consistent clusters is performed rapidly by using an efficient hash table implementation of the generalised Hough transform. Each cluster of 3 or more features that agree on an object and its pose is then subject to further detailed model verification and subsequently outliers are discarded. Finally the probability that a particular set of features indicates the presence of an object is computed, given the accuracy of fit and number of probable false matches. Object matches that pass all these tests can be identified as correct with high confidence.</li><li><a href=https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_sift_intro/py_sift_intro.html>https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_sift_intro/py_sift_intro.html</a></li><li>There are mainly five steps involved in SIFT algorithm.<ol><li>Scale-space Extrema Detection (using DoG)</li><li>Keypoint Localization</li><li>Orientation Assignment</li><li>Keypoint Descriptor</li><li>Keypoint Matching</li></ol></li></ul></li><li><a href=https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients rel=noopener>Histogram of oriented gradients (HOG)</a></li><li>Speeded up robust features (SURF)<ul><li><a href=https://en.wikipedia.org/wiki/Speeded_up_robust_features>https://en.wikipedia.org/wiki/Speeded_up_robust_features</a></li><li>In computer vision, speeded up robust features (SURF) is a patented local feature detector and descriptor. It can be used for tasks such as object recognition, image registration, classification or 3D reconstruction. It is partly inspired by the scale-invariant feature transform (SIFT) descriptor. The standard version of SURF is several times faster than SIFT and claimed by its authors to be more robust against different image transformations than SIFT.</li></ul></li></ul><h5 id=kanadelucastomasi-klt-feature-tracker>Kanade‚ÄìLucas‚ÄìTomasi (KLT) feature tracker</h5><ul><li><a href=https://en.wikipedia.org/wiki/Kanade%E2%80%93Lucas%E2%80%93Tomasi_feature_tracker>https://en.wikipedia.org/wiki/Kanade%E2%80%93Lucas%E2%80%93Tomasi_feature_tracker</a></li><li>In computer vision, the Kanade‚ÄìLucas‚ÄìTomasi (KLT) feature tracker is an approach to feature extraction. It is proposed mainly for the purpose of dealing with the problem that traditional image registration techniques are generally costly. KLT makes use of spatial intensity information to direct the search for the position that yields the best match. It is faster than traditional techniques for examining far fewer potential matches between the images.</li><li>Summary of KLT tracking:<ul><li>Find a good point to track (harriscorner)</li><li>Use intensity second moment matrix and difference across frames to find displacement</li><li>Iterate and use coarse-to-fine search to deal with larger movements</li><li>When creating long tracks, check appearance of registered patch against appearance of initial patch to find points that have drifted</li></ul></li></ul><h5 id=optical-flow>Optical flow</h5><ul><li><a href=https://en.wikipedia.org/wiki/Optical_flow>https://en.wikipedia.org/wiki/Optical_flow</a></li><li>Vector field function of the spatio-temporal image brightness variations</li><li>#PAPER
<a href=https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/papers/brox_cvpr09.pdf rel=noopener>Large Displacement Optical Flow</a></li></ul></article><hr><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=https://carlos-gg.github.io/digitalgarden/AI/AI>Artificial Intelligence</a></li><li><a href=https://carlos-gg.github.io/digitalgarden/AI/Data-Science-Data-Engineering/Data-engineering-and-computer-science>Data Engineering and Computer Science</a></li><li><a href=https://carlos-gg.github.io/digitalgarden/AI/Unsupervised-learning/Clustering>Clustering</a></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script>async function run(){const{index:a,links:p,content:u}=await fetchData(),j="https://carlos-gg.github.io/digitalgarden/AI/Computer-Vision/Computer-vision".replace("https://carlos-gg.github.io/digitalgarden",""),n=[{"/moc":"#4388cc"}];let h=-1;const m=a=>[...new Set(a.flatMap(a=>[a.source,a.target]))],e=new Set,f=[j||"/","__SENTINEL"];if(h>=0)while(h>=0&&f.length>0){const b=f.shift();if(b==="__SENTINEL")h--,f.push("__SENTINEL");else{e.add(b);const c=a.links[b]||[],d=a.backlinks[b]||[];f.push(...c.map(a=>a.target),...d.map(a=>a.source))}}else m(p).forEach(a=>e.add(a));const g={nodes:[...e].map(a=>({id:a})),links:p.filter(a=>e.has(a.source)&&e.has(a.target))},k=a=>{if(a.id===j||a.id==="/"&&j==="")return"var(--g-node-active)";for(const b of n){const c=Object.keys(b)[0],d=b[c];if(a.id.startsWith(c))return d}return"var(--g-node)"},l=c=>{function d(b,a){b.active||c.alphaTarget(1).restart(),a.fx=a.x,a.fy=a.y}function e(a,b){b.fx=a.x,b.fy=a.y}function f(b,a){b.active||c.alphaTarget(0),a.fx=null,a.fy=null}const a=!0,b=()=>{};return d3.drag().on("start",a?d:b).on("drag",a?e:b).on("end",a?f:b)},c=250,b=document.getElementById("graph-container").offsetWidth,i=d3.forceSimulation(g.nodes).force("charge",d3.forceManyBody().strength(-30)).force("link",d3.forceLink(g.links).id(a=>a.id)).force("center",d3.forceCenter()),d=d3.select('#graph-container').append('svg').attr('width',b).attr('height',c).attr("viewBox",[-b/2,-c/2,b,c]),t=!1;if(t){const a=[{Current:"var(--g-node-active)"},{Note:"var(--g-node)"},...n];a.forEach((a,e)=>{const f=Object.keys(a)[0],g=a[f];d.append("circle").attr("cx",-b/2+20).attr("cy",c/2-30*(e+1)).attr("r",6).style("fill",g),d.append("text").attr("x",-b/2+40).attr("y",c/2-30*(e+1)).text(f).style("font-size","15px").attr("alignment-baseline","middle")})}const r=d.append("g").selectAll("line").data(g.links).join("line").attr("class","link").attr("stroke","var(--g-link)").attr("stroke-width",2).attr("data-source",a=>a.source.id).attr("data-target",a=>a.target.id),s=d.append("g").selectAll("g").data(g.nodes).enter().append("g"),q=s.append("circle").attr("class","node").attr("id",a=>a.id).attr("r",b=>{const c=a.links[b.id]?.length||0,d=a.backlinks[b.id]?.length||0;return 3+(c+d)/4}).attr("fill",k).style("cursor","pointer").on("click",(b,a)=>{window.location.href="https://carlos-gg.github.io/digitalgarden"+decodeURI(a.id).replace(/\s+/g,'-')}).on("mouseover",function(g,b){d3.selectAll(".node").transition().duration(100).attr("fill","var(--g-node-inactive)");const d=m([...a.links[b.id]||[],...a.backlinks[b.id]||[]]),e=d3.selectAll(".node").filter(a=>d.includes(a.id)),c=b.id,f=d3.selectAll(".link").filter(a=>a.source.id===c||a.target.id===c);e.transition().duration(200).attr("fill",k),f.transition().duration(200).attr("stroke","var(--g-link-active)"),d3.select(this.parentNode).select("text").raise().transition().duration(200).style("opacity",1)}).on("mouseleave",function(d,b){d3.selectAll(".node").transition().duration(200).attr("fill",k);const a=b.id,c=d3.selectAll(".link").filter(b=>b.source.id===a||b.target.id===a);c.transition().duration(200).attr("stroke","var(--g-link)"),d3.select(this.parentNode).select("text").transition().duration(200).style("opacity",0)}).call(l(i)),o=s.append("text").attr("dx",12).attr("dy",".35em").text(a=>u[decodeURI(a.id).replace(/\s+/g,'-')]?.title||"Untitled").style("opacity",0).style("pointer-events","none").call(l(i)),v=!0;v&&d.call(d3.zoom().extent([[0,0],[b,c]]).scaleExtent([.25,4]).on("zoom",({transform:a})=>{r.attr("transform",a),q.attr("transform",a),o.attr("transform",a)})),i.on("tick",()=>{r.attr("x1",a=>a.source.x).attr("y1",a=>a.source.y).attr("x2",a=>a.target.x).attr("y2",a=>a.target.y),q.attr("cx",a=>a.x).attr("cy",a=>a.y),o.attr("x",a=>a.x).attr("y",a=>a.y)})}run()</script></div></div><div id=contact_buttons><footer><p>Made by Carlos Alberto Gomez Gonzalez using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, ¬© 2022</p><a href=/>Home</a>
<a href=https://carlos-gg.github.io/digitalgarden/>DigitalGarden</a><a href=https://carlos-gg.github.io>CarlosGG</a></footer></div></div></body></html>