<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="See [[AI/Computer Vision/Computer vision]]
 Resources Background subtraction See [[AI/Computer Vision/Background subtraction]]
Image interpolation   https://pixinsight.com/doc/docs/InterpolationAlgorithms/InterpolationAlgorithms.html  https://www.cambridgeincolour.com/tutorials/image-interpolation.htm  https://www."><title>Traditional Computer Vision (CV) techniques</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=/icon.png><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Source+Sans+Pro:wght@400;600;700&family=Fira+Code:wght@400;700&display=swap" rel=stylesheet><link href=https://carlos-gg.github.io/digitalgarden/styles.cd61336c89ed6e03702366ce4a492b75.min.css rel=stylesheet><script src=https://carlos-gg.github.io/digitalgarden/js/darkmode.46b07878b7f5d9e26ad7a3c40f8a0605.min.js></script>
<script>const BASE_URL="https://carlos-gg.github.io/digitalgarden/",fetchData=Promise.all([fetch("https://carlos-gg.github.io/digitalgarden/indices/linkIndex.687ef89d97995715c35fb6ef6dcf5be1.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://carlos-gg.github.io/digitalgarden/indices/contentIndex.aa84b4b89e72ab61c2396a7dc9cabb65.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n}))</script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-S103D50NQ0"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S103D50NQ0",{anonymize_ip:!1})}</script><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://carlos-gg.github.io/digitalgarden/js/graph.27e8521c25c27c79dea35f434c486167.js></script>
<script>drawGraph("https://carlos-gg.github.io/digitalgarden/AI/Computer-Vision/Traditional-CV","https://carlos-gg.github.io/digitalgarden",[{"/moc":"#4388cc"}],-1,!0,!1,!0)</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://carlos-gg.github.io/digitalgarden/js/search.bc849b857f2c1b822264d40635bb67b6.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://carlos-gg.github.io/digitalgarden/>CarlosGG's Knowledge Garden 🪴</a></h1><svg tabindex="0" id="search-icon" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg><div class=spacer></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Traditional Computer Vision (CV) techniques</h1><p class=meta>Last updated May 27, 2022</p><ul class=tags></ul><aside class=mainTOC><details open><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#resources>Resources</a><ol><li><a href=#background-subtraction>Background subtraction</a></li><li><a href=#image-interpolation>Image interpolation</a></li><li><a href=#geometric-transformations>Geometric transformations</a><ol><li><a href=#affine-transformations>Affine transformations</a></li><li><a href=#projective-transformations>Projective transformations</a></li></ol></li><li><a href=#filtering>Filtering</a><ol><li><a href=#spatial-domain>Spatial domain</a></li><li><a href=#frequency-domain>Frequency domain</a></li></ol></li><li><a href=#template-matching>Template matching</a><ol><li><a href=#template-based>Template based</a></li><li><a href=#feature-based>Feature based</a></li></ol></li><li><a href=#feature-extraction>Feature extraction</a><ol><li><a href=#blob-detection>Blob detection</a></li><li><a href=#edge-detection>Edge detection</a></li><li><a href=#corner-detection>Corner detection</a></li><li><a href=#feature-descriptors>Feature descriptors</a></li><li><a href=#kanadelucastomasi-klt-feature-tracker>Kanade–Lucas–Tomasi (KLT) feature tracker</a></li><li><a href=#optical-flow>Optical flow</a></li></ol></li></ol></li></ol></nav></details></aside><blockquote><p>See <a href=/digitalgarden/AI/Computer-Vision/Computer-vision rel=noopener class=internal-link data-src=/digitalgarden/AI/Computer-Vision/Computer-vision>AI/Computer Vision/Computer vision</a></p></blockquote><h2 id=resources>Resources</h2><h3 id=background-subtraction>Background subtraction</h3><p>See <a href=/digitalgarden/AI/Computer-Vision/Background-subtraction rel=noopener class=internal-link data-src=/digitalgarden/AI/Computer-Vision/Background-subtraction>AI/Computer Vision/Background subtraction</a></p><h3 id=image-interpolation>Image interpolation</h3><ul><li><a href=https://pixinsight.com/doc/docs/InterpolationAlgorithms/InterpolationAlgorithms.html rel=noopener>https://pixinsight.com/doc/docs/InterpolationAlgorithms/InterpolationAlgorithms.html</a></li><li><a href=https://www.cambridgeincolour.com/tutorials/image-interpolation.htm rel=noopener>https://www.cambridgeincolour.com/tutorials/image-interpolation.htm</a></li><li><a href=https://www.unioviedo.es/compnum/expositive/presentations/T3C_Interpolation_image.pdf rel=noopener>https://www.unioviedo.es/compnum/expositive/presentations/T3C_Interpolation_image.pdf</a></li></ul><h3 id=geometric-transformations>Geometric transformations</h3><ul><li><a href=https://en.wikipedia.org/wiki/Geometric_transformation rel=noopener>https://en.wikipedia.org/wiki/Geometric_transformation</a></li><li><a href=https://www.graphicsmill.com/docs/gm/affine-and-projective-transformations.htm rel=noopener>https://www.graphicsmill.com/docs/gm/affine-and-projective-transformations.htm</a></li><li><a href=https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_geometric_transformations/py_geometric_transformations.html rel=noopener>https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_geometric_transformations/py_geometric_transformations.html</a></li><li><a href=http://eeweb.poly.edu/~yao/EL5123/lecture12_ImageWarping.pdf rel=noopener>http://eeweb.poly.edu/~yao/EL5123/lecture12_ImageWarping.pdf</a></li></ul><h4 id=affine-transformations>Affine transformations</h4><ul><li><a href=https://en.wikipedia.org/wiki/Affine_transformation rel=noopener>https://en.wikipedia.org/wiki/Affine_transformation</a></li><li>Affine transformations are combinations of linear transformations and translations. Properties of affine transformations:</li><li>Lines map to lines</li><li>Parallel lines remain parallel</li><li>Ratios are preserved</li><li>Closed under composition</li></ul><h4 id=projective-transformations>Projective transformations</h4><ul><li><a href=https://en.wikipedia.org/wiki/Projective_transformation rel=noopener>https://en.wikipedia.org/wiki/Projective_transformation</a></li><li>Projective transformations are combos of Affine transformations, and projective warps. Properties of projective transformations:</li><li>Lines map to lines</li><li>Parallel lines do not necessarily remain parallel</li><li>Ratios are not preserved</li><li>Closed under composition</li><li>Models change of basis</li><li>Projective matrix is defined up to a scale (8 DOF)</li></ul><h3 id=filtering>Filtering</h3><ul><li>For each pixel we compute a function of local neighborhood and output a new value. Use cases:<ul><li>Enhance images: Denoise, smooth, increase contrast, etc.</li><li>Extract information from images: Texture, edges, distinctive points, etc.</li><li>Detect patterns: Template matching</li></ul></li></ul><h4 id=spatial-domain>Spatial domain</h4><ul><li><a href=https://en.wikipedia.org/wiki/Kernel_%28image_processing%29 rel=noopener>https://en.wikipedia.org/wiki/Kernel_(image_processing)</a></li><li><a href="https://www.youtube.com/watch?v=8rrHTtUzyZA" rel=noopener>Convolutions in image processing</a></li><li><a href=https://en.wikipedia.org/wiki/Kernel_%28image_processing%29 rel=noopener>Convolution and kernels</a></li><li><a href=http://setosa.io/ev/image-kernels/ rel=noopener>Image Kernels visualization</a></li><li><a href=https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_gradients/py_gradients.html rel=noopener>OpenCV - Image Gradients</a></li><li><a href=http://scikit-image.org/docs/dev/api/skimage.filters.html rel=noopener>http://scikit-image.org/docs/dev/api/skimage.filters.html</a></li><li><a href=https://dsp.stackexchange.com/questions/12684/difference-between-correlation-and-convolution-on-an-image rel=noopener>https://dsp.stackexchange.com/questions/12684/difference-between-correlation-and-convolution-on-an-image</a></li><li>Examples:<ul><li><a href=https://en.wikipedia.org/wiki/Box_blur rel=noopener>Box filter. Replaces each pixel with an average of its neighborhood (smoothing effect)</a></li><li>Sharpening filter. Accentuates differences with local average.<ul><li><a href=http://northstar-www.dartmouth.edu/doc/idl/html_6.2/Sharpening_an_Image.html rel=noopener>http://northstar-www.dartmouth.edu/doc/idl/html_6.2/Sharpening_an_Image.html</a></li><li><a href=https://en.wikipedia.org/wiki/Box_blur rel=noopener>https://en.wikipedia.org/wiki/Box_blur</a></li></ul></li><li>Sobel filter. This filter is used in image processing and computer vision, particularly within edge detection algorithms where it creates an image emphasising edges.<ul><li><a href=http://aishack.in/tutorials/sobel-laplacian-edge-detectors/ rel=noopener>http://aishack.in/tutorials/sobel-laplacian-edge-detectors/</a></li><li><a href=https://en.wikipedia.org/wiki/Sobel_operator rel=noopener>https://en.wikipedia.org/wiki/Sobel_operator</a></li></ul></li><li><a href=https://en.wikipedia.org/wiki/Gaussian_filter rel=noopener>Gaussian filter. Smoothing. Remove “high-frequency” components from the image (low-pass filter)</a></li><li><a href=https://en.wikipedia.org/wiki/Median_filter rel=noopener>Median filter. Non linear filter for image smoothing. Robustness to outliers</a></li><li><a href=https://en.wikipedia.org/wiki/Bilateral_filter rel=noopener>Bilateral filter. A bilateral filter is a non-linear, edge-preserving, and noise-reducing smoothing filter for images. It replaces the intensity of each pixel with a weighted average of intensity values from nearby pixels</a></li><li>Laplacian filter. Filtering with a Laplacian operator:<ul><li><a href=http://aishack.in/tutorials/sobel-laplacian-edge-detectors/ rel=noopener>http://aishack.in/tutorials/sobel-laplacian-edge-detectors/</a></li><li><a href=https://docs.opencv.org/2.4/doc/tutorials/imgproc/imgtrans/laplace_operator/laplace_operator.html rel=noopener>https://docs.opencv.org/2.4/doc/tutorials/imgproc/imgtrans/laplace_operator/laplace_operator.html</a></li></ul></li></ul></li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>0	 1	  0
</span></span><span class=line><span class=cl>1 	-4	  1
</span></span><span class=line><span class=cl>0 	 1	  0
</span></span></code></pre></td></tr></table></div></div><h4 id=frequency-domain>Frequency domain</h4><ul><li>Fourier transform stores the magnitude and phase at each frequency. The magnitude encodes how much signal there is at a particular frequency while the phase encodes spatial information (indirectly).</li><li>The Convolution Theorem:<ul><li>The Fourier transform of the convolution of two functions is the product of their Fourier transforms</li><li>The inverse Fourier transform of the product of two Fourier transforms is the convolution of the two inverse Fourier transforms</li><li>Convolution in spatial domain is equivalent to multiplication in frequency domain</li></ul></li></ul><h3 id=template-matching>Template matching</h3><ul><li><a href=https://en.wikipedia.org/wiki/Template_matching rel=noopener>https://en.wikipedia.org/wiki/Template_matching</a></li></ul><h4 id=template-based>Template based</h4><ul><li>See the &ldquo;Filtering&rdquo; section</li><li><a href=http://aishack.in/tutorials/template-matching/ rel=noopener>http://aishack.in/tutorials/template-matching/</a></li><li>For templates without strong features, or for when the bulk of the template image constitutes the matching image, a template-based approach may be effective.</li><li>Cross-correlation. Linear filtering: function is a weighted sum/difference of pixel values (dot products at each position)<ul><li><a href=https://en.wikipedia.org/wiki/Cross-correlation rel=noopener>https://en.wikipedia.org/wiki/Cross-correlation</a></li></ul></li><li><a href=https://en.wikipedia.org/wiki/Pyramid_%28image_processing%29 rel=noopener>Using image pyramids</a></li></ul><h4 id=feature-based>Feature based</h4><ul><li>Feature-based approach relies on the extraction of image features such, i.e. shapes, textures , colors, to match in the target image or frame. This approach is currently achieved by using Neural Networks and Deep Learning classifiers such as VGG, AlexNet, ResNet. Deep Convolutional Neural Networks process the image by passing it through different hidden layers and at each layer produce a vector with classification information about the image. These vectors are extracted from the network and are used as the features of the image. Feature extraction by using Deep Neural Networks is extremely effective and thus is the standard in state of the art template matching algorithms.</li><li>This method is considered more robust and is state of the art as it can match templates with non-rigid and out of plane transformation, it can match with high background clutter and illumination changes.</li></ul><h3 id=feature-extraction>Feature extraction</h3><ul><li><a href=https://freecontent.manning.com/the-computer-vision-pipeline-part-4-feature-extraction/ rel=noopener>The Computer Vision Pipeline: feature extraction</a><ul><li>Feature extraction is a core component of the computer vision pipeline. In fact, the entire deep learning model works around the idea of extracting useful features which clearly define the objects in the image. We’re going to spend a little more time here because it’s important that you understand what a feature is, what a vector of features is, and why we extract features.</li><li>A feature in Machine Learning is an individual measurable property or characteristic of a phenomenon being observed. Features are the input that you feed to your machine learning model to output a prediction or classification. Suppose you want to predict the price of a house, your input features (properties) might include: square_foot, number_of_rooms, bathrooms, etc. and the model will output the predicted price based on the values of your features. Selecting good features that clearly distinguish your objects increases the predictive power of machine learning algorithms.</li></ul></li><li>In image processing, algorithms are used to detect and isolate various desired portions or shapes (features) of a digitized image or video stream. It is particularly important in the area of optical character recognition.<ul><li>Low-level: Edge detection, Corner detection, Blob detection, Ridge detection, Scale-invariant feature transform,</li><li>Curvature: Edge direction, changing intensity, autocorrelation</li><li>Image motion: Motion detection. Area based, differential approach,
<a href=https://en.wikipedia.org/wiki/Optical_flow rel=noopener>Optical flow</a></li><li>Shape based: Thresholding, Blob extraction, Template matching, Hough transform<ul><li>Lines: Circles/ellipses, Arbitrary shapes (generalized Hough transform). Works with any parameterizable feature (class variables, cluster detection, etc..)</li></ul></li><li>Flexible methods: Deformable, parameterized shapesActive contours (snakes)</li></ul></li></ul><h4 id=blob-detection>Blob detection</h4><ul><li><a href=https://en.wikipedia.org/wiki/Blob_detection rel=noopener>https://en.wikipedia.org/wiki/Blob_detection</a></li><li>In computer vision, blob detection methods are aimed at detecting regions in a digital image that differ in properties, such as brightness or color, compared to surrounding regions. Informally, a blob is a region of an image in which some properties are constant or approximately constant; all the points in a blob can be considered in some sense to be similar to each other.</li><li><a href=https://en.wikipedia.org/wiki/Blob_detection#The_Laplacian_of_Gaussian rel=noopener>The Laplacian of Gaussian</a></li><li><a href=https://en.wikipedia.org/wiki/Difference_of_Gaussians rel=noopener>The difference of Gaussians</a></li><li><a href=https://en.wikipedia.org/wiki/Blob_detection#The_determinant_of_the_Hessian rel=noopener>The determinant of the Hessian</a></li><li><a href=https://en.wikipedia.org/wiki/Blob_detection#The_hybrid_Laplacian_and_determinant_of_the_Hessian_operator_%28Hessian-Laplace%29 rel=noopener>The hybrid Laplacian and determinant of the Hessian operator (Hessian-Laplace)</a></li><li><a href=https://en.wikipedia.org/wiki/Maximally_stable_extremal_regions rel=noopener>Maximally stable extremal regions (MSER)</a></li></ul><h4 id=edge-detection>Edge detection</h4><ul><li><a href=https://en.wikipedia.org/wiki/Template_matching rel=noopener>https://en.wikipedia.org/wiki/Template_matching</a></li><li><a href=https://en.wikipedia.org/wiki/Canny_edge_detector rel=noopener>Canny edge detector</a><ul><li>The Canny edge detector is an edge detection operator that uses a multi-stage algorithm to detect a wide range of edges in images.</li><li>The Process of Canny edge detection algorithm can be broken down to 5 different steps:<ol><li>Apply Gaussian filter to smooth the image in order to remove the noise</li><li>Find the intensity gradients of the image</li><li>Apply non-maximum suppression to get rid of spurious response to edge detection</li><li>Apply double threshold to determine potential edges</li><li>Track edge by hysteresis: Finalize the detection of edges by suppressing all the other edges that are weak and not connected to strong edges.</li></ol></li><li><a href=http://aishack.in/tutorials/canny-edge-detector/ rel=noopener>http://aishack.in/tutorials/canny-edge-detector/</a></li></ul></li><li>Robert cross<ul><li><a href=https://en.wikipedia.org/wiki/Roberts_cross rel=noopener>https://en.wikipedia.org/wiki/Roberts_cross</a></li><li>The Roberts cross operator is used in image processing and computer vision for edge detection.</li><li>As a differential operator, the idea behind the Roberts cross operator is to approximate the gradient of an image through discrete differentiation which is achieved by computing the sum of the squares of the differences between diagonally adjacent pixels.</li></ul></li><li>Prewitt operator<ul><li><a href=https://en.wikipedia.org/wiki/Prewitt_operator rel=noopener>https://en.wikipedia.org/wiki/Prewitt_operator</a></li><li>The Prewitt operator is used in image processing, particularly within edge detection algorithms. Technically, it is a discrete differentiation operator, computing an approximation of the gradient of the image intensity function. At each point in the image, the result of the Prewitt operator is either the corresponding gradient vector or the norm of this vector. The Prewitt operator is based on convolving the image with a small, separable, and integer valued filter in horizontal and vertical directions and is therefore relatively inexpensive in terms of computations like Sobel and Kayyali operators. On the other hand, the gradient approximation which it produces is relatively crude, in particular for high frequency variations in the image.</li></ul></li><li>Deriche edge detector<ul><li><a href=https://en.wikipedia.org/wiki/Deriche_edge_detector rel=noopener>https://en.wikipedia.org/wiki/Deriche_edge_detector</a></li><li>The Prewitt operator is used in image processing, particularly within edge detection algorithms. Technically, it is a discrete differentiation operator, computing an approximation of the gradient of the image intensity function. At each point in the image, the result of the Prewitt operator is either the corresponding gradient vector or the norm of this vector. The Prewitt operator is based on convolving the image with a small, separable, and integer valued filter in horizontal and vertical directions and is therefore relatively inexpensive in terms of computations like Sobel and Kayyali operators. On the other hand, the gradient approximation which it produces is relatively crude, in particular for high frequency variations in the image.</li></ul></li></ul><h4 id=corner-detection>Corner detection</h4><ul><li><a href=https://en.wikipedia.org/wiki/Corner_detection rel=noopener>https://en.wikipedia.org/wiki/Corner_detection</a></li><li><a href=http://aishack.in/tutorials/corner-detection-opencv/ rel=noopener>http://aishack.in/tutorials/corner-detection-opencv/</a></li><li>Corner detection is an approach used within computer vision systems to extract certain kinds of features and infer the contents of an image.</li><li><a href=https://en.wikipedia.org/wiki/Harris_Corner_Detector rel=noopener>Harris operator: detects corners (patches that have strong gradients in two orthogonal directions)</a></li><li><a href=https://en.wikipedia.org/wiki/Corner_detection#The_F%C3%B6rstner_corner_detector rel=noopener>Förstner corner detector</a></li><li><a href=https://en.wikipedia.org/wiki/Corner_detection#The_Wang_and_Brady_corner_detection_algorithm rel=noopener>The Wang and Brady corner detection algorithm</a></li><li><a href=https://en.wikipedia.org/wiki/Corner_detection#The_SUSAN_corner_detector rel=noopener>The SUSAN corner detector</a></li><li><a href=https://en.wikipedia.org/wiki/Corner_detection#The_Trajkovic_and_Hedley_corner_detector rel=noopener>The Trajkovic and Hedley corner detector</a></li></ul><h4 id=feature-descriptors>Feature descriptors</h4><ul><li>Scale-invariant feature transform (SIFT)<ul><li><a href=http://aishack.in/tutorials/sift-scale-invariant-feature-transform-introduction/ rel=noopener>http://aishack.in/tutorials/sift-scale-invariant-feature-transform-introduction/</a></li><li>SIFT keypoints of objects are first extracted from a set of reference images and stored in a database. An object is recognized in a new image by individually comparing each feature from the new image to this database and finding candidate matching features based on Euclidean distance of their feature vectors. From the full set of matches, subsets of keypoints that agree on the object and its location, scale, and orientation in the new image are identified to filter out good matches. The determination of consistent clusters is performed rapidly by using an efficient hash table implementation of the generalised Hough transform. Each cluster of 3 or more features that agree on an object and its pose is then subject to further detailed model verification and subsequently outliers are discarded. Finally the probability that a particular set of features indicates the presence of an object is computed, given the accuracy of fit and number of probable false matches. Object matches that pass all these tests can be identified as correct with high confidence.</li><li><a href=https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_sift_intro/py_sift_intro.html rel=noopener>https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_sift_intro/py_sift_intro.html</a></li><li>There are mainly five steps involved in SIFT algorithm.<ol><li>Scale-space Extrema Detection (using DoG)</li><li>Keypoint Localization</li><li>Orientation Assignment</li><li>Keypoint Descriptor</li><li>Keypoint Matching</li></ol></li></ul></li><li><a href=https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients rel=noopener>Histogram of oriented gradients (HOG)</a></li><li>Speeded up robust features (SURF)<ul><li><a href=https://en.wikipedia.org/wiki/Speeded_up_robust_features rel=noopener>https://en.wikipedia.org/wiki/Speeded_up_robust_features</a></li><li>In computer vision, speeded up robust features (SURF) is a patented local feature detector and descriptor. It can be used for tasks such as object recognition, image registration, classification or 3D reconstruction. It is partly inspired by the scale-invariant feature transform (SIFT) descriptor. The standard version of SURF is several times faster than SIFT and claimed by its authors to be more robust against different image transformations than SIFT.</li></ul></li></ul><h4 id=kanadelucastomasi-klt-feature-tracker>Kanade–Lucas–Tomasi (KLT) feature tracker</h4><ul><li><a href=https://en.wikipedia.org/wiki/Kanade%E2%80%93Lucas%E2%80%93Tomasi_feature_tracker rel=noopener>https://en.wikipedia.org/wiki/Kanade%E2%80%93Lucas%E2%80%93Tomasi_feature_tracker</a></li><li>In computer vision, the Kanade–Lucas–Tomasi (KLT) feature tracker is an approach to feature extraction. It is proposed mainly for the purpose of dealing with the problem that traditional image registration techniques are generally costly. KLT makes use of spatial intensity information to direct the search for the position that yields the best match. It is faster than traditional techniques for examining far fewer potential matches between the images.</li><li>Summary of KLT tracking:<ul><li>Find a good point to track (harriscorner)</li><li>Use intensity second moment matrix and difference across frames to find displacement</li><li>Iterate and use coarse-to-fine search to deal with larger movements</li><li>When creating long tracks, check appearance of registered patch against appearance of initial patch to find points that have drifted</li></ul></li></ul><h4 id=optical-flow>Optical flow</h4><ul><li><a href=https://en.wikipedia.org/wiki/Optical_flow rel=noopener>https://en.wikipedia.org/wiki/Optical_flow</a></li><li>Vector field function of the spatio-temporal image brightness variations</li><li>#PAPER
<a href=https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/papers/brox_cvpr09.pdf rel=noopener>Large Displacement Optical Flow</a></li></ul></article><hr><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div></div><div id=contact_buttons><footer><p>Made by Carlos Alberto Gomez Gonzalez using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2022</p><ul><li><a href=https://carlos-gg.github.io/digitalgarden/>Home</a></li><a href=https://carlos-gg.github.io>Carlos'Homepage</a></ul></footer></div><script src=https://carlos-gg.github.io/digitalgarden/js/popover.e57188d2e4c06b0654e020b3a734bb62.min.js></script>
<script>initPopover("https://carlos-gg.github.io/digitalgarden")</script></div></body></html>