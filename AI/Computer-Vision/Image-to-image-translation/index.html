<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="The task of Image-to-image translation is to learn the mapping from a given image (X) to a specific target image (Y), e."><title>Image-to-image translation</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=/icon.png><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Source+Sans+Pro:wght@400;600;700&family=Fira+Code:wght@400;700&display=swap" rel=stylesheet><link href=https://carlos-gg.github.io/digitalgarden/styles.cd61336c89ed6e03702366ce4a492b75.min.css rel=stylesheet><script src=https://carlos-gg.github.io/digitalgarden/js/darkmode.46b07878b7f5d9e26ad7a3c40f8a0605.min.js></script>
<script>const BASE_URL="https://carlos-gg.github.io/digitalgarden/",fetchData=Promise.all([fetch("https://carlos-gg.github.io/digitalgarden/indices/linkIndex.687ef89d97995715c35fb6ef6dcf5be1.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://carlos-gg.github.io/digitalgarden/indices/contentIndex.cd589cbd407a3172336486081d3aaad2.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n}))</script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-S103D50NQ0"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S103D50NQ0",{anonymize_ip:!1})}</script><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://carlos-gg.github.io/digitalgarden/js/graph.27e8521c25c27c79dea35f434c486167.js></script>
<script>drawGraph("https://carlos-gg.github.io/digitalgarden/AI/Computer-Vision/Image-to-image-translation","https://carlos-gg.github.io/digitalgarden",[{"/moc":"#4388cc"}],-1,!0,!1,!0)</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://carlos-gg.github.io/digitalgarden/js/search.bc849b857f2c1b822264d40635bb67b6.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://carlos-gg.github.io/digitalgarden/>CarlosGG's Knowledge Garden ü™¥</a></h1><svg tabindex="0" id="search-icon" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg><div class=spacer></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Image-to-image translation</h1><p class=meta>Last updated June 8, 2022</p><ul class=tags></ul><aside class=mainTOC><details open><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#resources>Resources</a></li><li><a href=#references>References</a><ol><li><a href=#cnn-based>CNN-based</a></li><li><a href=#gan-based>GAN-based</a><ol><li><a href=#paired-supervised-translation>Paired (supervised) translation</a></li><li><a href=#unpaired-unsupervised-translation>Unpaired (unsupervised) translation</a></li></ol></li><li><a href=#flow-based>Flow-based</a></li></ol></li></ol></nav></details></aside><blockquote><p>The task of Image-to-image translation is to learn the mapping from a given image (X) to a specific target image (Y), e.g., mapping grayscale images to RGB images</p></blockquote><h2 id=resources>Resources</h2><ul><li>Learning the mapping from one visual representation to another requires an understanding of underlying features that are shared between these representations, such features are either domain-independent or domain-specific.</li><li><a href=https://paperswithcode.com/task/image-to-image-translation rel=noopener>https://paperswithcode.com/task/image-to-image-translation</a></li><li><a href=https://github.com/weihaox/awesome-image-translation rel=noopener>https://github.com/weihaox/awesome-image-translation</a></li><li><a href=https://towardsdatascience.com/deep-domain-adaptation-in-computer-vision-8da398d3167f rel=noopener>Deep Domain Adaptation In Computer Vision</a></li></ul><h2 id=references>References</h2><p>Review papers:</p><ul><li>#PAPER
<a href=https://arxiv.org/abs/2101.08629 rel=noopener>Image-to-Image Translation: Methods and Applications (Pang 2021)</a></li></ul><h3 id=cnn-based>CNN-based</h3><p>See <a href=/digitalgarden/AI/Deep-learning/Encoder-decoder-networks rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Encoder-decoder-networks>AI/Deep learning/Encoder-decoder networks</a></p><ul><li>Related to the task of supervised semantic segmentation but changing the Y and the loss (MAE, MSE or other reconstruction loss)</li></ul><h3 id=gan-based>GAN-based</h3><p>See &ldquo;GANs for representation learning and image synthesis&rdquo; section in <a href=/digitalgarden/AI/Deep-learning/GANs rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/GANs>AI/Deep learning/GANs</a></p><ul><li>#PAPER
<a href=https://www.mdpi.com/2073-8994/12/10/1705/htm# rel=noopener>Deep Generative Adversarial Networks for Image-to-Image Translation: A Review (Alotaibi 2020)</a> ^I2IGANs20<ul><li>The powerful ability of deep feature learning to automatically utilize complex and high-level feature representations has significantly advanced the performance of state-of-the-art methods across computer applications</li><li>The underlying structure and distinctive (complex) features are both discovered via deep learning-based methods that can be classified further into discriminative feature-learning algorithms and generative feature-learning algorithms</li><li>Discriminative models focus on the classification-learning process by learning the conditional probability p (x|y) to map input x to class label y. One of the most popular methods used for image feature learning utilizes convolutional neural networks (CNN) for feature extraction and image classification (LeNet, AlexNet, VGGNet, ResNet and other supervised learning algorithms)</li><li>Generative models focus on the data distribution to discover the underlying features from large amounts of data in an unsupervised setting. Such models are able to generate new samples by learning the estimation of the joint probability distribution p (x,y) and predicting y</li><li>The most dominant and efficient deep generative models of recent years have been VAE and GAN. A variational autoencoder learns the underlying probability distribution and generates a new sample that is based on Bayesian inference by maximizing the lower bound of the data‚Äôs log-likelihood. In contrast, generative adversarial networks learn data distributions through the adversarial training process based on game theory instead of maximizing the likelihood.</li><li>I2I methods:<ul><li>Supervised<ul><li>Directional translation (Pix2Pix, StarGAN)</li><li>Bidirectional translation (BicycleGAN, CEGAN)</li></ul></li><li>Unsupervised<ul><li>Cyclic consistency (CycleGAN, DiscoGAN, DualGAN, QGAN, XGAN)</li><li>Autoencoder-based (UNIT, BranchGAN)</li><li>Disentangler representation (MUNIT, DIRT, DosGAN)</li></ul></li></ul></li></ul></li></ul><h4 id=paired-supervised-translation>Paired (supervised) translation</h4><ul><li>#PAPER
<a href=https://arxiv.org/abs/1611.07004 rel=noopener>Image-to-Image Translation with Conditional Adversarial Networks, pix2pix (Isola 2016)</a> ^pix2pix<ul><li>Loss function learned by the network itself instead of L2, L1 norms</li><li>UNET generator, CNN discriminator</li><li>Euclidean distance is minimized by averaging all plausible outputs, which causes blurring. Coming up with loss functions that force the CNN to do what we really want‚Äì e.g., output sharp, realistic images ‚Äì is an open problem and generally requires expert knowledge</li><li>Evaluating the quality of synthesized images is an open and difficult problem. Traditional metrics such as per-pixel mean-squared error do not assess joint statistics of the result, and therefore do not measure the very structure that structured losses aim to capture</li><li>#CODE
<a href=https://github.com/phillipi/pix2pix rel=noopener>https://github.com/phillipi/pix2pix</a></li><li>#CODE
<a href=https://www.tensorflow.org/tutorials/generative/pix2pix rel=noopener>https://www.tensorflow.org/tutorials/generative/pix2pix</a></li><li>#CODE
<a href=https://github.com/He-Jian/pix2pix-keras rel=noopener>https://github.com/He-Jian/pix2pix-keras</a></li><li><a href=https://affinelayer.com/pixsrv/ rel=noopener>https://affinelayer.com/pixsrv/</a></li><li><a href=https://medium.com/deep-math-machine-learning-ai/ch-14-2-pix2pix-gan-and-cycle-gan-55cd84318fb8 rel=noopener>https://medium.com/deep-math-machine-learning-ai/ch-14-2-pix2pix-gan-and-cycle-gan-55cd84318fb8</a></li><li><a href=https://machinelearningmastery.com/how-to-develop-a-conditional-generative-adversarial-network-from-scratch/ rel=noopener>https://machinelearningmastery.com/how-to-develop-a-conditional-generative-adversarial-network-from-scratch/</a></li><li><a href=https://machinelearningmastery.com/how-to-develop-a-pix2pix-gan-for-image-to-image-translation/ rel=noopener>https://machinelearningmastery.com/how-to-develop-a-pix2pix-gan-for-image-to-image-translation/</a></li><li><a href=https://affinelayer.com/pix2pix/ rel=noopener>Image-to-Image Translation in Tensorflow</a></li><li><a href="https://www.youtube.com/watch?v=u7kQ5lNfUfg" rel=noopener>Two minutes papers</a></li><li><a href="https://www.youtube.com/watch?v=9SGs4Nm0VR4" rel=noopener>Paper walkthrough</a></li><li>Stochastic inference:<ul><li><a href=https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/152 rel=noopener>https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/152</a><ul><li>tried a few ways of adding z to the nets, e.g., adding z to a latent state, concatenating with a latent state, applying dropout, etc. The output tended not to vary much as a function of z</li><li>see follow up paper by Zhu et al 2017 (BicycleGAN). Shows one way of getting z to actually have a substantial effect</li></ul></li><li><a href=https://machinelearningmastery.com/how-to-implement-pix2pix-gan-models-from-scratch-with-keras/ rel=noopener>https://machinelearningmastery.com/how-to-implement-pix2pix-gan-models-from-scratch-with-keras/</a><ul><li>unlike traditional generator models in the GAN architecture, the U-Net generator does not take a point from the latent space as input. Instead, dropout layers are used as a source of randomness both during training and when the model is used to make a prediction, e.g. generate an image at inference time</li><li>similarly, batch normalization is used in the same way during training and inference, meaning that statistics are calculated for each batch and not fixed at the end of the training process. This is referred to as instance normalization, specifically when the batch size is set to 1 as it is with the Pix2Pix model</li><li>&ldquo;At inference time, we run the generator net in exactly the same manner as during the training phase. This differs from the usual protocol in that we apply dropout at test time, and we apply batch normalization using the statistics of the test batch, rather than aggregated statistics of the training batch.&rdquo;</li><li>in Keras, layers like Dropout and BatchNormalization operate differently during training and in inference model. We can set the ‚Äútraining‚Äù argument when calling these layers to ‚ÄúTrue‚Äù to ensure that they always operate in training-model, even when used during inference</li></ul></li></ul></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1711.11586 rel=noopener>Toward Multimodal Image-to-Image Translation, BicycleGAN (Zhu 2017)</a> ^bicyclegan<ul><li>#CODE
<a href=https://github.com/junyanz/BicycleGAN rel=noopener>https://github.com/junyanz/BicycleGAN</a></li><li>#CODE
<a href=https://github.com/clvrai/BicycleGAN-Tensorflow rel=noopener>https://github.com/clvrai/BicycleGAN-Tensorflow</a></li><li>#CODE
<a href=https://github.com/prakashpandey9/BicycleGAN rel=noopener>https://github.com/prakashpandey9/BicycleGAN</a></li><li>Aimed to model a distribution of possible outputs in a conditional generative modeling setting</li><li>The ambiguity of the mapping is distilled in a low-dimensional latent vector, which can be randomly sampled at test time. A generator learns to map the given input, combined with this latent code, to the output.</li><li>Encouraged the connection between output and the latent code to be invertible. This helps prevent a many-to-one mapping from the latent code to the output during training, also known as the problem of mode collapse, and produces more diverse results</li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1706.05477 rel=noopener>Bayesian Conditional Generative Adverserial Networks (Ehsan Abbasnejad 2017)</a></li><li>#PAPER
<a href=https://cs230.stanford.edu/projects_spring_2018/reports/8289557.pdf rel=noopener>Image-to-image translation with conditional GAN (Hu 2018)</a></li><li>#PAPER
<a href=https://tcwang0509.github.io/pix2pixHD/ rel=noopener>High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs (Wang 2018)</a><ul><li>#CODE
<a href=https://github.com/NVIDIA/pix2pixHD rel=noopener>https://github.com/NVIDIA/pix2pixHD</a></li><li><a href=https://youtu.be/3AIpPlzM_qs rel=noopener>https://youtu.be/3AIpPlzM_qs</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1711.09020 rel=noopener>StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation (Choi 2018)</a><ul><li>#CODE
<a href=https://github.com/yunjey/stargan rel=noopener>https://github.com/yunjey/stargan</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1902.02729 rel=noopener>Reversible GANs for Memory-efficient Image-to-Image Translation (van der Ouderaa 2019)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1912.01865 rel=noopener>StarGAN v2: Diverse Image Synthesis for Multiple Domains (Choi 2020)</a><ul><li>#CODE
<a href=https://github.com/clovaai/stargan-v2 rel=noopener>https://github.com/clovaai/stargan-v2</a></li></ul></li></ul><h4 id=unpaired-unsupervised-translation>Unpaired (unsupervised) translation</h4><ul><li>#PAPER
<a href=https://arxiv.org/abs/1703.10593 rel=noopener>Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks, CycleGAN (Zhu, 2017)</a><ul><li>#TALK
<a href="https://www.youtube.com/watch?v=Fea4kZq0oFQ" rel=noopener>https://www.youtube.com/watch?v=Fea4kZq0oFQ</a></li><li>#CODE
<a href=https://github.com/clvrai/CycleGAN-Tensorflow rel=noopener>https://github.com/clvrai/CycleGAN-Tensorflow</a></li><li><a href=https://junyanz.github.io/CycleGAN/ rel=noopener>https://junyanz.github.io/CycleGAN/</a></li><li>CycleGAN is an approach to training deep convolutional networks for Image-to-Image translation tasks. Unlike other GANs models for image translation tasks, CycleGAN learns a mapping between one image domain and another using an unsupervised approach. This is done by training Generator Networks to learn a mapping from domain X into an image that looks like it came from domain Y (and vice-versa)</li><li>for the generator, residual functions (residual block) are used</li><li><a href=https://medium.com/deep-math-machine-learning-ai/ch-14-2-pix2pix-gan-and-cycle-gan-55cd84318fb8 rel=noopener>https://medium.com/deep-math-machine-learning-ai/ch-14-2-pix2pix-gan-and-cycle-gan-55cd84318fb8</a></li><li><a href=https://towardsdatascience.com/image-to-image-translation-using-cyclegan-model-d58cfff04755 rel=noopener>https://towardsdatascience.com/image-to-image-translation-using-cyclegan-model-d58cfff04755</a></li><li><a href=https://www.tensorflow.org/tutorials/generative/cyclegan rel=noopener>https://www.tensorflow.org/tutorials/generative/cyclegan</a></li><li><a href=https://machinelearningmastery.com/cyclegan-tutorial-with-keras/ rel=noopener>https://machinelearningmastery.com/cyclegan-tutorial-with-keras/</a></li><li><a href=https://yanjia.li/gender-swap-and-cyclegan-in-tensorflow-2-0/ rel=noopener>https://yanjia.li/gender-swap-and-cyclegan-in-tensorflow-2-0/</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1703.05192 rel=noopener>Learning to Discover Cross-Domain Relations with Generative Adversarial Networks (Kim 2017)</a><ul><li>#CODE
<a href=https://github.com/SKTBrain/DiscoGAN rel=noopener>https://github.com/SKTBrain/DiscoGAN</a></li><li>#CODE
<a href=https://github.com/clvrai/DiscoGAN-Tensorflow rel=noopener>https://github.com/clvrai/DiscoGAN-Tensorflow</a></li><li>This paper introduced an awesome framework for finding one-to-one mapping between two domains in an unsupervised way. The high-level idea is the joint training of two GAN model G1 and G2 in parallel (one for A->B and the other one for B->A)</li><li>Besides the adversarial loss, there is also reconstruction loss to ensure the consistency. Specifically, we restrict that G2(G1(A)) = A and G1(G2(B)) = B</li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1703.00848 rel=noopener>Unsupervised Image-to-Image Translation Networks (Liu 2018)</a><ul><li>#CODE
<a href=https://github.com/mingyuliutw/UNIT/ rel=noopener>https://github.com/mingyuliutw/UNIT/</a></li><li><a href="https://www.youtube.com/watch?v=dqxqbvyOnMY&feature=youtu.be" rel=noopener>https://www.youtube.com/watch?v=dqxqbvyOnMY&feature=youtu.be</a></li><li><a href=https://medium.com/@theehiproject/unet-unit-for-fast-unsupervised-image2image-translation-using-fastai-e366408eddb4 rel=noopener>https://medium.com/@theehiproject/unet-unit-for-fast-unsupervised-image2image-translation-using-fastai-e366408eddb4</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1804.04732 rel=noopener>MUNIT: Multimodal UNsupervised Image-to-image Translation (Huang 2018)</a><ul><li>#CODE
<a href=https://github.com/NVlabs/MUNIT rel=noopener>https://github.com/NVlabs/MUNIT</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1908.06965 rel=noopener>Fixed-point GAN - Learning Fixed Points in Generative Adversarial Networks: From Image-to-Image Translation to Disease Detection and Localization (Siddiquee 2019)</a> ^fixedpointGAN<ul><li>#CODE
<a href=https://github.com/mahfuzmohammad/Fixed-Point-GAN rel=noopener>https://github.com/mahfuzmohammad/Fixed-Point-GAN</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2007.15651 rel=noopener>Contrastive Learning for Unpaired Image-to-Image Translation (Park 2020)</a><ul><li><a href=https://taesung.me/ContrastiveUnpairedTranslation/ rel=noopener>https://taesung.me/ContrastiveUnpairedTranslation/</a></li><li>#CODE
<a href=https://github.com/taesungp/contrastive-unpaired-translation rel=noopener>https://github.com/taesungp/contrastive-unpaired-translation</a></li><li>#TALK
<a href="https://www.youtube.com/watch?v=jSGOzjmN8q0" rel=noopener>https://www.youtube.com/watch?v=jSGOzjmN8q0</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2006.06500 rel=noopener>Rethinking the Truly Unsupervised Image-to-Image Translation, TUNIT (Baek 2020)</a><ul><li>#CODE
<a href=https://github.com/clovaai/tunit rel=noopener>https://github.com/clovaai/tunit</a></li><li><a href="https://www.youtube.com/watch?v=sEG8hD64c_Q" rel=noopener>Paper explained</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1904.06913v4 rel=noopener>Implicit Pairs for Boosting Unpaired Image-to-Image Translation (Ginger 2020)</a></li><li>#PAPER
<a href=https://arxiv.org/pdf/2004.04634 rel=noopener>TuiGAN: Learning Versatile Image-to-Image Translation with Two Unpaired Images (Lin 2020)</a><ul><li>#CODE
<a href=https://github.com/linjx-ustc1106/TuiGAN-PyTorch rel=noopener>https://github.com/linjx-ustc1106/TuiGAN-PyTorch</a></li></ul></li></ul><h3 id=flow-based>Flow-based</h3><ul><li><p>#CODE
<a href=https://github.com/yenchenlin/pix2pix-flow rel=noopener>Image-to-image translation with flow-based generative model</a></p></li><li><p>#PAPER
<a href=https://papers.nips.cc/paper/2019/file/ffedf5be3a86e2ee281d54cdc97bc1cf-Paper.pdf rel=noopener>Flow-based Image-to-Image Translation with Feature Disentanglement (Kondo 2019)</a></p></li><li><p>#PAPER
<a href=https://arxiv.org/abs/1905.12892 rel=noopener>AlignFlow: Cycle Consistent Learning from Multiple Domains via Normalizing Flows (Grover, 2019)</a></p><ul><li>A generative modeling framework that models each domain via a normalizing flow</li><li>The use of normalizing flows allows for<ul><li>flexibility in specifying learning objectives via adversarial training, maximum likelihood estimation, or a hybrid of the two methods</li><li>learning and exact inference of a shared representation in the latent space of the generative model.</li></ul></li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/abs/2012.01777v1 rel=noopener>Flow-based Deformation Guidance for Unpaired Multi-Contrast MRI Image-to-Image Translation (Duc Bui 2020)</a></p><ul><li>Normalizing flows for unpaired image-to-image translation</li><li>Utilized the temporal information between consecutive slices to provide more constraints to the optimization for transforming one domain to another in un-paired volumetric medical image</li></ul></li></ul></article><hr><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/digitalgarden/AI/Computer-Vision/Deep-CV>Deep CV</a></li><li><a href=/digitalgarden/AI/Computer-Vision/Super-resolution>Super-resolution</a></li><li><a href=/digitalgarden/AI/Deep-learning/CNNs>Convolutional Neural Networks (CNNs)</a></li><li><a href=/digitalgarden/AI/Deep-learning/GANs>Generative Adversarial Networks (GANs)</a></li><li><a href=/digitalgarden/AI/Deep-learning/Generative-modelling>Generative modeling</a></li><li><a href=/digitalgarden/AI/Deep-learning/Normalizing-flows>Normalizing flows</a></li></ul></div></div><div id=contact_buttons><footer><p>Made by Carlos Alberto Gomez Gonzalez using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, ¬© 2022</p><ul><li><a href=https://carlos-gg.github.io/digitalgarden/>Home</a></li><a href=https://carlos-gg.github.io>Carlos'Homepage</a></ul></footer></div><script src=https://carlos-gg.github.io/digitalgarden/js/popover.e57188d2e4c06b0654e020b3a734bb62.min.js></script>
<script>initPopover("https://carlos-gg.github.io/digitalgarden")</script></div></body></html>