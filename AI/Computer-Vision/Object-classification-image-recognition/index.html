<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="See:
 [[AI/Deep learning/CNNs]] [[AI/Computer Vision/Object detection]] [[AI/Computer Vision/Semantic segmentation]] [[AI/Deep learning/Residual and dense neural networks]]   Resources   https://cv-tricks."><title>Object classification, image recognition</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://carlos-gg.github.io/digitalgarden//icon.png><link href=https://carlos-gg.github.io/digitalgarden/styles.b3e1e36b0403ac565c9392b3e23ef3b6.min.css rel=stylesheet><link href=https://carlos-gg.github.io/digitalgarden/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://carlos-gg.github.io/digitalgarden/js/darkmode.18b7c6dfe67ae3bf2317338cf4189144.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/util.9825137f5e7825e8553c68ce39ac9e44.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/popover.abe6a51cc7138c5dff00f151dd627ad1.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/code-title.b35124ad8db0ba37162b886afb711cbc.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/clipboard.c20857734e53a3fb733b7443879efa61.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const BASE_URL="https://carlos-gg.github.io/digitalgarden/",fetchData=Promise.all([fetch("https://carlos-gg.github.io/digitalgarden/indices/linkIndex.36332fb20fc41f515c0a269de715e752.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://carlos-gg.github.io/digitalgarden/indices/contentIndex.b1abe3840955a8d6f19240c289f5ef43.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://carlos-gg.github.io/digitalgarden",!0,!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!0;drawGraph("https://carlos-gg.github.io/digitalgarden",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/carlos-gg.github.io\/digitalgarden\/js\/router.9d4974281069e9ebb189f642ae1e3ca2.min.js"
    attachSPARouting(init, render)
  </script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-S103D50NQ0"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S103D50NQ0",{anonymize_ip:!1})}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://carlos-gg.github.io/digitalgarden/js/full-text-search.24827f874defbbc6d529926cbfcfb493.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://carlos-gg.github.io/digitalgarden/>CarlosGG's Knowledge Garden ðŸª´</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Object classification, image recognition</h1><p class=meta>Last updated
Sep 5, 2022
<a href=https://github.com/jackyzha0/quartz/tree/hugo/content/AI/Computer%20Vision/Object%20classification,%20image%20recognition.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details open><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#resources>Resources</a></li><li><a href=#references>References</a></li></ol></nav></details></aside><blockquote><p>See:</p><ul><li><a href=/digitalgarden/AI/Deep-learning/CNNs rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/CNNs>AI/Deep learning/CNNs</a></li><li><a href=/digitalgarden/AI/Computer-Vision/Object-detection rel=noopener class=internal-link data-src=/digitalgarden/AI/Computer-Vision/Object-detection>AI/Computer Vision/Object detection</a></li><li><a href=/digitalgarden/AI/Computer-Vision/Semantic-segmentation rel=noopener class=internal-link data-src=/digitalgarden/AI/Computer-Vision/Semantic-segmentation>AI/Computer Vision/Semantic segmentation</a></li><li><a href=/digitalgarden/AI/Deep-learning/Residual-and-dense-neural-networks rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Residual-and-dense-neural-networks>AI/Deep learning/Residual and dense neural networks</a></li></ul></blockquote><a href=#resources><h2 id=resources><span class=hanchor arialabel=Anchor># </span>Resources</h2></a><ul><li><a href=https://cv-tricks.com/cnn/understand-resnet-alexnet-vgg-inception/ rel=noopener>https://cv-tricks.com/cnn/understand-resnet-alexnet-vgg-inception/</a></li><li><a href=https://blog.paralleldots.com/data-science/must-read-path-breaking-papers-about-image-classification/ rel=noopener>https://blog.paralleldots.com/data-science/must-read-path-breaking-papers-about-image-classification/</a></li></ul><a href=#references><h2 id=references><span class=hanchor arialabel=Anchor># </span>References</h2></a><ul><li>#PAPER
<a href=https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks rel=noopener>AlexNet: ImageNet Classification with Deep Convolutional Neural Networks (2012)</a><ul><li>This architecture was one of the first deep networks to push ImageNet Classification accuracy by a significant stride in comparison to traditional methodologies. It is composed of 5 convolutional layers followed by 3 fully connected layers.</li><li>AlexNet, proposed by Alex Krizhevsky, uses ReLu(Rectified Linear Unit) for the non-linear part, instead of a Tanh or Sigmoid function which was the earlier standard for traditional neural networks. Another problem that this architecture solved was reducing the over-fitting by using a Dropout layer after every FC layer.</li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1311.2901 rel=noopener>Visualizing and Understanding Convolutional Networks (Zeiler and Fergus 2013)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1409.1556 rel=noopener>Very Deep Convolutional Networks for Large-Scale Image Recognition, VGG16 (Symonian 2014)</a><ul><li>This architecture is from VGG group, Oxford. It makes the improvement over AlexNet by replacing large kernel-sized filters (11 and 5 in the first and second convolutional layer, respectively) with multiple 3X3 kernel-sized filters one after another.</li></ul></li><li>#PAPER
<a href=https://ai.google/research/pubs/pub43022 rel=noopener>Going Deeper with Convolutions (Szegedy 2015)</a><ul><li>GoogLeNet (Inception-V1, 2015)</li><li><a href=http://nicolovaligi.com/history-inception-deep-learning-architecture.html rel=noopener>http://nicolovaligi.com/history-inception-deep-learning-architecture.html</a></li><li>GoogLeNet devised a module called inception module that approximates a sparse CNN with a normal dense construction(shown in the figure). Since only a small number of neurons are effective as mentioned earlier, the width/number of the convolutional filters of a particular kernel size is kept small. Also, it uses convolutions of different sizes to capture details at varied scales(5X5, 3X3, 1X1). Another salient point about the module is that it has a so-called bottleneck layer(1X1 convolutions in the figure). It helps in the massive reduction of the computation requirement. Another change that GoogLeNet made, was to replace the fully-connected layers at the end with a simple global average pooling which averages out the channel values across the 2D feature map, after the last convolutional layer.</li></ul></li><li>#PAPER See Resnet in
<a href=/digitalgarden/AI/Deep-learning/Residual-and-dense-neural-networks/ rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Residual-and-dense-neural-networks/>Residual and dense neural networks</a></li><li>#PAPER See Resnext in
<a href=/digitalgarden/AI/Deep-learning/Residual-and-dense-neural-networks/ rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Residual-and-dense-neural-networks/>Residual and dense neural networks</a></li><li>#PAPER See Densenet in
<a href=/digitalgarden/AI/Deep-learning/Residual-and-dense-neural-networks/ rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Residual-and-dense-neural-networks/>Residual and dense neural networks</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1602.07360 rel=noopener>SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size (Iandola 2016)</a><ul><li><a href="https://www.youtube.com/watch?v=ge_RT5wvHvY" rel=noopener>Paper explained</a></li><li><a href=https://towardsdatascience.com/squeeze-and-excitation-networks-9ef5e71eacd7 rel=noopener>https://towardsdatascience.com/squeeze-and-excitation-networks-9ef5e71eacd7</a></li></ul></li><li>#PAPER See SENets in
<a href=/digitalgarden/AI/Deep-learning/CNNs/ rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/CNNs/>CNNs</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1611.05431 rel=noopener>Aggregated Residual Transformations for Deep Neural Networks (Xie 2017)</a><ul><li>#CODE
<a href=https://github.com/taki0112/SENet-Tensorflow rel=noopener>https://github.com/taki0112/SENet-Tensorflow</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1904.11491 rel=noopener>Local Relation Networks for Image Recognition (Hu 2019)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/2003.13678v1 rel=noopener>Designing Network Design Spaces (Radosavovic 2020)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/2102.06171 rel=noopener>NFNets. High-Performance Large-Scale Image Recognition Without Normalization (Brock 2021)</a><ul><li>#CODE
<a href=https://github.com/deepmind/deepmind-research/tree/master/nfnets rel=noopener>https://github.com/deepmind/deepmind-research/tree/master/nfnets</a></li><li><a href=https://towardsdatascience.com/deepmind-releases-a-new-state-of-the-art-image-classification-model-nfnets-75c0b3f37312 rel=noopener>https://towardsdatascience.com/deepmind-releases-a-new-state-of-the-art-image-classification-model-nfnets-75c0b3f37312</a></li></ul></li><li>#PAPER
<a href="https://openreview.net/forum?id=TVHS5Y4dNvM" rel=noopener>Patches Are All You Need? (2021)</a><ul><li>#CODE
<a href=https://github.com/tmp-iclr/convmixer rel=noopener>https://github.com/tmp-iclr/convmixer</a></li><li><a href=https://syncedreview.com/2021/10/12/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-121/ rel=noopener>https://syncedreview.com/2021/10/12/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-121/</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2106.04803 rel=noopener>CoAtNet: Marrying Convolution and Attention for All Data Sizes (Dai 2021)</a><ul><li><a href=https://ai.googleblog.com/2021/09/toward-fast-and-accurate-neural.html rel=noopener>https://ai.googleblog.com/2021/09/toward-fast-and-accurate-neural.html</a></li><li><a href=https://analyticsindiamag.com/a-guide-to-coatnet-the-combination-of-convolution-and-attention-networks/ rel=noopener>https://analyticsindiamag.com/a-guide-to-coatnet-the-combination-of-convolution-and-attention-networks/</a></li></ul></li></ul></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/digitalgarden/AI/Computer-Vision/Deep-CV/ data-ctx="AI/Computer Vision/Object classification, image recognition" data-src=/AI/Computer-Vision/Deep-CV class=internal-link>Deep CV</a></li><li><a href=/digitalgarden/AI/Deep-learning/CNNs/ data-ctx="AI/Computer Vision/Object classification, image recognition" data-src=/AI/Deep-learning/CNNs class=internal-link>Convolutional Neural Networks (CNNs)</a></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://carlos-gg.github.io/digitalgarden/js/graph.abd4bc2af3869a96524d7d23b76152c7.js></script></div></div><div id=contact_buttons><footer><p>Made by Carlos Alberto Gomez Gonzalez using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2022</p><ul><li><a href=https://carlos-gg.github.io/digitalgarden/>Home</a></li><li><a href=https://carlos-gg.github.io>Carlos'Homepage</a></li></ul></footer></div></div></body></html>