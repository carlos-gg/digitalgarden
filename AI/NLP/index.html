<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="A Computer Science field connected to Artificial Intelligence and Computational Linguistics which focuses on interactions between computers and human language and a machineâ€™s ability to understand, or mimic the understanding of human language"><title>Natural Language Processing (NLP)</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://carlos-gg.github.io/digitalgarden//icon.png><link href=https://carlos-gg.github.io/digitalgarden/styles.b3e1e36b0403ac565c9392b3e23ef3b6.min.css rel=stylesheet><link href=https://carlos-gg.github.io/digitalgarden/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://carlos-gg.github.io/digitalgarden/js/darkmode.18b7c6dfe67ae3bf2317338cf4189144.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/util.9825137f5e7825e8553c68ce39ac9e44.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/popover.abe6a51cc7138c5dff00f151dd627ad1.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/code-title.b35124ad8db0ba37162b886afb711cbc.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/clipboard.c20857734e53a3fb733b7443879efa61.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const BASE_URL="https://carlos-gg.github.io/digitalgarden/",fetchData=Promise.all([fetch("https://carlos-gg.github.io/digitalgarden/indices/linkIndex.2b86fa123a302b63605ac6bc5108dffd.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://carlos-gg.github.io/digitalgarden/indices/contentIndex.7223f8a0016699a7151bb2cf96a531bd.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://carlos-gg.github.io/digitalgarden",!0,!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!0;drawGraph("https://carlos-gg.github.io/digitalgarden",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/carlos-gg.github.io\/digitalgarden\/js\/router.9d4974281069e9ebb189f642ae1e3ca2.min.js"
    attachSPARouting(init, render)
  </script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-S103D50NQ0"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S103D50NQ0",{anonymize_ip:!1})}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://carlos-gg.github.io/digitalgarden/js/full-text-search.24827f874defbbc6d529926cbfcfb493.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://carlos-gg.github.io/digitalgarden/>CarlosGG's Knowledge Garden ðŸª´</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Natural Language Processing (NLP)</h1><p class=meta>Last updated
Sep 3, 2022
<a href=https://github.com/jackyzha0/quartz/tree/hugo/content/AI/NLP.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details open><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#resources>Resources</a><ol><li><ol><li><a href=#deep-learning-based>Deep learning-based</a></li></ol></li></ol></li><li><a href=#books>Books</a></li><li><a href=#courses>Courses</a></li><li><a href=#talks>Talks</a></li><li><a href=#models>Models</a></li><li><a href=#code>Code</a><ol><li><a href=#web-scrapping-and-cleaning>Web scrapping and cleaning</a></li></ol></li><li><a href=#subtopics>Subtopics</a><ol><li><a href=#text-preparation>Text preparation</a></li><li><a href=#feature-engineering>Feature engineering</a><ol><li><a href=#bag-of-words>Bag of words</a></li><li><a href=#tfidf>tfâ€“idf</a></li><li><a href=#word-embeddings>Word embeddings</a></li></ol></li><li><a href=#semantics>Semantics</a><ol><li><a href=#distributional-semantics>Distributional semantics</a></li><li><a href=#topic-modelling>Topic Modelling</a></li><li><a href=#neural-semantic-parsing>Neural semantic parsing</a></li><li><a href=#explicit-semantic-analysis>Explicit semantic analysis</a></li></ol></li><li><a href=#sentiment-analysis>Sentiment analysis</a></li><li><a href=#speech-recognition>Speech recognition</a><ol><li><a href=#hmm>HMM</a></li></ol></li><li><a href=#cnn-based>CNN-based</a></li><li><a href=#rnn-based>RNN-based</a></li><li><a href=#sequence-to-sequence-seq2seq>Sequence-to-sequence (seq2seq)</a><ol><li><a href=#google-neural-machine-translation-gnmt>Google Neural Machine Translation (GNMT)</a></li><li><a href=#transformer-based>Transformer-based</a></li></ol></li></ol></li></ol></nav></details></aside><blockquote><p>A Computer Science field connected to Artificial Intelligence and Computational Linguistics which focuses on interactions between computers and human language and a machineâ€™s ability to understand, or mimic the understanding of human language</p></blockquote><a href=#resources><h2 id=resources><span class=hanchor arialabel=Anchor># </span>Resources</h2></a><ul><li><a href=https://en.wikipedia.org/wiki/Natural_language_processing rel=noopener>https://en.wikipedia.org/wiki/Natural_language_processing</a></li><li>NLP is a branch of data science that consists of systematic processes for analyzing, understanding, and deriving information from the text data in a smart and efficient manner. By utilizing NLP and its components, one can organize the massive chunks of text data, perform numerous automated tasks and solve a wide range of problems such as â€“ automatic summarization, machine translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation etc.</li><li><a href=https://github.com/keon/awesome-nlp rel=noopener>https://github.com/keon/awesome-nlp</a></li><li><a href=https://github.com/omarsar/nlp_highlights rel=noopener>The most important NLP highlights of 2018</a></li><li><a href=https://github.com/jmportilla/Udemy---Machine-Learning/blob/master/NLP-%28Natural-Language-Processing%29.ipynb rel=noopener>NLP - Udemy ML</a></li><li><a href=https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/ rel=noopener>https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/</a></li><li><a href=https://www.datascience.com/blog/introduction-to-natural-language-processing-lexical-units-learn-data-science-tutorials rel=noopener>https://www.datascience.com/blog/introduction-to-natural-language-processing-lexical-units-learn-data-science-tutorials</a></li><li><a href=https://github.com/BotCube/awesome-bots rel=noopener>https://github.com/BotCube/awesome-bots</a></li><li><a href=http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html rel=noopener>http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html</a></li></ul><a href=#deep-learning-based><h4 id=deep-learning-based><span class=hanchor arialabel=Anchor># </span>Deep learning-based</h4></a><ul><li><a href=https://nlpoverview.com/ rel=noopener>Modern Deep Learning Techniques Applied to Natural Language Processing</a></li><li><a href=https://github.com/brianspiering/awesome-dl4nlp rel=noopener>https://github.com/brianspiering/awesome-dl4nlp</a></li><li><a href=https://veredshwartz.blogspot.com/2018/08/deep-learning-in-nlp.html# rel=noopener>Deep Learning in NLP</a></li><li><a href=https://softwaremill.com/deep-learning-for-nlp/ rel=noopener>https://softwaremill.com/deep-learning-for-nlp/</a></li><li><a href=http://blog.aylien.com/modeling-documents-generative-adversarial-networks/ rel=noopener>http://blog.aylien.com/modeling-documents-generative-adversarial-networks/</a></li></ul><a href=#books><h2 id=books><span class=hanchor arialabel=Anchor># </span>Books</h2></a><ul><li>#BOOK
<a href=http://www.datascienceassn.org/sites/default/files/Natural-Language-Processing-with-Python.pdf rel=noopener>Natural Language Processing with Python (Bird, 2013 OREILLY)</a></li><li>#BOOK
<a href=https://www.packtpub.com/big-data-and-business-intelligence/mastering-natural-language-processing-python rel=noopener>Mastering NLP with Python (Chopra, 2016 PACKT)</a><ul><li><a href=https://github.com/PacktPublishing/Mastering-Natural-Language-Processing-with-Python rel=noopener>https://github.com/PacktPublishing/Mastering-Natural-Language-Processing-with-Python</a></li></ul></li><li>#BOOK
<a href=https://nlp.stanford.edu/IR-book/ rel=noopener>An Introduction to Information Retrieval (Manning 2009, CAMBRIDGE)</a></li><li>#BOOK
<a href=https://tidytextmining.com/ rel=noopener>Text mining with R (Silge, 2020 OREILLY)</a></li></ul><a href=#courses><h2 id=courses><span class=hanchor arialabel=Anchor># </span>Courses</h2></a><ul><li>#COURSE
<a href=http://phontron.com/class/nn4nlp2017/# rel=noopener>Neural networks for NLP</a> (Carnegie Mellon)<ul><li><a href=https://github.com/neubig/nn4nlp-code rel=noopener>https://github.com/neubig/nn4nlp-code</a></li><li><a href=https://www.youtube.com/user/neubig/videos rel=noopener>https://www.youtube.com/user/neubig/videos</a></li></ul></li><li>#COURSE
<a href=https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1162/ rel=noopener>NLP (Standford 15)</a></li><li>#COURSE
<a href=http://web.stanford.edu/class/cs224n/ rel=noopener>NLP with Deep Learning (Standford 16,17)</a></li><li>#COURSE
<a href=http://web.stanford.edu/class/cs224u/ rel=noopener>Natural Language Understanding (Standford 16)</a></li><li>#COURSE
<a href=https://www.cs.ox.ac.uk/teaching/courses/2016-2017/dl/ rel=noopener>Deep Learning for NLP (Oxford/Deepmind 17)</a><ul><li><a href=https://github.com/oxford-cs-deepnlp-2017 rel=noopener>https://github.com/oxford-cs-deepnlp-2017</a></li><li><a href=https://github.com/oxford-cs-deepnlp-2017/lectures rel=noopener>https://github.com/oxford-cs-deepnlp-2017/lectures</a></li><li><a href="https://www.youtube.com/playlist?list=PL613dYIGMXoZBtZhbyiBqb0QtgK6oJbpm" rel=noopener>https://www.youtube.com/playlist?list=PL613dYIGMXoZBtZhbyiBqb0QtgK6oJbpm</a></li></ul></li><li>#COURSE
<a href=https://github.com/yandexdataschool/nlp_course rel=noopener>YSDA Natural Language Processing course (Yandex)</a></li></ul><a href=#talks><h2 id=talks><span class=hanchor arialabel=Anchor># </span>Talks</h2></a><ul><li>#TALK
<a href="https://www.youtube.com/watch?v=8S3qHHUKqYk" rel=noopener>Introduction to Natural Language Processing - Cambridge Data Science Bootcamp</a></li><li>#TALK
<a href="https://www.youtube.com/watch?v=HVdPWoZ_swY" rel=noopener>Rob Romijnders | Using deep learning in natural language processing (PyData)</a></li><li>#TALK
<a href="https://www.youtube.com/watch?v=7Z3ojkuul9U" rel=noopener>Jeff Abrahamson - WTF am I doing? An introduction to NLP and ANN&rsquo;s</a></li><li>#TALK
<a href="https://www.youtube.com/watch?v=AsW0QzbYVow" rel=noopener>Natural Language Processing with PySpark</a></li><li>#TALK
<a href="https://www.youtube.com/watch?v=qYpdW9cyEqY" rel=noopener>Feeding Word2vec with tens of billions of items, what could possibly go wrong? (Simon DollÃ©)</a></li><li>#TALK
<a href="https://www.youtube.com/watch?v=tdLmf8t4oqM" rel=noopener>Deep Learning for Natural Language Processing (2015)</a></li></ul><a href=#models><h2 id=models><span class=hanchor arialabel=Anchor># </span>Models</h2></a><ul><li><a href=https://bigscience.huggingface.co/blog/bloom rel=noopener>Introducing The Worldâ€™s Largest Open Multilingual Language Model: BLOOM</a><ul><li><a href=https://www.nature.com/articles/d41586-022-01705-z rel=noopener>https://www.nature.com/articles/d41586-022-01705-z</a></li></ul></li><li><a href=https://github.com/yandex/YaLM-100B rel=noopener>YaLM-100B</a></li></ul><a href=#code><h2 id=code><span class=hanchor arialabel=Anchor># </span>Code</h2></a><ul><li>#CODE
<a href=https://github.com/PaddlePaddle/PaddleSpeech rel=noopener>PaddleSpeech</a><ul><li>PaddleSpeech is an open-source toolkit on PaddlePaddle platform for a variety of critical tasks in speech and audio, with the state-of-art and influential models</li><li>#PAPER
<a href=https://arxiv.org/pdf/2205.12007v1 rel=noopener>PaddleSpeech: An Easy-to-Use All-in-One Speech Toolkit (Zhang 2022)</a></li></ul></li><li>#CODE
<a href=https://github.com/recognai/rubrix rel=noopener>Rubrix</a><ul><li>Rubrix, open-source framework for data-centric NLP. Data annotation and monitoring for enterprise NLP</li></ul></li><li>#CODE
<a href=https://github.com/UKPLab/beir rel=noopener>Beir - Heterogeneous Benchmark for Information Retrieval</a><ul><li>#PAPER
<a href=https://arxiv.org/abs/2104.08663 rel=noopener>BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models (Thakur 2021)</a></li></ul></li><li>#CODE
<a href=https://fasttext.cc/ rel=noopener>FastText</a><ul><li>Library for efficient text classification and representation learning</li><li><a href="https://colab.research.google.com/notebook#fileId=1MikDdEIDzFVpH7V6ZSTho8bwOALEmUoa" rel=noopener>Install FastText on Google colaboratory</a></li></ul></li><li>#CODE
<a href=https://github.com/facebookresearch/fairseq rel=noopener>Fairseq</a><ul><li>Fairseq(-py) is a sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling and other text generation tasks</li></ul></li><li>#CODE
<a href=https://github.com/OpenNMT/OpenNMT-tf rel=noopener>OpenNMT-tf - OpenNMT-tf is a general purpose sequence learning toolkit using TensorFlow 2</a></li><li>#CODE
<a href=https://opennlp.apache.org/ rel=noopener>OpenNLP - The Apache OpenNLP library is a machine learning based toolkit for the processing of natural language text</a></li><li>#CODE
<a href=https://github.com/minimaxir/textgenrnn rel=noopener>Textgen RNN</a></li><li>#CODE
<a href=https://stanfordnlp.github.io/CoreNLP/ rel=noopener>Standford CoreNLP</a></li><li>#CODE
<a href=http://www.nltk.org/ rel=noopener>NLTK - NLTK is a leading platform for building Python programs to work with human language data</a></li><li>#CODE
<a href=https://textblob.readthedocs.io/en/dev/ rel=noopener>Textblob</a><ul><li>TextBlobis a Python library for processing textual data</li><li><a href=http://ahmedbesbes.com/how-to-mine-newsfeed-data-and-extract-interactive-insights-in-python.html rel=noopener>http://ahmedbesbes.com/how-to-mine-newsfeed-data-and-extract-interactive-insights-in-python.html</a></li><li><a href=http://nicschrading.com/project/Intro-to-NLP-in-Python/ rel=noopener>http://nicschrading.com/project/Intro-to-NLP-in-Python/</a></li><li><a href=https://github.com/NSchrading/intro-python-nlp/blob/master/Intro_Python_NLP.ipynb rel=noopener>https://github.com/NSchrading/intro-python-nlp/blob/master/Intro_Python_NLP.ipynb</a></li></ul></li><li>#CODE
<a href=https://spacy.io/ rel=noopener>Spacy (Industrial-strength NLP)</a><ul><li>#TALK
<a href="https://www.youtube.com/watch?v=gJJQs47aUQ0" rel=noopener>Matthew Honnibal - Designing spaCy: Industrial-strength NLP</a></li><li>#TALK
<a href="https://www.youtube.com/watch?v=6zm9NC9uRkk" rel=noopener>Patrick Harrison | Modern NLP in Python (SpaCy and gensim for recommendation-reviews analysis)</a></li><li><a href=https://spacy.io/docs/usage/tutorials rel=noopener>https://spacy.io/docs/usage/tutorials</a></li><li><a href=https://nicschrading.com/project/Intro-to-NLP-with-spaCy/ rel=noopener>https://nicschrading.com/project/Intro-to-NLP-with-spaCy/</a></li><li><a href=http://blog.thedataincubator.com/2016/04/nltk-vs-spacy-natural-language-processing-in-python/ rel=noopener>http://blog.thedataincubator.com/2016/04/nltk-vs-spacy-natural-language-processing-in-python/</a></li></ul></li><li>#CODE
<a href=http://www.parl.ai/ rel=noopener>ParlAI - A unified platform for sharing, training and evaluating dialogue models across many tasks</a></li><li>#CODE
<a href=https://github.com/johnsnowlabs/spark-nlp rel=noopener>Spark-NLP</a></li><li>#CODE
<a href=https://radimrehurek.com/gensim/ rel=noopener>Gensim - Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora</a></li></ul><a href=#web-scrapping-and-cleaning><h3 id=web-scrapping-and-cleaning><span class=hanchor arialabel=Anchor># </span>Web scrapping and cleaning</h3></a><ul><li>#CODE
<a href=http://docs.python-requests.org/en/master/user/quickstart/ rel=noopener>Requests (For fetching HTML/XML from web pages)</a></li><li>#CODE
<a href=https://www.crummy.com/software/BeautifulSoup/ rel=noopener>BeautifulSoup (web scraping data parsing)</a><ul><li>#TALK
<a href="https://www.youtube.com/watch?v=XQgXKtPSzUI" rel=noopener>Introduction To Web Scraping (with Python and Beautiful Soup)</a></li></ul></li><li>#CODE
<a href=http://lxml.de/ rel=noopener>LXML (web scraping data parsing)</a></li><li>#CODE
<a href=https://dryscrape.readthedocs.io/en/latest/ rel=noopener>Dryscape (web scraping with javascript)</a></li><li>#CODE
<a href=http://selenium-python.readthedocs.io/ rel=noopener>Selenium (web scraping with javascript)</a></li><li>#CODE
<a href=https://github.com/scrapy/scrapy rel=noopener>Scrapy (web scraping framework)</a><ul><li><a href=https://doc.scrapy.org/en/latest/intro/overview.html rel=noopener>https://doc.scrapy.org/en/latest/intro/overview.html</a></li><li>Scrapy is an application framework for crawling web sites and extracting structured data which can be used for a wide range of useful applications, like data mining, information processing or historical archival.</li><li><a href=https://medium.com/@kaismh/extracting-data-from-websites-using-scrapy-e1e1e357651a#.j9hrs2scn rel=noopener>https://medium.com/@kaismh/extracting-data-from-websites-using-scrapy-e1e1e357651a#.j9hrs2scn</a></li></ul></li><li>#CODE
<a href=http://ftfy.readthedocs.org/ rel=noopener>python-ftfy: fixes text for you</a></li><li>#CODE
<a href=http://arrow.readthedocs.io/en/latest/ rel=noopener>Arrow - working with dates and times</a></li><li>#CODE
<a href=https://github.com/sachinvettithanam/beautifier rel=noopener>Beautifier - clean and prettify URLs and email addresses</a></li></ul><a href=#subtopics><h2 id=subtopics><span class=hanchor arialabel=Anchor># </span>Subtopics</h2></a><a href=#text-preparation><h3 id=text-preparation><span class=hanchor arialabel=Anchor># </span>Text preparation</h3></a><ul><li><a href=http://nitin-panwar.github.io/Text-prepration-before-Sentiment-analysis/ rel=noopener>http://nitin-panwar.github.io/Text-prepration-before-Sentiment-analysis/</a><ul><li>Removing numbers</li><li>Removing Urls and Links</li><li>Removing stopwords</li><li>Stemming words</li><li>Suffix-dropping algorithms</li><li>Lemmatisation algorithms</li><li>n-gram analysis</li><li>Removing punctuation</li><li>Stripping whitespace</li><li>Checking for impure characters</li></ul></li><li><a href=http://thinknook.com/10-ways-to-improve-your-classification-algorithm-performance-2013-01-21/ rel=noopener>http://thinknook.com/10-ways-to-improve-your-classification-algorithm-performance-2013-01-21/</a></li></ul><a href=#feature-engineering><h3 id=feature-engineering><span class=hanchor arialabel=Anchor># </span>Feature engineering</h3></a><ul><li><a href=https://www.geeksforgeeks.org/feature-extraction-techniques-nlp/ rel=noopener>https://www.geeksforgeeks.org/feature-extraction-techniques-nlp/</a></li><li><a href=https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/ rel=noopener>https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/</a></li></ul><a href=#bag-of-words><h4 id=bag-of-words><span class=hanchor arialabel=Anchor># </span>Bag of words</h4></a><ul><li>A commonly used model in methods of Text Classification. As part of the BOW model, a piece of text (sentence or a document) is represented as a bag or multiset of words, disregarding grammar and even word order and the frequency or occurrence of each word is used as a feature for training a classifier.</li><li>BoW is different from Word2vec, which weâ€™ll cover next. The main difference is that Word2vec produces one vector per word, whereas BoW produces one number (a wordcount). Word2vec is great for digging into documents and identifying content and subsets of content. Its vectors represent each wordâ€™s context, the ngrams of which it is a part. BoW is good for classifying documents as a whole.</li></ul><a href=#tfidf><h4 id=tfidf><span class=hanchor arialabel=Anchor># </span>tfâ€“idf</h4></a><ul><li><a href=https://en.wikipedia.org/wiki/Tf%E2%80%93idf rel=noopener>tf-idf</a></li><li>Term Frequency-Inverse Document Frequency</li><li>tfâ€“idf, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.</li><li>The tf-idf value increases proportionally to the number of times a word appears in the document, but is often offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general.</li><li><a href=https://deeplearning4j.org/bagofwords-tf-idf rel=noopener>https://deeplearning4j.org/bagofwords-tf-idf</a></li><li><a href=http://www.tfidf.com/ rel=noopener>http://www.tfidf.com/</a></li><li><a href=http://scikit-learn.org/stable/auto_examples/text/document_clustering.html#sphx-glr-auto-examples-text-document-clustering-py rel=noopener>http://scikit-learn.org/stable/auto_examples/text/document_clustering.html#sphx-glr-auto-examples-text-document-clustering-py</a></li></ul><a href=#word-embeddings><h4 id=word-embeddings><span class=hanchor arialabel=Anchor># </span>Word embeddings</h4></a><ul><li>Word embedding is the modern way of representing words as vectors. The aim of word embedding is to redefine the high dimensional word features into low dimensional feature vectors by preserving the contextual similarity in the corpus. They are widely used in deep learning models such as Convolutional Neural Networks and Recurrent Neural Networks.</li><li>#PAPER
<a href=https://arxiv.org/abs/1310.4546 rel=noopener>Word2Vec: Distributed Representations of Words and Phrases and their Compositionality (Mikolov 2013)</a><ul><li><a href=https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf rel=noopener>https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf</a></li><li>Skip-gram model with negative sampling</li><li><a href=https://code.google.com/archive/p/word2vec/ rel=noopener>https://code.google.com/archive/p/word2vec/</a></li><li><a href="https://www.youtube.com/watch?v=yexR53My2O4" rel=noopener>Paper explained</a></li><li><a href=http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html rel=noopener>http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html</a></li><li><a href=http://www.deeplearningweekly.com/blog/demystifying-word2vec rel=noopener>http://www.deeplearningweekly.com/blog/demystifying-word2vec</a></li><li><a href=http://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/ rel=noopener>http://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/</a></li></ul></li><li>#PAPER
<a href=https://dl.acm.org/doi/10.5555/3044805.3045025 rel=noopener>Distributed representations of sentences and documents (Le 2014)</a><ul><li><a href=https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e rel=noopener>https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e</a></li></ul></li><li>#PAPER
<a href=https://aclanthology.org/D14-1162/ rel=noopener>GloVe: Global Vectors for Word Representation (Pennington 2014)</a><ul><li><a href=http://nlp.stanford.edu/projects/glove/ rel=noopener>Glove</a></li><li>GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.</li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1511.06388 rel=noopener>sense2vec - A Fast and Accurate Method for Word Sense Disambiguation In Neural Word Embeddings (Trask 2015)</a><ul><li><a href=https://explosion.ai/blog/sense2vec-with-spacy rel=noopener>Sense2vec</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1607.04606 rel=noopener>Enriching Word Vectors with Subword Information (Bojanowski 2017)</a></li></ul><a href=#semantics><h3 id=semantics><span class=hanchor arialabel=Anchor># </span>Semantics</h3></a><a href=#distributional-semantics><h4 id=distributional-semantics><span class=hanchor arialabel=Anchor># </span>Distributional semantics</h4></a><ul><li>General recipe:<ul><li>form a word-context matrix of counts (data)</li><li>perform dimensionality reduction (<a href=/digitalgarden/AI/Math-and-Statistics/SVD rel=noopener class=internal-link data-src=/digitalgarden/AI/Math-and-Statistics/SVD>AI/Math and Statistics/SVD</a>) for generalization</li></ul></li><li>For LSA the context is the document where the word appears.</li><li>For word2vec the context is just a work, nearby words (in some window) in a document.</li><li><a href=https://en.wikipedia.org/wiki/Latent_semantic_analysis rel=noopener>Latent semantic analysis</a><ul><li>The process of analyzing relationships between a set of documents and the terms they contain. Accomplished by producing a set of concepts related to the documents and terms. LSA assumes that words that are close in meaning will occur in similar pieces of text.</li><li>Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms.</li><li>LSA assumes that words that are close in meaning will occur in similar pieces of text. A matrix containing word counts per paragraph (rows represent unique words and columns represent each paragraph) is constructed from a large piece of text and SVD is used to reduce the number of rows while preserving the similarity structure among columns. Words are then compared by taking the cosine of the angle between the two vectors (or the dot product between the normalizations of the two vectors) formed by any two rows. Values close to 1 represent very similar words while values close to 0 represent very dissimilar words.</li><li><a href=http://mccormickml.com/2016/03/25/lsa-for-text-classification-tutorial/ rel=noopener>http://mccormickml.com/2016/03/25/lsa-for-text-classification-tutorial/</a></li><li><a href=https://github.com/chrisjmccormick/LSA_Classification/blob/master/runClassification_LSA.py rel=noopener>https://github.com/chrisjmccormick/LSA_Classification/blob/master/runClassification_LSA.py</a></li><li><a href=http://stackoverflow.com/questions/30590881/python-lsa-with-sklearn rel=noopener>http://stackoverflow.com/questions/30590881/python-lsa-with-sklearn</a></li></ul></li></ul><a href=#topic-modelling><h4 id=topic-modelling><span class=hanchor arialabel=Anchor># </span>Topic Modelling</h4></a><ul><li><a href=https://en.wikipedia.org/wiki/Topic_model rel=noopener>https://en.wikipedia.org/wiki/Topic_model</a></li><li><a href=https://es.wikipedia.org/wiki/Latent_Dirichlet_Allocation rel=noopener>Latent Dirichlet Allocation</a><ul><li>A common topic modeling technique, LDA is based on the premise that each document or piece of text is a mixture of a small number of topics and that each word in a document is attributable to one of the topics.</li><li><a href=http://engineering.flipboard.com/2017/02/storyclustering rel=noopener>http://engineering.flipboard.com/2017/02/storyclustering</a></li><li><a href=http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html rel=noopener>http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html</a></li><li><a href=http://ahmedbesbes.com/how-to-mine-newsfeed-data-and-extract-interactive-insights-in-python.html rel=noopener>http://ahmedbesbes.com/how-to-mine-newsfeed-data-and-extract-interactive-insights-in-python.html</a></li></ul></li></ul><a href=#neural-semantic-parsing><h4 id=neural-semantic-parsing><span class=hanchor arialabel=Anchor># </span>Neural semantic parsing</h4></a><ul><li>#PAPER
<a href=https://www.aclweb.org/anthology/P18-5006/ rel=noopener>Neural Semantic Parsing (Jia & Liang 2016)</a></li></ul><a href=#explicit-semantic-analysis><h4 id=explicit-semantic-analysis><span class=hanchor arialabel=Anchor># </span>Explicit semantic analysis</h4></a><ul><li><a href=https://en.wikipedia.org/wiki/Explicit_semantic_analysis rel=noopener>https://en.wikipedia.org/wiki/Explicit_semantic_analysis</a></li><li>In NLP and information retrieval, explicit semantic analysis (ESA) is a vectorial representation of text (individual words or entire documents) that uses a document corpus as a knowledge base. Specifically, in ESA, a word is represented as a column vector in the tfâ€“idf matrix of the text corpus and a document (string of words) is represented as the centroid of the vectors representing its words. Typically, the text corpus is English Wikipedia, though other corpora including the Open Directory Project have been used.</li><li>Used in Information Retrieval, Document Classification and Semantic Relatedness calculation (i.e. how similar in meaning two words or pieces of text are to each other), ESA is the process of understanding the meaning of a piece text, as a combination of the concepts found in that text.</li><li>Corpus or Corpora. A usually large collection of documents that can be used to infer and validate linguistic rules, as well as to do statistical analysis and hypothesis testing.</li></ul><a href=#sentiment-analysis><h3 id=sentiment-analysis><span class=hanchor arialabel=Anchor># </span>Sentiment analysis</h3></a><ul><li><a href=https://en.wikipedia.org/wiki/Sentiment_analysis rel=noopener>https://en.wikipedia.org/wiki/Sentiment_analysis</a></li><li>The use of NLP techniques to extract subjective information from a piece of text. i.e. whether an author is being subjective or objective or even positive or negative</li><li><a href=http://varianceexplained.org/r/trump-tweets/ rel=noopener>http://varianceexplained.org/r/trump-tweets/</a></li><li><a href=http://blog.aylien.com/sentiment-analysis-of-2-2-million-tweets-from-super-bowl-51/ rel=noopener>http://blog.aylien.com/sentiment-analysis-of-2-2-million-tweets-from-super-bowl-51/</a></li></ul><a href=#speech-recognition><h3 id=speech-recognition><span class=hanchor arialabel=Anchor># </span>Speech recognition</h3></a><ul><li><a href=https://www.wikiwand.com/en/Speech_recognition rel=noopener>https://www.wikiwand.com/en/Speech_recognition</a></li></ul><a href=#hmm><h4 id=hmm><span class=hanchor arialabel=Anchor># </span>HMM</h4></a><ul><li><a href=https://en.wikipedia.org/wiki/Hidden_Markov_model rel=noopener>https://en.wikipedia.org/wiki/Hidden_Markov_model</a></li></ul><a href=#cnn-based><h3 id=cnn-based><span class=hanchor arialabel=Anchor># </span>CNN-based</h3></a><p>See <a href=/digitalgarden/AI/Deep-learning/CNNs rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/CNNs>AI/Deep learning/CNNs</a></p><ul><li><a href=https://cnvrg.io/cnn-sentence-classification/ rel=noopener>Convolutional Neural Network for Sentence Classification</a></li><li><a href=http://www.kdnuggets.com/2017/05/deep-learning-extract-knowledge-job-descriptions.html rel=noopener>http://www.kdnuggets.com/2017/05/deep-learning-extract-knowledge-job-descriptions.html</a></li><li><a href=https://offbit.github.io/how-to-read/ rel=noopener>How to read: Character level deep learning</a></li><li>#PAPER
<a href=https://distill.pub/2017/ctc/ rel=noopener>Connectionist Temporal Classification (Hannun 2017)</a></li></ul><a href=#rnn-based><h3 id=rnn-based><span class=hanchor arialabel=Anchor># </span>RNN-based</h3></a><p>See <a href=/digitalgarden/AI/Deep-learning/RNNs rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/RNNs>AI/Deep learning/RNNs</a></p><ul><li><a href=https://adeshpande3.github.io/adeshpande3.github.io/Deep-Learning-Research-Review-Week-3-Natural-Language-Processing rel=noopener>RNN for NLP</a></li><li><a href=http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html rel=noopener>http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1709.03714 rel=noopener>RRA: Recurrent Residual Attention for Sequence Learning (Wang 2017)</a></li></ul><a href=#sequence-to-sequence-seq2seq><h3 id=sequence-to-sequence-seq2seq><span class=hanchor arialabel=Anchor># </span>Sequence-to-sequence (seq2seq)</h3></a><ul><li>#PAPER
<a href=https://arxiv.org/abs/1409.3215 rel=noopener>Sequence to Sequence Learning with Neural Networks (Sustkever 2014)</a></li><li>Sequence-to-sequence models are deep learning models that have achieved a lot of success in tasks like machine translation, text summarization, and image captioning. Google Translate started using such a model in production in late 2016. These models are explained in the two pioneering papers (Sutskever et al., 2014, Cho et al., 2014).</li><li>A sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an imagesâ€¦etc) and outputs another sequence of items.</li><li>Under the hood, the model is composed of an encoder and a decoder. The encoder processes each item in the input sequence, it compiles the information it captures into a vector (called the context). After processing the entire input sequence, the encoder sends the context over to the decoder, which begins producing the output sequence item by item.</li><li>The context is a vector (an array of numbers, basically) in the case of machine translation. The encoder and decoder tend to both be recurrent neural networks.</li><li><a href=https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/ rel=noopener>https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/</a></li></ul><a href=#google-neural-machine-translation-gnmt><h4 id=google-neural-machine-translation-gnmt><span class=hanchor arialabel=Anchor># </span>Google Neural Machine Translation (GNMT)</h4></a><ul><li><a href=https://en.wikipedia.org/wiki/Google_Neural_Machine_Translation rel=noopener>https://en.wikipedia.org/wiki/Google_Neural_Machine_Translation</a></li><li>Google Neural Machine Translation (GNMT) is a neural machine translation (NMT) system developed by Google and introduced in November 2016, that uses an artificial neural network to increase fluency and accuracy in Google Translate.</li><li>#PAPER
<a href=https://arxiv.org/abs/1611.04558 rel=noopener>Zero-shot translation</a><ul><li>Google Neural Machine Translation (GNMT) is a neural machine translation (NMT) system developed by Google and introduced in November 2016, that uses an artificial neural network to increase fluency and accuracy in Google Translate.</li><li><a href=https://research.googleblog.com/2016/11/zero-shot-translation-with-googles.html rel=noopener>https://research.googleblog.com/2016/11/zero-shot-translation-with-googles.html</a></li></ul></li></ul><a href=#transformer-based><h4 id=transformer-based><span class=hanchor arialabel=Anchor># </span>Transformer-based</h4></a><p>See &ldquo;For NLP&rdquo; section in <a href=/digitalgarden/AI/Deep-learning/Transformers rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Transformers>AI/Deep learning/Transformers</a></p></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/digitalgarden/AI/AI/ data-ctx=AI/NLP data-src=/AI/AI class=internal-link>Artificial Intelligence</a></li><li><a href=/digitalgarden/AI/Deep-learning/DL/ data-ctx=AI/NLP data-src=/AI/Deep-learning/DL class=internal-link>Deep Learning (DL)</a></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://carlos-gg.github.io/digitalgarden/js/graph.abd4bc2af3869a96524d7d23b76152c7.js></script></div></div><div id=contact_buttons><footer><p>Made by Carlos Alberto Gomez Gonzalez using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2022</p><ul><li><a href=https://carlos-gg.github.io/digitalgarden/>Home</a></li><li><a href=https://carlos-gg.github.io>Carlos'Homepage</a></li></ul></footer></div></div></body></html>