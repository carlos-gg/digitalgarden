<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="A Computer Science field connected to Artificial Intelligence and Computational Linguistics which focuses on interactions between computers and human language and a machineâ€™s ability to understand, or mimic the understanding of human language"><title>Natural Language Processing (NLP)</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=/icon.png><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Source+Sans+Pro:wght@400;600;700&family=Fira+Code:wght@400;700&display=swap" rel=stylesheet><link href=https://carlos-gg.github.io/digitalgarden/styles.cd61336c89ed6e03702366ce4a492b75.min.css rel=stylesheet><script src=https://carlos-gg.github.io/digitalgarden/js/darkmode.46b07878b7f5d9e26ad7a3c40f8a0605.min.js></script>
<script>const BASE_URL="https://carlos-gg.github.io/digitalgarden/",fetchData=Promise.all([fetch("https://carlos-gg.github.io/digitalgarden/indices/linkIndex.7f19b91fe04ef4490deebd0b2349a6c5.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://carlos-gg.github.io/digitalgarden/indices/contentIndex.c5b3d6c63081a3e6ee45bc1af6f63c48.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n}))</script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-S103D50NQ0"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S103D50NQ0",{anonymize_ip:!1})}</script><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://carlos-gg.github.io/digitalgarden/js/graph.27e8521c25c27c79dea35f434c486167.js></script>
<script>drawGraph("https://carlos-gg.github.io/digitalgarden/AI/NLP","https://carlos-gg.github.io/digitalgarden",[{"/moc":"#4388cc"}],-1,!0,!1,!0)</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://carlos-gg.github.io/digitalgarden/js/search.bc849b857f2c1b822264d40635bb67b6.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://carlos-gg.github.io/digitalgarden/>CarlosGG's Knowledge Garden ðŸª´</a></h1><svg tabindex="0" id="search-icon" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg><div class=spacer></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Natural Language Processing (NLP)</h1><p class=meta>Last updated July 1, 2022</p><ul class=tags></ul><aside class=mainTOC><details open><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#resources>Resources</a></li><li><a href=#books>Books</a></li><li><a href=#courses>Courses</a></li><li><a href=#talks>Talks</a></li><li><a href=#code>Code</a></li><li><a href=#subtopics>Subtopics</a><ol><li><a href=#text-preparation>Text preparation</a></li><li><a href=#feature-extraction-engineering>Feature extraction (engineering)</a><ol><li><a href=#bag-of-words-bow>Bag of words (BOW)</a></li><li><a href=#tfidf-term-frequency-inverse-document-frequency>tfâ€“idf (Term Frequency-Inverse Document Frequency)</a></li><li><a href=#word-embedding>Word embedding</a></li></ol></li><li><a href=#semantics>Semantics</a><ol><li><a href=#distributional-semantics>Distributional semantics</a></li><li><a href=#topic-modelling>Topic Modelling</a></li><li><a href=#neural-semantic-parsing>Neural semantic parsing</a></li><li><a href=#explicit-semantic-analysis>Explicit semantic analysis</a></li></ol></li><li><a href=#sentiment-analysis>Sentiment analysis</a></li><li><a href=#speech-recognition>Speech recognition</a><ol><li><a href=#hmm>HMM</a></li></ol></li><li><a href=#deep-learning-approaches>Deep learning approaches</a><ol><li><a href=#cnn-based>CNN-based</a></li><li><a href=#rnn-based>RNN-based</a></li></ol></li><li><a href=#sequence-to-sequence-seq2seq>Sequence-to-sequence (seq2seq)</a><ol><li><a href=#google-neural-machine-translation-gnmt>Google Neural Machine Translation (GNMT)</a></li><li><a href=#transformer-based>Transformer-based</a></li></ol></li></ol></li></ol></nav></details></aside><blockquote><p>A Computer Science field connected to Artificial Intelligence and Computational Linguistics which focuses on interactions between computers and human language and a machineâ€™s ability to understand, or mimic the understanding of human language</p></blockquote><h2 id=resources>Resources</h2><ul><li><a href=https://en.wikipedia.org/wiki/Natural_language_processing rel=noopener>https://en.wikipedia.org/wiki/Natural_language_processing</a></li><li>NLP is a branch of data science that consists of systematic processes for analyzing, understanding, and deriving information from the text data in a smart and efficient manner. By utilizing NLP and its components, one can organize the massive chunks of text data, perform numerous automated tasks and solve a wide range of problems such as â€“ automatic summarization, machine translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation etc.</li><li><a href=https://github.com/keon/awesome-nlp rel=noopener>https://github.com/keon/awesome-nlp</a></li><li><a href=https://github.com/omarsar/nlp_highlights rel=noopener>The most important NLP highlights of 2018</a></li><li><a href=https://github.com/jmportilla/Udemy---Machine-Learning/blob/master/NLP-%28Natural-Language-Processing%29.ipynb rel=noopener>NLP - Udemy ML</a></li><li><a href=https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/ rel=noopener>https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/</a></li><li><a href=https://www.datascience.com/blog/introduction-to-natural-language-processing-lexical-units-learn-data-science-tutorials rel=noopener>https://www.datascience.com/blog/introduction-to-natural-language-processing-lexical-units-learn-data-science-tutorials</a></li><li><a href=https://github.com/BotCube/awesome-bots rel=noopener>https://github.com/BotCube/awesome-bots</a></li><li><a href=http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html rel=noopener>http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html</a></li></ul><h2 id=books>Books</h2><ul><li>#BOOK
<a href=http://www.datascienceassn.org/sites/default/files/Natural-Language-Processing-with-Python.pdf rel=noopener>Natural Language Processing with Python (Bird, 2013 OREILLY)</a></li><li>#BOOK
<a href=https://www.packtpub.com/big-data-and-business-intelligence/mastering-natural-language-processing-python rel=noopener>Mastering NLP with Python (Chopra, 2016 PACKT)</a><ul><li><a href=https://github.com/PacktPublishing/Mastering-Natural-Language-Processing-with-Python rel=noopener>https://github.com/PacktPublishing/Mastering-Natural-Language-Processing-with-Python</a></li></ul></li><li>#BOOK
<a href=https://nlp.stanford.edu/IR-book/ rel=noopener>An Introduction to Information Retrieval (Manning 2009, CAMBRIDGE)</a></li><li>#BOOK
<a href=https://tidytextmining.com/ rel=noopener>Text mining with R (Silge, 2020 OREILLY)</a></li></ul><h2 id=courses>Courses</h2><ul><li>#COURSE
<a href=https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1162/ rel=noopener>NLP (Standford 15)</a></li><li>#COURSE
<a href=http://web.stanford.edu/class/cs224n/ rel=noopener>NLP with Deep Learning (Standford 16,17)</a></li><li>#COURSE
<a href=http://web.stanford.edu/class/cs224u/ rel=noopener>Natural Language Understanding (Standford 16)</a></li><li>#COURSE
<a href=https://www.cs.ox.ac.uk/teaching/courses/2016-2017/dl/ rel=noopener>Deep Learning for NLP (Oxford/Deepmind 17)</a><ul><li><a href=https://github.com/oxford-cs-deepnlp-2017 rel=noopener>https://github.com/oxford-cs-deepnlp-2017</a></li><li><a href=https://github.com/oxford-cs-deepnlp-2017/lectures rel=noopener>https://github.com/oxford-cs-deepnlp-2017/lectures</a></li><li><a href="https://www.youtube.com/playlist?list=PL613dYIGMXoZBtZhbyiBqb0QtgK6oJbpm" rel=noopener>https://www.youtube.com/playlist?list=PL613dYIGMXoZBtZhbyiBqb0QtgK6oJbpm</a></li></ul></li><li>#COURSE
<a href=https://github.com/yandexdataschool/nlp_course rel=noopener>YSDA Natural Language Processing course (Yandex)</a></li></ul><h2 id=talks>Talks</h2><ul><li>#TALK
<a href="https://www.youtube.com/watch?v=8S3qHHUKqYk" rel=noopener>Introduction to Natural Language Processing - Cambridge Data Science Bootcamp</a></li><li>#TALK
<a href="https://www.youtube.com/watch?v=HVdPWoZ_swY" rel=noopener>Rob Romijnders | Using deep learning in natural language processing (PyData)</a></li><li>#TALK
<a href="https://www.youtube.com/watch?v=7Z3ojkuul9U" rel=noopener>Jeff Abrahamson - WTF am I doing? An introduction to NLP and ANN&rsquo;s</a></li><li>#TALK
<a href="https://www.youtube.com/watch?v=AsW0QzbYVow" rel=noopener>Natural Language Processing with PySpark</a></li><li>#TALK
<a href="https://www.youtube.com/watch?v=qYpdW9cyEqY" rel=noopener>Feeding Word2vec with tens of billions of items, what could possibly go wrong? (Simon DollÃ©)</a></li><li>#TALK
<a href="https://www.youtube.com/watch?v=tdLmf8t4oqM" rel=noopener>Deep Learning for Natural Language Processing (2015)</a></li></ul><h2 id=code>Code</h2><ul><li>#CODE
<a href=https://github.com/PaddlePaddle/PaddleSpeech rel=noopener>PaddleSpeech</a><ul><li>PaddleSpeech is an open-source toolkit on PaddlePaddle platform for a variety of critical tasks in speech and audio, with the state-of-art and influential models</li><li>#PAPER
<a href=https://arxiv.org/pdf/2205.12007v1 rel=noopener>PaddleSpeech: An Easy-to-Use All-in-One Speech Toolkit (Zhang 2022)</a></li></ul></li><li>#CODE
<a href=https://github.com/recognai/rubrix rel=noopener>Rubrix</a><ul><li>Rubrix, open-source framework for data-centric NLP. Data annotation and monitoring for enterprise NLP</li></ul></li><li>#CODE
<a href=https://github.com/UKPLab/beir rel=noopener>Beir - Heterogeneous Benchmark for Information Retrieval</a><ul><li>#PAPER
<a href=https://arxiv.org/abs/2104.08663 rel=noopener>BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models (Thakur 2021)</a></li></ul></li><li>#CODE
<a href=https://fasttext.cc/ rel=noopener>FastText - Library for efficient text classification and representation learning</a></li><li>#CODE
<a href=https://github.com/facebookresearch/fairseq rel=noopener>Fairseq</a><ul><li>Fairseq(-py) is a sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling and other text generation tasks</li></ul></li><li>#CODE
<a href=https://github.com/OpenNMT/OpenNMT-tf rel=noopener>OpenNMT-tf - OpenNMT-tf is a general purpose sequence learning toolkit using TensorFlow 2</a></li><li>#CODE
<a href=https://opennlp.apache.org/ rel=noopener>OpenNLP - The Apache OpenNLP library is a machine learning based toolkit for the processing of natural language text</a></li><li>#CODE
<a href=https://github.com/minimaxir/textgenrnn rel=noopener>Textgen RNN</a></li><li>#CODE
<a href=https://stanfordnlp.github.io/CoreNLP/ rel=noopener>Standford CoreNLP</a></li><li>#CODE
<a href=http://www.nltk.org/ rel=noopener>NLTK - NLTK is a leading platform for building Python programs to work with human language data</a></li><li>#CODE
<a href=https://textblob.readthedocs.io/en/dev/ rel=noopener>Textblob</a><ul><li>TextBlobis a Python (2 and 3) library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.</li><li><a href=http://ahmedbesbes.com/how-to-mine-newsfeed-data-and-extract-interactive-insights-in-python.html rel=noopener>http://ahmedbesbes.com/how-to-mine-newsfeed-data-and-extract-interactive-insights-in-python.html</a></li><li><a href=http://nicschrading.com/project/Intro-to-NLP-in-Python/ rel=noopener>http://nicschrading.com/project/Intro-to-NLP-in-Python/</a></li><li><a href=https://github.com/NSchrading/intro-python-nlp/blob/master/Intro_Python_NLP.ipynb rel=noopener>https://github.com/NSchrading/intro-python-nlp/blob/master/Intro_Python_NLP.ipynb</a></li></ul></li><li>#CODE
<a href=https://spacy.io/ rel=noopener>Spacy (Industrial-strength NLP)</a><ul><li>#TALK
<a href="https://www.youtube.com/watch?v=gJJQs47aUQ0" rel=noopener>Matthew Honnibal - Designing spaCy: Industrial-strength NLP</a></li><li>#TALK
<a href="https://www.youtube.com/watch?v=6zm9NC9uRkk" rel=noopener>Patrick Harrison | Modern NLP in Python (SpaCy and gensim for recommendation-reviews analysis)</a></li><li><a href=https://spacy.io/docs/usage/tutorials rel=noopener>https://spacy.io/docs/usage/tutorials</a></li><li><a href=https://nicschrading.com/project/Intro-to-NLP-with-spaCy/ rel=noopener>https://nicschrading.com/project/Intro-to-NLP-with-spaCy/</a></li><li><a href=http://blog.thedataincubator.com/2016/04/nltk-vs-spacy-natural-language-processing-in-python/ rel=noopener>http://blog.thedataincubator.com/2016/04/nltk-vs-spacy-natural-language-processing-in-python/</a></li></ul></li><li>#CODE
<a href=http://www.parl.ai/ rel=noopener>ParlAI - A unified platform for sharing, training and evaluating dialogue models across many tasks</a></li><li>#CODE
<a href=https://github.com/johnsnowlabs/spark-nlp rel=noopener>Spark-NLP</a></li><li>#CODE
<a href=https://radimrehurek.com/gensim/ rel=noopener>Gensim - Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora</a></li></ul><p>Web scrapping and cleaning:</p><ul><li>#CODE
<a href=http://docs.python-requests.org/en/master/user/quickstart/ rel=noopener>Requests (For fetching HTML/XML from web pages)</a></li><li>#CODE
<a href=https://www.crummy.com/software/BeautifulSoup/ rel=noopener>BeautifulSoup (web scraping data parsing)</a><ul><li>#TALK
<a href="https://www.youtube.com/watch?v=XQgXKtPSzUI" rel=noopener>Introduction To Web Scraping (with Python and Beautiful Soup)</a></li></ul></li><li>#CODE
<a href=http://lxml.de/ rel=noopener>LXML (web scraping data parsing)</a></li><li>#CODE
<a href=https://dryscrape.readthedocs.io/en/latest/ rel=noopener>Dryscape (web scraping with javascript)</a></li><li>#CODE
<a href=http://selenium-python.readthedocs.io/ rel=noopener>Selenium (web scraping with javascript)</a></li><li>#CODE
<a href=https://github.com/scrapy/scrapy rel=noopener>Scrapy (web scraping framework)</a><ul><li><a href=https://doc.scrapy.org/en/latest/intro/overview.html rel=noopener>https://doc.scrapy.org/en/latest/intro/overview.html</a></li><li>Scrapy is an application framework for crawling web sites and extracting structured data which can be used for a wide range of useful applications, like data mining, information processing or historical archival.</li><li><a href=https://medium.com/@kaismh/extracting-data-from-websites-using-scrapy-e1e1e357651a#.j9hrs2scn rel=noopener>https://medium.com/@kaismh/extracting-data-from-websites-using-scrapy-e1e1e357651a#.j9hrs2scn</a></li></ul></li><li>#CODE
<a href=http://ftfy.readthedocs.org/ rel=noopener>python-ftfy: fixes text for you</a></li><li>#CODE
<a href=http://arrow.readthedocs.io/en/latest/ rel=noopener>Arrow - working with dates and times</a></li><li>#CODE
<a href=https://github.com/sachinvettithanam/beautifier rel=noopener>Beautifier - clean and prettify URLs and email addresses</a></li></ul><h2 id=subtopics>Subtopics</h2><h3 id=text-preparation>Text preparation</h3><ul><li><a href=http://nitin-panwar.github.io/Text-prepration-before-Sentiment-analysis/ rel=noopener>http://nitin-panwar.github.io/Text-prepration-before-Sentiment-analysis/</a><ul><li>Removing numbers</li><li>Removing Urls and Links</li><li>Removing stopwords</li><li>Stemming words</li><li>Suffix-dropping algorithms</li><li>Lemmatisation algorithms</li><li>n-gram analysis</li><li>Removing punctuation</li><li>Stripping whitespace</li><li>Checking for impure characters</li></ul></li><li><a href=http://thinknook.com/10-ways-to-improve-your-classification-algorithm-performance-2013-01-21/ rel=noopener>http://thinknook.com/10-ways-to-improve-your-classification-algorithm-performance-2013-01-21/</a></li></ul><h3 id=feature-extraction-engineering>Feature extraction (engineering)</h3><ul><li><a href=https://www.geeksforgeeks.org/feature-extraction-techniques-nlp/ rel=noopener>https://www.geeksforgeeks.org/feature-extraction-techniques-nlp/</a></li><li><a href=https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/ rel=noopener>https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/</a></li></ul><h4 id=bag-of-words-bow>Bag of words (BOW)</h4><ul><li>A commonly used model in methods of Text Classification. As part of the BOW model, a piece of text (sentence or a document) is represented as a bag or multiset of words, disregarding grammar and even word order and the frequency or occurrence of each word is used as a feature for training a classifier.</li><li>BoW is different from Word2vec, which weâ€™ll cover next. The main difference is that Word2vec produces one vector per word, whereas BoW produces one number (a wordcount). Word2vec is great for digging into documents and identifying content and subsets of content. Its vectors represent each wordâ€™s context, the ngrams of which it is a part. BoW is good for classifying documents as a whole.</li></ul><h4 id=tfidf-term-frequency-inverse-document-frequency>tfâ€“idf (Term Frequency-Inverse Document Frequency)</h4><ul><li><a href=https://en.wikipedia.org/wiki/Tf%E2%80%93idf rel=noopener>tf-idf</a></li><li>tfâ€“idf, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.</li><li>The tf-idf value increases proportionally to the number of times a word appears in the document, but is often offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general.</li><li><a href=https://deeplearning4j.org/bagofwords-tf-idf rel=noopener>https://deeplearning4j.org/bagofwords-tf-idf</a></li><li><a href=http://www.tfidf.com/ rel=noopener>http://www.tfidf.com/</a></li><li><a href=http://scikit-learn.org/stable/auto_examples/text/document_clustering.html#sphx-glr-auto-examples-text-document-clustering-py rel=noopener>http://scikit-learn.org/stable/auto_examples/text/document_clustering.html#sphx-glr-auto-examples-text-document-clustering-py</a></li></ul><h4 id=word-embedding>Word embedding</h4><p>Word embedding is the modern way of representing words as vectors. The aim of word embedding is to redefine the high dimensional word features into low dimensional feature vectors by preserving the contextual similarity in the corpus. They are widely used in deep learning models such as Convolutional Neural Networks and Recurrent Neural Networks.</p><ul><li>#PAPER
<a href=https://arxiv.org/abs/1310.4546 rel=noopener>Word2Vec: Distributed Representations of Words and Phrases and their Compositionality (Mikolov 2013)</a><ul><li><a href=https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf rel=noopener>https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf</a></li><li>Skip-gram model with negative sampling</li><li><a href=https://code.google.com/archive/p/word2vec/ rel=noopener>https://code.google.com/archive/p/word2vec/</a></li><li><a href="https://www.youtube.com/watch?v=yexR53My2O4" rel=noopener>Paper explained</a></li><li><a href=http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html rel=noopener>http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html</a></li><li><a href=http://www.deeplearningweekly.com/blog/demystifying-word2vec rel=noopener>http://www.deeplearningweekly.com/blog/demystifying-word2vec</a></li><li><a href=http://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/ rel=noopener>http://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/</a></li></ul></li><li><a href=https://fasttext.cc/ rel=noopener>FastText</a><ul><li><a href=https://arxiv.org/abs/1607.04606 rel=noopener>Enriching Word Vectors with Subword Information</a></li><li><a href=https://www.quora.com/What-is-the-main-difference-between-word2vec-and-fastText rel=noopener>https://www.quora.com/What-is-the-main-difference-between-word2vec-and-fastText</a></li><li><a href="https://colab.research.google.com/notebook#fileId=1MikDdEIDzFVpH7V6ZSTho8bwOALEmUoa" rel=noopener>Install FastText on Google colaboratory</a></li><li><a href=http://nlp.stanford.edu/projects/glove/ rel=noopener>Glove</a></li><li><a href=https://explosion.ai/blog/sense2vec-with-spacy rel=noopener>Sense2vec</a></li></ul></li></ul><h3 id=semantics>Semantics</h3><h4 id=distributional-semantics>Distributional semantics</h4><ul><li><p>General recipe:</p><ul><li>form a word-context matrix of counts (data)</li><li>perform dim reduction (SVD) for generalization</li></ul></li><li><p>For LSA the context is the document where the word appears.</p></li><li><p>For word2vec the context is just a work, nearby words (in some window) in a document.</p></li><li><p><a href=https://en.wikipedia.org/wiki/Latent_semantic_analysis rel=noopener>Latent semantic analysis</a></p><ul><li>The process of analyzing relationships between a set of documents and the terms they contain. Accomplished by producing a set of concepts related to the documents and terms. LSA assumes that words that are close in meaning will occur in similar pieces of text.</li><li>Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms.</li><li>LSA assumes that words that are close in meaning will occur in similar pieces of text. A matrix containing word counts per paragraph (rows represent unique words and columns represent each paragraph) is constructed from a large piece of text and SVD is used to reduce the number of rows while preserving the similarity structure among columns. Words are then compared by taking the cosine of the angle between the two vectors (or the dot product between the normalizations of the two vectors) formed by any two rows. Values close to 1 represent very similar words while values close to 0 represent very dissimilar words.</li><li><a href=http://mccormickml.com/2016/03/25/lsa-for-text-classification-tutorial/ rel=noopener>http://mccormickml.com/2016/03/25/lsa-for-text-classification-tutorial/</a></li><li><a href=https://github.com/chrisjmccormick/LSA_Classification/blob/master/runClassification_LSA.py rel=noopener>https://github.com/chrisjmccormick/LSA_Classification/blob/master/runClassification_LSA.py</a></li><li><a href=http://stackoverflow.com/questions/30590881/python-lsa-with-sklearn rel=noopener>http://stackoverflow.com/questions/30590881/python-lsa-with-sklearn</a></li></ul></li></ul><h4 id=topic-modelling>Topic Modelling</h4><ul><li><a href=https://en.wikipedia.org/wiki/Topic_model rel=noopener>https://en.wikipedia.org/wiki/Topic_model</a></li><li>Latent Dirichlet Allocation<ul><li>A common topic modeling technique, LDA is based on the premise that each document or piece of text is a mixture of a small number of topics and that each word in a document is attributable to one of the topics.</li><li><a href=http://engineering.flipboard.com/2017/02/storyclustering rel=noopener>http://engineering.flipboard.com/2017/02/storyclustering</a></li><li><a href=http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html rel=noopener>http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html</a></li><li><a href=http://ahmedbesbes.com/how-to-mine-newsfeed-data-and-extract-interactive-insights-in-python.html rel=noopener>http://ahmedbesbes.com/how-to-mine-newsfeed-data-and-extract-interactive-insights-in-python.html</a></li></ul></li></ul><h4 id=neural-semantic-parsing>Neural semantic parsing</h4><ul><li>#PAPER
<a href=https://www.aclweb.org/anthology/P18-5006/ rel=noopener>Neural Semantic Parsing (Jia & Liang 2016)</a></li></ul><h4 id=explicit-semantic-analysis>Explicit semantic analysis</h4><ul><li><a href=https://en.wikipedia.org/wiki/Explicit_semantic_analysis rel=noopener>https://en.wikipedia.org/wiki/Explicit_semantic_analysis</a></li><li>In natural language processing and information retrieval, explicit semantic analysis (ESA) is a vectorial representation of text (individual words or entire documents) that uses a document corpus as a knowledge base. Specifically, in ESA, a word is represented as a column vector in the tfâ€“idf matrix of the text corpus and a document (string of words) is represented as the centroid of the vectors representing its words. Typically, the text corpus is English Wikipedia, though other corpora including the Open Directory Project have been used.</li><li>Used in Information Retrieval, Document Classification and Semantic Relatedness calculation (i.e. how similar in meaning two words or pieces of text are to each other), ESA is the process of understanding the meaning of a piece text, as a combination of the concepts found in that text.</li><li>Corpus or Corpora. A usually large collection of documents that can be used to infer and validate linguistic rules, as well as to do statistical analysis and hypothesis testing.</li></ul><h3 id=sentiment-analysis>Sentiment analysis</h3><ul><li><a href=https://en.wikipedia.org/wiki/Sentiment_analysis rel=noopener>https://en.wikipedia.org/wiki/Sentiment_analysis</a></li><li>The use of Natural Language Processing techniques to extract subjective information from a piece of text. i.e. whether an author is being subjective or objective or even positive or negative. (can also be referred to as Opinion Mining)</li><li><a href=http://varianceexplained.org/r/trump-tweets/ rel=noopener>http://varianceexplained.org/r/trump-tweets/</a></li><li><a href=http://blog.aylien.com/sentiment-analysis-of-2-2-million-tweets-from-super-bowl-51/ rel=noopener>http://blog.aylien.com/sentiment-analysis-of-2-2-million-tweets-from-super-bowl-51/</a></li></ul><h3 id=speech-recognition>Speech recognition</h3><ul><li><a href=https://www.wikiwand.com/en/Speech_recognition rel=noopener>https://www.wikiwand.com/en/Speech_recognition</a></li></ul><h4 id=hmm>HMM</h4><ul><li><a href=https://en.wikipedia.org/wiki/Hidden_Markov_model rel=noopener>https://en.wikipedia.org/wiki/Hidden_Markov_model</a></li></ul><h3 id=deep-learning-approaches>Deep learning approaches</h3><ul><li><a href=https://nlpoverview.com/ rel=noopener>Modern Deep Learning Techniques Applied to Natural Language Processing</a></li><li><a href=https://github.com/brianspiering/awesome-dl4nlp rel=noopener>https://github.com/brianspiering/awesome-dl4nlp</a></li><li><a href=https://veredshwartz.blogspot.com/2018/08/deep-learning-in-nlp.html# rel=noopener>Deep Learning in NLP</a></li><li><a href=https://softwaremill.com/deep-learning-for-nlp/ rel=noopener>https://softwaremill.com/deep-learning-for-nlp/</a></li><li><a href=http://blog.aylien.com/modeling-documents-generative-adversarial-networks/ rel=noopener>http://blog.aylien.com/modeling-documents-generative-adversarial-networks/</a></li></ul><h4 id=cnn-based>CNN-based</h4><p>See <a href=/digitalgarden/AI/Deep-learning/CNNs rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/CNNs>AI/Deep learning/CNNs</a></p><ul><li><a href=https://cnvrg.io/cnn-sentence-classification/ rel=noopener>Convolutional Neural Network for Sentence Classification</a></li><li><a href=http://www.kdnuggets.com/2017/05/deep-learning-extract-knowledge-job-descriptions.html rel=noopener>http://www.kdnuggets.com/2017/05/deep-learning-extract-knowledge-job-descriptions.html</a></li><li><a href=https://offbit.github.io/how-to-read/ rel=noopener>How to read: Character level deep learning</a></li><li>#PAPER
<a href=https://distill.pub/2017/ctc/ rel=noopener>Connectionist Temporal Classification</a></li></ul><h4 id=rnn-based>RNN-based</h4><p>See <a href=/digitalgarden/AI/Deep-learning/RNNs rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/RNNs>AI/Deep learning/RNNs</a></p><ul><li><a href=https://adeshpande3.github.io/adeshpande3.github.io/Deep-Learning-Research-Review-Week-3-Natural-Language-Processing rel=noopener>RNN for NLP</a></li><li><a href=http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html rel=noopener>http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1709.03714 rel=noopener>RRA: Recurrent Residual Attention for Sequence Learning (2017)</a></li></ul><h3 id=sequence-to-sequence-seq2seq>Sequence-to-sequence (seq2seq)</h3><ul><li>#PAPER
<a href=https://arxiv.org/abs/1409.3215 rel=noopener>Sequence to Sequence Learning with Neural Networks</a></li><li>Sequence-to-sequence models are deep learning models that have achieved a lot of success in tasks like machine translation, text summarization, and image captioning. Google Translate started using such a model in production in late 2016. These models are explained in the two pioneering papers (Sutskever et al., 2014, Cho et al., 2014).</li><li>A sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an imagesâ€¦etc) and outputs another sequence of items.</li><li>Under the hood, the model is composed of an encoder and a decoder. The encoder processes each item in the input sequence, it compiles the information it captures into a vector (called the context). After processing the entire input sequence, the encoder sends the context over to the decoder, which begins producing the output sequence item by item.</li><li>The context is a vector (an array of numbers, basically) in the case of machine translation. The encoder and decoder tend to both be recurrent neural networks.</li><li><a href=https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/ rel=noopener>https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/</a></li></ul><h4 id=google-neural-machine-translation-gnmt>Google Neural Machine Translation (GNMT)</h4><ul><li><a href=https://en.wikipedia.org/wiki/Google_Neural_Machine_Translation rel=noopener>https://en.wikipedia.org/wiki/Google_Neural_Machine_Translation</a></li><li>Google Neural Machine Translation (GNMT) is a neural machine translation (NMT) system developed by Google and introduced in November 2016, that uses an artificial neural network to increase fluency and accuracy in Google Translate.</li><li>#PAPER
<a href=https://arxiv.org/abs/1611.04558 rel=noopener>Zero-shot translation</a><ul><li>Google Neural Machine Translation (GNMT) is a neural machine translation (NMT) system developed by Google and introduced in November 2016, that uses an artificial neural network to increase fluency and accuracy in Google Translate.</li><li><a href=https://research.googleblog.com/2016/11/zero-shot-translation-with-googles.html rel=noopener>https://research.googleblog.com/2016/11/zero-shot-translation-with-googles.html</a></li></ul></li></ul><h4 id=transformer-based>Transformer-based</h4><p>See &ldquo;For NLP&rdquo; section in <a href=/digitalgarden/AI/Deep-learning/Transformers rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Transformers>AI/Deep learning/Transformers</a></p></article><hr><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/digitalgarden/AI/AI>Artificial Intelligence</a></li><li><a href=/digitalgarden/AI/Deep-learning/DL>Deep Learning (DL)</a></li></ul></div></div><div id=contact_buttons><footer><p>Made by Carlos Alberto Gomez Gonzalez using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2022</p><ul><li><a href=https://carlos-gg.github.io/digitalgarden/>Home</a></li><a href=https://carlos-gg.github.io>Carlos'Homepage</a></ul></footer></div><script src=https://carlos-gg.github.io/digitalgarden/js/popover.e57188d2e4c06b0654e020b3a734bb62.min.js></script>
<script>initPopover("https://carlos-gg.github.io/digitalgarden")</script></div></body></html>