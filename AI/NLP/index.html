<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Resources  NLP is a branch of data science that consists of systematic processes for analyzing, understanding, and deriving information from the text data in a smart and efficient manner."><title>Natural Language Processing (NLP)</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=/icon.png><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Source+Sans+Pro:wght@400;600;700&family=Fira+Code:wght@400;700&display=swap" rel=stylesheet><style>:root{--lt-colours-light:var(--light) !important;--lt-colours-lightgray:var(--lightgray) !important;--lt-colours-dark:var(--secondary) !important;--lt-colours-secondary:var(--tertiary) !important;--lt-colours-gray:var(--outlinegray) !important}h1,h2,h3,h4,h5,h6,ol,ul,thead{font-family:Inter;color:var(--dark);font-weight:revert;margin:revert;padding:revert}p,ul,text{font-family:source sans pro,sans-serif;color:var(--gray);fill:var(--gray);font-weight:revert;margin:revert;padding:revert}#TableOfContents>ol{counter-reset:section;margin-left:0;padding-left:1.5em}#TableOfContents>ol>li{counter-increment:section}#TableOfContents>ol>li>ol{counter-reset:subsection}#TableOfContents>ol>li>ol>li{counter-increment:subsection}#TableOfContents>ol>li>ol>li::marker{content:counter(section)"." counter(subsection)"  "}#TableOfContents>ol>li::marker{content:counter(section)"  "}#TableOfContents>ol>li::marker,#TableOfContents>ol>li>ol>li::marker{font-family:Source Sans Pro;font-weight:700}footer{margin-top:4em;text-align:center}table{width:100%}img{width:100%;border-radius:3px;margin:1em 0}p>img+em{display:block;transform:translateY(-1em)}sup{line-height:0}p,tbody,li{font-family:Source Sans Pro;color:var(--gray);line-height:1.5em}blockquote{margin-left:0;border-left:3px solid var(--secondary);padding-left:1em;transition:border-color .2s ease}blockquote:hover{border-color:var(--tertiary)}table{padding:1.5em}td,th{padding:.1em .5em}.footnotes p{margin:.5em 0}.pagination{list-style:none;padding-left:0;display:flex;margin-top:2em;gap:1.5em;justify-content:center}.pagination>li{text-align:center;display:inline-block}.pagination>li a{background-color:initial!important}.pagination>li a[href$="#"]{opacity:.2}.section h3>a{font-weight:700;font-family:Inter;margin:0}.section p{margin-top:0}article>.meta{margin:-1.5em 0 1em;opacity:.7}article>.tags{list-style:none;padding-left:0}article>.tags .meta>h1{margin:0}article>.tags .meta>p{margin:0}article>.tags>li{display:inline-block}article>.tags>li>a{border-radius:8px;border:var(--outlinegray)1px solid;padding:.2em .5em}article>.tags>li>a::before{content:"#";margin-right:.3em;color:var(--outlinegray)}article a{font-family:Source Sans Pro;font-weight:600}article a.internal-link{text-decoration:none;background-color:rgba(143,159,169,.15);padding:0 .1em;margin:auto -.1em;border-radius:3px}.backlinks a{font-weight:600;font-size:.9rem}sup>a{text-decoration:none;padding:0 .1em 0 .2em}a{font-family:Inter,sans-serif;font-size:1em;font-weight:700;text-decoration:none;transition:all .2s ease;color:var(--secondary)}a:hover{color:var(--tertiary)!important}pre{font-family:fira code;padding:.75em;border-radius:3px;overflow-x:scroll}code{font-family:fira code;font-size:.85em;padding:.15em .3em;border-radius:5px;background:var(--lightgray)}html{scroll-behavior:smooth}html:lang(ar) p,html:lang(ar) h1,html:lang(ar) h2,html:lang(ar) h3,html:lang(ar) article{direction:rtl;text-align:right}body{margin:0;height:100vh;width:100vw;overflow-x:hidden;background-color:var(--light)}@keyframes fadeIn{0%{opacity:0}100%{opacity:1}}footer{margin-top:4em}footer>a{font-size:1em;color:var(--secondary);padding:0 .5em 3em}hr{width:25%;margin:4em auto;height:2px;border-radius:1px;border-width:0;color:var(--dark);background-color:var(--dark)}.singlePage{margin:4em 30vw}@media all and (max-width:1200px){.singlePage{margin:25px 5vw}}.page-end{display:flex;flex-direction:row;gap:2em}@media all and (max-width:780px){.page-end{flex-direction:column}}.page-end>*{flex:1 0}.page-end>.backlinks-container>ul{list-style:none;padding-left:0}.page-end>.backlinks-container>ul>li{margin:.5em 0;padding:.25em 1em;border:var(--outlinegray)1px solid;border-radius:5px}.page-end #graph-container{border:var(--outlinegray)1px solid;border-radius:5px}.centered{margin-top:30vh}article>h1{font-size:2em}header{display:flex;flex-direction:row;align-items:center}header>h1{font-size:2em}@media all and (max-width:600px){header>nav{display:none}}header>.spacer{flex:auto}header>svg{cursor:pointer;width:18px;min-width:18px;margin:0 1em}header>svg:hover .search-path{stroke:var(--tertiary)}header>svg .search-path{stroke:var(--gray);stroke-width:2px;transition:stroke .5s ease}#search-container{position:fixed;z-index:9999;left:0;top:0;width:100vw;height:100%;overflow:scroll;display:none;backdrop-filter:blur(4px);-webkit-backdrop-filter:blur(4px)}#search-container>div{width:50%;margin-top:15vh;margin-left:auto;margin-right:auto}@media all and (max-width:1200px){#search-container>div{width:90%}}#search-container>div>*{width:100%;border-radius:4px;background:var(--light);box-shadow:0 14px 50px rgba(27,33,48,.12),0 10px 30px rgba(27,33,48,.16);margin-bottom:2em}#search-container>div>input{box-sizing:border-box;padding:.5em 1em;font-family:Inter,sans-serif;color:var(--dark);font-size:1.1em;border:1px solid var(--outlinegray)}#search-container>div>input:focus{outline:none}#search-container>div>#results-container>.result-card{padding:1em;cursor:pointer;transition:background .2s ease;border:1px solid var(--outlinegray);border-bottom:none;width:100%;font-family:inherit;font-size:100%;line-height:1.15;margin:0;overflow:visible;text-transform:none;text-align:left;background:var(--light);outline:none}#search-container>div>#results-container>.result-card:hover,#search-container>div>#results-container>.result-card:focus{background:rgba(180,180,180,.15)}#search-container>div>#results-container>.result-card:first-of-type{border-top-left-radius:5px;border-top-right-radius:5px}#search-container>div>#results-container>.result-card:last-of-type{border-bottom-left-radius:5px;border-bottom-right-radius:5px;border-bottom:1px solid var(--outlinegray)}#search-container>div>#results-container>.result-card>h3,#search-container>div>#results-container>.result-card>p{margin:0}#search-container>div>#results-container>.result-card .search-highlight{background-color:#afbfc966;padding:.05em .2em;border-radius:3px}.section-ul{list-style:none;padding-left:0}.section-ul>li{border:1px solid var(--outlinegray);border-radius:5px;padding:0 1em;margin-bottom:1em}.section-ul>li h3{opacity:1;font-weight:700;margin-bottom:0}.section-ul>li .meta{opacity:.6}.popover{z-index:999;position:absolute;width:20em;display:inline-block;visibility:hidden;background-color:var(--light);padding:1em;border:1px solid var(--outlinegray);border-radius:5px;transform:translate(-50%,40%);opacity:0;pointer-events:none;transition:opacity .2s ease,transform .2s ease;transition-delay:.3s;user-select:none}@media all and (max-width:600px){.popover{display:none}}.popover.visible{opacity:1;visibility:visible;transform:translate(-50%,20%)}.popover>h3{font-size:1rem;margin:.25em 0}.popover>.meta{margin-top:.25em;opacity:.5}.popover>p{margin:0;font-weight:400;user-select:none}</style><style>.darkmode{float:right;padding:1em;min-width:30px;position:relative}@media all and (max-width:450px){.darkmode{padding:1em}}.darkmode>.toggle{display:none;box-sizing:border-box}.darkmode svg{opacity:0;position:absolute;width:20px;height:20px;top:calc(50% - 10px);margin:0 7px;fill:var(--gray);transition:opacity .1s ease}.toggle:checked~label>#dayIcon{opacity:0}.toggle:checked~label>#nightIcon{opacity:1}.toggle:not(:checked)~label>#dayIcon{opacity:1}.toggle:not(:checked)~label>#nightIcon{opacity:0}</style><style>.chroma{color:#f8f8f2;background-color:#282a36;overflow:hidden}.chroma .lntd{vertical-align:top;padding:0;margin:0;border:0}.chroma .lntable{border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block}.chroma .hl{display:block;width:100%;background-color:#ffc}.chroma .lnt{margin-right:.4em;padding:0 .4em;color:#7f7f7f}.chroma .ln{margin-right:.4em;padding:0 .4em;color:#7f7f7f}.chroma .k{color:#ff79c6}.chroma .kc{color:#ff79c6}.chroma .kd{color:#8be9fd;font-style:italic}.chroma .kn{color:#ff79c6}.chroma .kp{color:#ff79c6}.chroma .kr{color:#ff79c6}.chroma .kt{color:#8be9fd}.chroma .na{color:#50fa7b}.chroma .nb{color:#8be9fd;font-style:italic}.chroma .nc{color:#50fa7b}.chroma .nf{color:#50fa7b}.chroma .nl{color:#8be9fd;font-style:italic}.chroma .nt{color:#ff79c6}.chroma .nv{color:#8be9fd;font-style:italic}.chroma .vc{color:#8be9fd;font-style:italic}.chroma .vg{color:#8be9fd;font-style:italic}.chroma .vi{color:#8be9fd;font-style:italic}.chroma .s{color:#f1fa8c}.chroma .sa{color:#f1fa8c}.chroma .sb{color:#f1fa8c}.chroma .sc{color:#f1fa8c}.chroma .dl{color:#f1fa8c}.chroma .sd{color:#f1fa8c}.chroma .s2{color:#f1fa8c}.chroma .se{color:#f1fa8c}.chroma .sh{color:#f1fa8c}.chroma .si{color:#f1fa8c}.chroma .sx{color:#f1fa8c}.chroma .sr{color:#f1fa8c}.chroma .s1{color:#f1fa8c}.chroma .ss{color:#f1fa8c}.chroma .m{color:#bd93f9}.chroma .mb{color:#bd93f9}.chroma .mf{color:#bd93f9}.chroma .mh{color:#bd93f9}.chroma .mi{color:#bd93f9}.chroma .il{color:#bd93f9}.chroma .mo{color:#bd93f9}.chroma .o{color:#ff79c6}.chroma .ow{color:#ff79c6}.chroma .c{color:#6272a4}.chroma .ch{color:#6272a4}.chroma .cm{color:#6272a4}.chroma .c1{color:#6272a4}.chroma .cs{color:#6272a4}.chroma .cp{color:#ff79c6}.chroma .cpf{color:#ff79c6}.chroma .gd{color:#8b080b}.chroma .ge{text-decoration:underline}.chroma .gh{font-weight:700}.chroma .gi{font-weight:700}.chroma .go{color:#44475a}.chroma .gu{font-weight:700}.chroma .gl{text-decoration:underline}.lntd:first-of-type>.chroma{padding-right:0}.chroma code{font-family:fira code!important;font-size:.85em;line-height:1em;background:0 0;padding:0}.chroma{border-radius:3px;margin:0}</style><style>:root{--light:#faf8f8;--dark:#141021;--secondary:#284b63;--tertiary:#84a59d;--visited:#afbfc9;--primary:#f28482;--gray:#4e4e4e;--lightgray:#f0f0f0;--outlinegray:#dadada}[saved-theme=dark]{--light:#1e1e21 !important;--dark:#fbfffe !important;--secondary:#6b879a !important;--visited:#4a575e !important;--tertiary:#84a59d !important;--primary:#f58382 !important;--gray:#d4d4d4 !important;--lightgray:#292633 !important;--outlinegray:#343434 !important}</style><script>const userPref=window.matchMedia('(prefers-color-scheme: light)').matches?'light':'dark',currentTheme=localStorage.getItem('theme')??userPref;currentTheme&&document.documentElement.setAttribute('saved-theme',currentTheme);const switchTheme=a=>{a.target.checked?(document.documentElement.setAttribute('saved-theme','dark'),localStorage.setItem('theme','dark')):(document.documentElement.setAttribute('saved-theme','light'),localStorage.setItem('theme','light'))};window.addEventListener('DOMContentLoaded',()=>{const a=document.querySelector('#darkmode-toggle');a.addEventListener('change',switchTheme,!1),currentTheme==='dark'&&(a.checked=!0)})</script><script>let saved=!1;const fetchData=async()=>{if(saved)return saved;const promises=[fetch("https://carlos-gg.github.io/digitalgarden/linkIndex.json").then(a=>a.json()).then(a=>({index:a.index,links:a.links})),fetch("https://carlos-gg.github.io/digitalgarden/contentIndex.json").then(a=>a.json())],[{index,links},content]=await Promise.all(promises),res={index,links,content};return saved=res,res};fetchData()</script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-S103D50NQ0"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-S103D50NQ0',{anonymize_ip:!1})}</script><script>function htmlToElement(a){const b=document.createElement('template');return a=a.trim(),b.innerHTML=a,b.content.firstChild}const baseUrl="https://carlos-gg.github.io/digitalgarden".replace(window.location.origin,"");document.addEventListener("DOMContentLoaded",()=>{fetchData().then(({content:a})=>{const b=[...document.getElementsByClassName("internal-link")];b.forEach(b=>{const c=a[b.dataset.src.replace(baseUrl,"")];if(c){const d=`<div class="popover">
    <h3>${c.title}</h3>
    <p>${removeMarkdown(c.content).split(" ",20).join(" ")}...</p>
    <p class="meta">${new Date(c.lastmodified).toLocaleDateString()}</p>
</div>`,a=htmlToElement(d);b.appendChild(a),b.addEventListener("mouseover",()=>{a.classList.add("visible")}),b.addEventListener("mouseout",()=>{a.classList.remove("visible")})}})})})</script><script src=https://cdn.jsdelivr.net/npm/d3@6></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script>async function run(){const{index:a,links:p,content:u}=await fetchData(),j="https://carlos-gg.github.io/digitalgarden/AI/NLP".replace("https://carlos-gg.github.io/digitalgarden",""),n=[{"/moc":"#4388cc"}];let h=-1;const m=a=>[...new Set(a.flatMap(a=>[a.source,a.target]))],e=new Set,f=[j||"/","__SENTINEL"];if(h>=0)while(h>=0&&f.length>0){const b=f.shift();if(b==="__SENTINEL")h--,f.push("__SENTINEL");else{e.add(b);const c=a.links[b]||[],d=a.backlinks[b]||[];f.push(...c.map(a=>a.target),...d.map(a=>a.source))}}else m(p).forEach(a=>e.add(a));const g={nodes:[...e].map(a=>({id:a})),links:p.filter(a=>e.has(a.source)&&e.has(a.target))},k=a=>{if(a.id===j||a.id==="/"&&j==="")return"var(--g-node-active)";for(const b of n){const c=Object.keys(b)[0],d=b[c];if(a.id.startsWith(c))return d}return"var(--g-node)"},l=c=>{function d(b,a){b.active||c.alphaTarget(1).restart(),a.fx=a.x,a.fy=a.y}function e(a,b){b.fx=a.x,b.fy=a.y}function f(b,a){b.active||c.alphaTarget(0),a.fx=null,a.fy=null}const a=!0,b=()=>{};return d3.drag().on("start",a?d:b).on("drag",a?e:b).on("end",a?f:b)},c=250,b=document.getElementById("graph-container").offsetWidth,i=d3.forceSimulation(g.nodes).force("charge",d3.forceManyBody().strength(-30)).force("link",d3.forceLink(g.links).id(a=>a.id)).force("center",d3.forceCenter()),d=d3.select('#graph-container').append('svg').attr('width',b).attr('height',c).attr("viewBox",[-b/2,-c/2,b,c]),t=!1;if(t){const a=[{Current:"var(--g-node-active)"},{Note:"var(--g-node)"},...n];a.forEach((a,e)=>{const f=Object.keys(a)[0],g=a[f];d.append("circle").attr("cx",-b/2+20).attr("cy",c/2-30*(e+1)).attr("r",6).style("fill",g),d.append("text").attr("x",-b/2+40).attr("y",c/2-30*(e+1)).text(f).style("font-size","15px").attr("alignment-baseline","middle")})}const r=d.append("g").selectAll("line").data(g.links).join("line").attr("class","link").attr("stroke","var(--g-link)").attr("stroke-width",2).attr("data-source",a=>a.source.id).attr("data-target",a=>a.target.id),s=d.append("g").selectAll("g").data(g.nodes).enter().append("g"),q=s.append("circle").attr("class","node").attr("id",a=>a.id).attr("r",b=>{const c=a.links[b.id]?.length||0,d=a.backlinks[b.id]?.length||0;return 3+(c+d)/4}).attr("fill",k).style("cursor","pointer").on("click",(b,a)=>{window.location.href="https://carlos-gg.github.io/digitalgarden"+decodeURI(a.id).replace(/\s+/g,'-')}).on("mouseover",function(g,b){d3.selectAll(".node").transition().duration(100).attr("fill","var(--g-node-inactive)");const d=m([...a.links[b.id]||[],...a.backlinks[b.id]||[]]),e=d3.selectAll(".node").filter(a=>d.includes(a.id)),c=b.id,f=d3.selectAll(".link").filter(a=>a.source.id===c||a.target.id===c);e.transition().duration(200).attr("fill",k),f.transition().duration(200).attr("stroke","var(--g-link-active)"),d3.select(this.parentNode).select("text").raise().transition().duration(200).style("opacity",1)}).on("mouseleave",function(d,b){d3.selectAll(".node").transition().duration(200).attr("fill",k);const a=b.id,c=d3.selectAll(".link").filter(b=>b.source.id===a||b.target.id===a);c.transition().duration(200).attr("stroke","var(--g-link)"),d3.select(this.parentNode).select("text").transition().duration(200).style("opacity",0)}).call(l(i)),o=s.append("text").attr("dx",12).attr("dy",".35em").text(a=>u[decodeURI(a.id).replace(/\s+/g,'-')]?.title||"Untitled").style("opacity",0).style("pointer-events","none").call(l(i)),v=!0;v&&d.call(d3.zoom().extent([[0,0],[b,c]]).scaleExtent([.25,4]).on("zoom",({transform:a})=>{r.attr("transform",a),q.attr("transform",a),o.attr("transform",a)})),i.on("tick",()=>{r.attr("x1",a=>a.source.x).attr("y1",a=>a.source.y).attr("x2",a=>a.target.x).attr("y2",a=>a.target.y),q.attr("cx",a=>a.x).attr("cy",a=>a.y),o.attr("x",a=>a.x).attr("y",a=>a.y)})}run()</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/gh/nextapps-de/flexsearch@0.7.2/dist/flexsearch.bundle.js></script><script>const removeMarkdown=(c,b={listUnicodeChar:!1,stripListLeaders:!0,gfm:!0,useImgAltText:!1,preserveLinks:!1})=>{let a=c||"";a=a.replace(/^(-\s*?|\*\s*?|_\s*?){3,}\s*$/gm,"");try{b.stripListLeaders&&(b.listUnicodeChar?a=a.replace(/^([\s\t]*)([\*\-\+]|\d+\.)\s+/gm,b.listUnicodeChar+" $1"):a=a.replace(/^([\s\t]*)([\*\-\+]|\d+\.)\s+/gm,"$1")),b.gfm&&(a=a.replace(/\n={2,}/g,"\n").replace(/~{3}.*\n/g,"").replace(/~~/g,"").replace(/`{3}.*\n/g,"")),b.preserveLinks&&(a=a.replace(/\[(.*?)\][\[\(](.*?)[\]\)]/g,"$1 ($2)")),a=a.replace(/<[^>]*>/g,"").replace(/^[=\-]{2,}\s*$/g,"").replace(/\[\^.+?\](\: .*?$)?/g,"").replace(/\s{0,2}\[.*?\]: .*?$/g,"").replace(/\!\[(.*?)\][\[\(].*?[\]\)]/g,b.useImgAltText?"$1":"").replace(/\[(.*?)\][\[\(].*?[\]\)]/g,"$1").replace(/^\s{0,3}>\s?/g,"").replace(/(^|\n)\s{0,3}>\s?/g,"\n\n").replace(/^\s{1,2}\[(.*?)\]: (\S+)( ".*?")?\s*$/g,"").replace(/^(\n)?\s{0,}#{1,6}\s+| {0,}(\n)?\s{0,}#{0,} {0,}(\n)?\s{0,}$/gm,"$1$2$3").replace(/([\*_]{1,3})(\S.*?\S{0,1})\1/g,"$2").replace(/([\*_]{1,3})(\S.*?\S{0,1})\1/g,"$2").replace(/(`{3,})(.*?)\1/gm,"$2").replace(/`(.+?)`/g,"$1").replace(/\n{2,}/g,"\n\n")}catch(a){return console.error(a),c}return a}</script><script>async function run(){const g=new FlexSearch.Document({cache:!0,charset:"latin:extra",optimize:!0,worker:!0,document:{index:[{field:"content",tokenize:"strict",context:{resolution:5,depth:3,bidirectional:!0},suggest:!0},{field:"title",tokenize:"forward"}]}}),{content:d}=await fetchData();for(const[b,a]of Object.entries(d))g.add({id:b,title:a.title,content:removeMarkdown(a.content)});const j=(i,j)=>{const a=20,k=j.split(/\s+/).filter(a=>a!==""),b=i.split(/\s+/).filter(a=>a!==""),f=a=>k.some(b=>a.toLowerCase().startsWith(b.toLowerCase())),d=b.map(f);let e=0,g=0;for(let b=0;b<Math.max(d.length-a,0);b++){const f=d.slice(b,b+a),c=f.reduce((a,b)=>a+b,0);c>=e&&(e=c,g=b)}const c=Math.max(g-a,0),h=Math.min(c+2*a,b.length),l=b.slice(c,h).map(a=>{return f(a)?`<span class="search-highlight">${a}</span>`:a}).join(" ").replaceAll('</span> <span class="search-highlight">'," ");return`${c===0?"":"..."}${l}${h===b.length?"":"..."}`},m=({url:b,title:c,content:d,term:a})=>{const e=removeMarkdown(d),f=j(c,a),g=j(e,a);return`<button class="result-card" id="${b}">
        <h3>${f}</h3>
        <p>${g}</p>
    </button>`},h=(a,b)=>{window.location.href="https://carlos-gg.github.io/digitalgarden"+`${a}#:~:text=${encodeURIComponent(b)}`},l=a=>({id:a,url:a,title:d[a].title,content:d[a].content}),c=document.getElementById('search-bar'),e=document.getElementById("results-container");let b;c.addEventListener("keyup",a=>{if(a.key==="Enter"){const a=document.getElementsByClassName("result-card")[0];h(a.id,b)}}),c.addEventListener('input',a=>{b=a.target.value,g.search(b,[{field:"content",limit:10,suggest:!0},{field:"title",limit:5}]).then(d=>{const a=b=>{const a=d.filter(a=>a.field===b);return a.length===0?[]:[...a[0].result]},f=new Set([...a('title'),...a('content')]),c=[...f].map(l);if(c.length===0)e.innerHTML=`<button class="result-card">
                    <h3>No results.</h3>
                    <p>Try another search term?</p>
                </button>`;else{e.innerHTML=c.map(a=>m({...a,term:b})).join("\n");const a=document.getElementsByClassName("result-card");[...a].forEach(a=>{a.onclick=()=>h(a.id,b)})}})});const a=document.getElementById("search-container");function f(){a.style.display==="none"||a.style.display===""?(c.value="",e.innerHTML="",a.style.display="block",c.focus()):a.style.display="none"}function i(){a.style.display="none"}document.addEventListener('keydown',a=>{a.key==="/"&&(a.preventDefault(),f()),a.key==="Escape"&&(a.preventDefault(),i())});const k=document.getElementById("search-icon");k.addEventListener('click',a=>{f()}),k.addEventListener('keydown',a=>{f()}),a.addEventListener('click',a=>{i()}),document.getElementById("search-space").addEventListener('click',a=>{a.stopPropagation()})}run()</script><div class=singlePage><header><h1 id=page-title><a href=https://carlos-gg.github.io/digitalgarden>CarlosGG's Digital Garden 🪴</a></h1><svg tabindex="0" id="search-icon" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg><div class=spacer></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Natural Language Processing (NLP)</h1><p class=meta>Last updated March 11, 2022</p><ul class=tags></ul><aside class=mainTOC><h3>Table of Contents</h3><nav id=TableOfContents><ol><li><a href=#resources>Resources</a></li><li><a href=#books>Books</a></li><li><a href=#courses>Courses</a></li><li><a href=#talks>Talks</a></li><li><a href=#code>Code</a></li><li><a href=#subtopics>Subtopics</a><ol><li><a href=#text-preparation>Text preparation</a></li><li><a href=#feature-extraction-engineering>Feature extraction (engineering)</a></li><li><a href=#semantics>Semantics</a></li><li><a href=#sentiment-analysis>Sentiment analysis</a></li><li><a href=#speech-recognition>Speech recognition</a></li><li><a href=#deep-learning-approaches>Deep learning approaches</a></li><li><a href=#sequence-to-sequence-seq2seq>Sequence-to-sequence (seq2seq)</a></li></ol></li></ol></nav></aside><h2 id=resources>Resources</h2><ul><li>NLP is a branch of data science that consists of systematic processes for analyzing, understanding, and deriving information from the text data in a smart and efficient manner. By utilizing NLP and its components, one can organize the massive chunks of text data, perform numerous automated tasks and solve a wide range of problems such as – automatic summarization, machine translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation etc.</li><li><a href=https://en.wikipedia.org/wiki/Natural_language_processing>https://en.wikipedia.org/wiki/Natural_language_processing</a></li><li>A Computer Science field connected to Artificial Intelligence and Computational Linguistics which focuses on interactions between computers and human language and a machine’s ability to understand, or mimic the understanding of human language</li><li><a href=https://github.com/keon/awesome-nlp>https://github.com/keon/awesome-nlp</a></li><li><a href=https://github.com/omarsar/nlp_highlights rel=noopener>The most important NLP highlights of 2018</a></li><li><a href=https://github.com/jmportilla/Udemy---Machine-Learning/blob/master/NLP-%28Natural-Language-Processing%29.ipynb rel=noopener>NLP - Udemy ML</a></li><li><a href=https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/>https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/</a></li><li><a href=https://www.datascience.com/blog/introduction-to-natural-language-processing-lexical-units-learn-data-science-tutorials>https://www.datascience.com/blog/introduction-to-natural-language-processing-lexical-units-learn-data-science-tutorials</a></li><li><a href=https://github.com/BotCube/awesome-bots>https://github.com/BotCube/awesome-bots</a></li><li><a href=http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html>http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html</a></li></ul><h2 id=books>Books</h2><ul><li>#BOOK
<a href=http://www.datascienceassn.org/sites/default/files/Natural-Language-Processing-with-Python.pdf rel=noopener>Natural Language Processing with Python (Bird, 2013 OREILLY)</a></li><li>#BOOK
<a href=https://www.packtpub.com/big-data-and-business-intelligence/mastering-natural-language-processing-python rel=noopener>Mastering NLP with Python (Chopra, 2016 PACKT)</a><ul><li><a href=https://github.com/PacktPublishing/Mastering-Natural-Language-Processing-with-Python>https://github.com/PacktPublishing/Mastering-Natural-Language-Processing-with-Python</a></li></ul></li><li>#BOOK
<a href=https://nlp.stanford.edu/IR-book/ rel=noopener>An Introduction to Information Retrieval (Manning 2009, CAMBRIDGE)</a></li><li>#BOOK
<a href=https://tidytextmining.com/ rel=noopener>Text mining with R (Silge, 2020 OREILLY)</a></li></ul><h2 id=courses>Courses</h2><ul><li>#COURSE
<a href=https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1162/ rel=noopener>NLP (Standford 15)</a></li><li>#COURSE
<a href=http://web.stanford.edu/class/cs224n/ rel=noopener>NLP with Deep Learning (Standford 16,17)</a></li><li>#COURSE
<a href=http://web.stanford.edu/class/cs224u/ rel=noopener>Natural Language Understanding (Standford 16)</a></li><li>#COURSE
<a href=https://www.cs.ox.ac.uk/teaching/courses/2016-2017/dl/ rel=noopener>Deep Learning for NLP (Oxford/Deepmind 17)</a><ul><li><a href=https://github.com/oxford-cs-deepnlp-2017>https://github.com/oxford-cs-deepnlp-2017</a></li><li><a href=https://github.com/oxford-cs-deepnlp-2017/lectures>https://github.com/oxford-cs-deepnlp-2017/lectures</a></li><li><a href="https://www.youtube.com/playlist?list=PL613dYIGMXoZBtZhbyiBqb0QtgK6oJbpm">https://www.youtube.com/playlist?list=PL613dYIGMXoZBtZhbyiBqb0QtgK6oJbpm</a></li></ul></li><li>#COURSE
<a href=https://github.com/yandexdataschool/nlp_course rel=noopener>YSDA Natural Language Processing course (Yandex)</a></li></ul><h2 id=talks>Talks</h2><ul><li>#TALK
<a href="https://www.youtube.com/watch?v=8S3qHHUKqYk" rel=noopener>Introduction to Natural Language Processing - Cambridge Data Science Bootcamp</a></li><li>#TALK
<a href="https://www.youtube.com/watch?v=HVdPWoZ_swY" rel=noopener>Rob Romijnders | Using deep learning in natural language processing (PyData)</a></li><li>#TALK
<a href="https://www.youtube.com/watch?v=7Z3ojkuul9U" rel=noopener>Jeff Abrahamson - WTF am I doing? An introduction to NLP and ANN&rsquo;s</a></li><li>#TALK
<a href="https://www.youtube.com/watch?v=AsW0QzbYVow" rel=noopener>Natural Language Processing with PySpark</a></li><li>#TALK
<a href="https://www.youtube.com/watch?v=qYpdW9cyEqY" rel=noopener>Feeding Word2vec with tens of billions of items, what could possibly go wrong? (Simon Dollé)</a></li><li>#TALK
<a href="https://www.youtube.com/watch?v=tdLmf8t4oqM" rel=noopener>Deep Learning for Natural Language Processing (2015)</a></li></ul><h2 id=code>Code</h2><ul><li>#CODE
<a href=https://github.com/UKPLab/beir rel=noopener>Beir - Heterogeneous Benchmark for Information Retrieval</a><ul><li>#PAPER
<a href=https://arxiv.org/abs/2104.08663 rel=noopener>BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models (Thakur 2021)</a></li></ul></li><li>#CODE
<a href=https://fasttext.cc/ rel=noopener>FastText - Library for efficient text classification and representation learning</a></li><li>#CODE
<a href=https://github.com/facebookresearch/fairseq rel=noopener>fairseq</a></li><li>#CODE
<a href=https://github.com/OpenNMT/OpenNMT-tf rel=noopener>OpenNMT-tf - OpenNMT-tf is a general purpose sequence learning toolkit using TensorFlow 2</a></li><li>#CODE
<a href=https://opennlp.apache.org/ rel=noopener>OpenNLP - The Apache OpenNLP library is a machine learning based toolkit for the processing of natural language text</a></li><li>#CODE
<a href=https://huggingface.co/ rel=noopener>Huggingface</a><ul><li><a href=https://github.com/huggingface/datasets rel=noopener>Datasets</a></li><li><a href=https://github.com/huggingface/nlp-viewer rel=noopener>NLP-viewer</a><ul><li><a href=https://huggingface.co/nlp/viewer/>https://huggingface.co/nlp/viewer/</a></li></ul></li></ul></li><li>#CODE
<a href=https://github.com/minimaxir/textgenrnn rel=noopener>Textgen RNN</a></li><li>#CODE
<a href=https://stanfordnlp.github.io/CoreNLP/ rel=noopener>Standford CoreNLP</a></li><li>#CODE
<a href=http://www.nltk.org/ rel=noopener>NLTK - NLTK is a leading platform for building Python programs to work with human language data</a></li><li>#CODE
<a href=https://textblob.readthedocs.io/en/dev/ rel=noopener>Textblob</a><ul><li>TextBlobis a Python (2 and 3) library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.</li><li><a href=http://ahmedbesbes.com/how-to-mine-newsfeed-data-and-extract-interactive-insights-in-python.html>http://ahmedbesbes.com/how-to-mine-newsfeed-data-and-extract-interactive-insights-in-python.html</a></li><li><a href=http://nicschrading.com/project/Intro-to-NLP-in-Python/>http://nicschrading.com/project/Intro-to-NLP-in-Python/</a></li><li><a href=https://github.com/NSchrading/intro-python-nlp/blob/master/Intro_Python_NLP.ipynb>https://github.com/NSchrading/intro-python-nlp/blob/master/Intro_Python_NLP.ipynb</a></li></ul></li><li>#CODE
<a href=https://spacy.io/ rel=noopener>Spacy (Industrial-strength NLP)</a><ul><li>#TALK
<a href="https://www.youtube.com/watch?v=gJJQs47aUQ0" rel=noopener>Matthew Honnibal - Designing spaCy: Industrial-strength NLP</a></li><li>#TALK
<a href="https://www.youtube.com/watch?v=6zm9NC9uRkk" rel=noopener>Patrick Harrison | Modern NLP in Python (SpaCy and gensim for recommendation-reviews analysis)</a></li><li><a href=https://spacy.io/docs/usage/tutorials>https://spacy.io/docs/usage/tutorials</a></li><li><a href=https://nicschrading.com/project/Intro-to-NLP-with-spaCy/>https://nicschrading.com/project/Intro-to-NLP-with-spaCy/</a></li><li><a href=http://blog.thedataincubator.com/2016/04/nltk-vs-spacy-natural-language-processing-in-python/>http://blog.thedataincubator.com/2016/04/nltk-vs-spacy-natural-language-processing-in-python/</a></li></ul></li><li>#CODE
<a href=http://www.parl.ai/ rel=noopener>ParlAI - A unified platform for sharing, training and evaluating dialogue models across many tasks</a></li><li>#CODE
<a href=https://github.com/johnsnowlabs/spark-nlp rel=noopener>Spark-NLP</a></li><li>#CODE
<a href=https://radimrehurek.com/gensim/ rel=noopener>Gensim - Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora</a></li></ul><p>Web scrapping and cleaning:</p><ul><li>#CODE
<a href=http://docs.python-requests.org/en/master/user/quickstart/ rel=noopener>Requests (For fetching HTML/XML from web pages)</a></li><li>#CODE
<a href=https://www.crummy.com/software/BeautifulSoup/ rel=noopener>BeautifulSoup (web scraping data parsing)</a><ul><li>#TALK
<a href="https://www.youtube.com/watch?v=XQgXKtPSzUI" rel=noopener>Introduction To Web Scraping (with Python and Beautiful Soup)</a></li></ul></li><li>#CODE
<a href=http://lxml.de/ rel=noopener>LXML (web scraping data parsing)</a></li><li>#CODE
<a href=https://dryscrape.readthedocs.io/en/latest/ rel=noopener>Dryscape (web scraping with javascript)</a></li><li>#CODE
<a href=http://selenium-python.readthedocs.io/ rel=noopener>Selenium (web scraping with javascript)</a></li><li>#CODE
<a href=https://github.com/scrapy/scrapy rel=noopener>Scrapy (web scraping framework)</a><ul><li><a href=https://doc.scrapy.org/en/latest/intro/overview.html>https://doc.scrapy.org/en/latest/intro/overview.html</a></li><li>Scrapy is an application framework for crawling web sites and extracting structured data which can be used for a wide range of useful applications, like data mining, information processing or historical archival.</li><li><a href=https://medium.com/@kaismh/extracting-data-from-websites-using-scrapy-e1e1e357651a#.j9hrs2scn>https://medium.com/@kaismh/extracting-data-from-websites-using-scrapy-e1e1e357651a#.j9hrs2scn</a></li></ul></li><li>#CODE
<a href=http://ftfy.readthedocs.org/ rel=noopener>python-ftfy: fixes text for you</a></li><li>#CODE
<a href=http://arrow.readthedocs.io/en/latest/ rel=noopener>Arrow - working with dates and times</a></li><li>#CODE
<a href=https://github.com/sachinvettithanam/beautifier rel=noopener>Beautifier - clean and prettify URLs and email addresses</a></li></ul><h2 id=subtopics>Subtopics</h2><h3 id=text-preparation>Text preparation</h3><ul><li><a href=http://nitin-panwar.github.io/Text-prepration-before-Sentiment-analysis/>http://nitin-panwar.github.io/Text-prepration-before-Sentiment-analysis/</a><ul><li>Removing numbers</li><li>Removing Urls and Links</li><li>Removing stopwords</li><li>Stemming words</li><li>Suffix-dropping algorithms</li><li>Lemmatisation algorithms</li><li>n-gram analysis</li><li>Removing punctuation</li><li>Stripping whitespace</li><li>Checking for impure characters</li></ul></li><li><a href=http://thinknook.com/10-ways-to-improve-your-classification-algorithm-performance-2013-01-21/>http://thinknook.com/10-ways-to-improve-your-classification-algorithm-performance-2013-01-21/</a></li></ul><h3 id=feature-extraction-engineering>Feature extraction (engineering)</h3><ul><li><a href=https://www.geeksforgeeks.org/feature-extraction-techniques-nlp/>https://www.geeksforgeeks.org/feature-extraction-techniques-nlp/</a></li><li><a href=https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/>https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/</a></li></ul><h4 id=bag-of-words-bow>Bag of words (BOW)</h4><ul><li>A commonly used model in methods of Text Classification. As part of the BOW model, a piece of text (sentence or a document) is represented as a bag or multiset of words, disregarding grammar and even word order and the frequency or occurrence of each word is used as a feature for training a classifier.</li><li>BoW is different from Word2vec, which we’ll cover next. The main difference is that Word2vec produces one vector per word, whereas BoW produces one number (a wordcount). Word2vec is great for digging into documents and identifying content and subsets of content. Its vectors represent each word’s context, the ngrams of which it is a part. BoW is good for classifying documents as a whole.</li></ul><h4 id=tfidf-term-frequency-inverse-document-frequency>tf–idf (Term Frequency-Inverse Document Frequency)</h4><ul><li><a href=https://en.wikipedia.org/wiki/Tf%E2%80%93idf>https://en.wikipedia.org/wiki/Tf%E2%80%93idf</a></li><li>tf–idf, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.</li><li>The tf-idf value increases proportionally to the number of times a word appears in the document, but is often offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general.</li><li><a href=https://deeplearning4j.org/bagofwords-tf-idf>https://deeplearning4j.org/bagofwords-tf-idf</a></li><li><a href=http://www.tfidf.com/>http://www.tfidf.com/</a></li><li><a href=http://scikit-learn.org/stable/auto_examples/text/document_clustering.html#sphx-glr-auto-examples-text-document-clustering-py>http://scikit-learn.org/stable/auto_examples/text/document_clustering.html#sphx-glr-auto-examples-text-document-clustering-py</a></li></ul><h4 id=word-embedding>Word embedding</h4><p>Word embedding is the modern way of representing words as vectors. The aim of word embedding is to redefine the high dimensional word features into low dimensional feature vectors by preserving the contextual similarity in the corpus. They are widely used in deep learning models such as Convolutional Neural Networks and Recurrent Neural Networks.</p><ul><li>#PAPER
<a href=https://arxiv.org/abs/1310.4546 rel=noopener>Word2Vec: Distributed Representations of Words and Phrases and their Compositionality (Mikolov 2013)</a><ul><li><a href=https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf>https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf</a></li><li>Skip-gram model with negative sampling</li><li><a href=https://code.google.com/archive/p/word2vec/>https://code.google.com/archive/p/word2vec/</a></li><li><a href="https://www.youtube.com/watch?v=yexR53My2O4" rel=noopener>Paper explained</a></li><li><a href=http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html>http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html</a></li><li><a href=http://www.deeplearningweekly.com/blog/demystifying-word2vec>http://www.deeplearningweekly.com/blog/demystifying-word2vec</a></li><li><a href=http://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/>http://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/</a></li></ul></li><li><a href=https://fasttext.cc/ rel=noopener>FastText</a><ul><li><a href=https://arxiv.org/abs/1607.04606 rel=noopener>Enriching Word Vectors with Subword Information</a></li><li><a href=https://www.quora.com/What-is-the-main-difference-between-word2vec-and-fastText>https://www.quora.com/What-is-the-main-difference-between-word2vec-and-fastText</a></li><li><a href="https://colab.research.google.com/notebook#fileId=1MikDdEIDzFVpH7V6ZSTho8bwOALEmUoa" rel=noopener>Install FastText on Google colaboratory</a></li><li><a href=http://nlp.stanford.edu/projects/glove/ rel=noopener>Glove</a></li><li><a href=https://explosion.ai/blog/sense2vec-with-spacy rel=noopener>Sense2vec</a></li></ul></li></ul><h3 id=semantics>Semantics</h3><h4 id=distributional-semantics>Distributional semantics</h4><ul><li><p>General recipe:</p><ul><li>form a word-context matrix of counts (data)</li><li>perform dim reduction (SVD) for generalization</li></ul></li><li><p>For LSA the context is the document where the word appears.</p></li><li><p>For word2vec the context is just a work, nearby words (in some window) in a document.</p></li><li><p><a href=https://en.wikipedia.org/wiki/Latent_semantic_analysis rel=noopener>Latent semantic analysis</a></p><ul><li>The process of analyzing relationships between a set of documents and the terms they contain. Accomplished by producing a set of concepts related to the documents and terms. LSA assumes that words that are close in meaning will occur in similar pieces of text.</li><li>Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms.</li><li>LSA assumes that words that are close in meaning will occur in similar pieces of text. A matrix containing word counts per paragraph (rows represent unique words and columns represent each paragraph) is constructed from a large piece of text and SVD is used to reduce the number of rows while preserving the similarity structure among columns. Words are then compared by taking the cosine of the angle between the two vectors (or the dot product between the normalizations of the two vectors) formed by any two rows. Values close to 1 represent very similar words while values close to 0 represent very dissimilar words.</li><li><a href=http://mccormickml.com/2016/03/25/lsa-for-text-classification-tutorial/>http://mccormickml.com/2016/03/25/lsa-for-text-classification-tutorial/</a></li><li><a href=https://github.com/chrisjmccormick/LSA_Classification/blob/master/runClassification_LSA.py>https://github.com/chrisjmccormick/LSA_Classification/blob/master/runClassification_LSA.py</a></li><li><a href=http://stackoverflow.com/questions/30590881/python-lsa-with-sklearn>http://stackoverflow.com/questions/30590881/python-lsa-with-sklearn</a></li></ul></li></ul><h4 id=topic-modelling>Topic Modelling</h4><ul><li><a href=https://en.wikipedia.org/wiki/Topic_model>https://en.wikipedia.org/wiki/Topic_model</a></li><li>Latent Dirichlet Allocation<ul><li>A common topic modeling technique, LDA is based on the premise that each document or piece of text is a mixture of a small number of topics and that each word in a document is attributable to one of the topics.</li><li><a href=http://engineering.flipboard.com/2017/02/storyclustering>http://engineering.flipboard.com/2017/02/storyclustering</a></li><li><a href=http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html>http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html</a></li><li><a href=http://ahmedbesbes.com/how-to-mine-newsfeed-data-and-extract-interactive-insights-in-python.html>http://ahmedbesbes.com/how-to-mine-newsfeed-data-and-extract-interactive-insights-in-python.html</a></li></ul></li></ul><h4 id=neural-semantic-parsing>Neural semantic parsing</h4><ul><li>#PAPER
<a href=https://www.aclweb.org/anthology/P18-5006/ rel=noopener>Neural Semantic Parsing (Jia & Liang 2016)</a></li></ul><h4 id=explicit-semantic-analysis>Explicit semantic analysis</h4><ul><li><a href=https://en.wikipedia.org/wiki/Explicit_semantic_analysis>https://en.wikipedia.org/wiki/Explicit_semantic_analysis</a></li><li>In natural language processing and information retrieval, explicit semantic analysis (ESA) is a vectorial representation of text (individual words or entire documents) that uses a document corpus as a knowledge base. Specifically, in ESA, a word is represented as a column vector in the tf–idf matrix of the text corpus and a document (string of words) is represented as the centroid of the vectors representing its words. Typically, the text corpus is English Wikipedia, though other corpora including the Open Directory Project have been used.</li><li>Used in Information Retrieval, Document Classification and Semantic Relatedness calculation (i.e. how similar in meaning two words or pieces of text are to each other), ESA is the process of understanding the meaning of a piece text, as a combination of the concepts found in that text.</li><li>Corpus or Corpora. A usually large collection of documents that can be used to infer and validate linguistic rules, as well as to do statistical analysis and hypothesis testing.</li></ul><h3 id=sentiment-analysis>Sentiment analysis</h3><ul><li><a href=https://en.wikipedia.org/wiki/Sentiment_analysis>https://en.wikipedia.org/wiki/Sentiment_analysis</a></li><li>The use of Natural Language Processing techniques to extract subjective information from a piece of text. i.e. whether an author is being subjective or objective or even positive or negative. (can also be referred to as Opinion Mining)</li><li><a href=http://varianceexplained.org/r/trump-tweets/>http://varianceexplained.org/r/trump-tweets/</a></li><li><a href=http://blog.aylien.com/sentiment-analysis-of-2-2-million-tweets-from-super-bowl-51/>http://blog.aylien.com/sentiment-analysis-of-2-2-million-tweets-from-super-bowl-51/</a></li></ul><h3 id=speech-recognition>Speech recognition</h3><ul><li><a href=https://www.wikiwand.com/en/Speech_recognition>https://www.wikiwand.com/en/Speech_recognition</a></li></ul><h4 id=hmm>HMM</h4><ul><li><a href=https://en.wikipedia.org/wiki/Hidden_Markov_model>https://en.wikipedia.org/wiki/Hidden_Markov_model</a></li></ul><h3 id=deep-learning-approaches>Deep learning approaches</h3><ul><li><a href=https://nlpoverview.com/ rel=noopener>Modern Deep Learning Techniques Applied to Natural Language Processing</a></li><li><a href=https://github.com/brianspiering/awesome-dl4nlp>https://github.com/brianspiering/awesome-dl4nlp</a></li><li><a href=https://veredshwartz.blogspot.com/2018/08/deep-learning-in-nlp.html# rel=noopener>Deep Learning in NLP</a></li><li><a href=https://softwaremill.com/deep-learning-for-nlp/>https://softwaremill.com/deep-learning-for-nlp/</a></li><li><a href=http://blog.aylien.com/modeling-documents-generative-adversarial-networks/>http://blog.aylien.com/modeling-documents-generative-adversarial-networks/</a></li></ul><h4 id=cnn-based>CNN-based</h4><p>See
<a href=/digitalgarden/AI/Deep-learning/CNNs rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/CNNs>CNNs</a></p><ul><li><a href=http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/ rel=noopener>CNN for NLP</a></li><li><a href=http://www.kdnuggets.com/2017/05/deep-learning-extract-knowledge-job-descriptions.html>http://www.kdnuggets.com/2017/05/deep-learning-extract-knowledge-job-descriptions.html</a></li><li><a href=https://offbit.github.io/how-to-read/ rel=noopener>How to read: Character level deep learning</a></li><li>#PAPER
<a href=https://distill.pub/2017/ctc/ rel=noopener>Connectionist Temporal Classification</a></li></ul><h4 id=rnn-based>RNN-based</h4><p>See
<a href=/digitalgarden/AI/Deep-learning/RNNs rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/RNNs>RNNs</a></p><ul><li><a href=https://adeshpande3.github.io/adeshpande3.github.io/Deep-Learning-Research-Review-Week-3-Natural-Language-Processing rel=noopener>RNN for NLP</a></li><li><a href=http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html>http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1709.03714 rel=noopener>RRA: Recurrent Residual Attention for Sequence Learning (2017)</a></li></ul><h3 id=sequence-to-sequence-seq2seq>Sequence-to-sequence (seq2seq)</h3><ul><li>#PAPER
<a href=https://arxiv.org/abs/1409.3215 rel=noopener>Sequence to Sequence Learning with Neural Networks</a></li><li>Sequence-to-sequence models are deep learning models that have achieved a lot of success in tasks like machine translation, text summarization, and image captioning. Google Translate started using such a model in production in late 2016. These models are explained in the two pioneering papers (Sutskever et al., 2014, Cho et al., 2014).</li><li>A sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an images…etc) and outputs another sequence of items.</li><li>Under the hood, the model is composed of an encoder and a decoder. The encoder processes each item in the input sequence, it compiles the information it captures into a vector (called the context). After processing the entire input sequence, the encoder sends the context over to the decoder, which begins producing the output sequence item by item.</li><li>The context is a vector (an array of numbers, basically) in the case of machine translation. The encoder and decoder tend to both be recurrent neural networks.</li><li><a href=https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/>https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/</a></li></ul><h4 id=google-neural-machine-translation-gnmt>Google Neural Machine Translation (GNMT)</h4><ul><li><a href=https://en.wikipedia.org/wiki/Google_Neural_Machine_Translation>https://en.wikipedia.org/wiki/Google_Neural_Machine_Translation</a></li><li>Google Neural Machine Translation (GNMT) is a neural machine translation (NMT) system developed by Google and introduced in November 2016, that uses an artificial neural network to increase fluency and accuracy in Google Translate.</li><li>#PAPER
<a href=https://arxiv.org/abs/1611.04558 rel=noopener>Zero-shot translation</a><ul><li>Google Neural Machine Translation (GNMT) is a neural machine translation (NMT) system developed by Google and introduced in November 2016, that uses an artificial neural network to increase fluency and accuracy in Google Translate.</li><li><a href=https://research.googleblog.com/2016/11/zero-shot-translation-with-googles.html>https://research.googleblog.com/2016/11/zero-shot-translation-with-googles.html</a></li></ul></li></ul><h4 id=transformer-based>Transformer-based</h4><p>See
<a href=/digitalgarden/AI/Deep-learning/Transformers rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Transformers>Transformers</a></p></article><hr><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=https://carlos-gg.github.io/digitalgarden/AI/AI>Artificial Intelligence</a></li><li><a href=https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/Deep-learning>Deep Learning (DL)</a></li></ul></div></div><div id=contact_buttons><footer><p>Made by Carlos Alberto Gomez Gonzalez using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2022</p><a href=/>Home</a>
<a href=https://carlos-gg.github.io/digitalgarden/>DigitalGarden</a><a href=https://carlos-gg.github.io>CarlosGG</a></footer></div></div></body></html>