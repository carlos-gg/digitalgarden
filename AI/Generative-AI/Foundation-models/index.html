<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="A foundation model is a large artificial intelligence model trained on a vast quantity of unlabeled data at scale (usually by [[AI/Self-supervised learning]]) resulting in a model that can be adapted to a wide range of downstream tasks"><title>Foundation models</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://carlos-gg.github.io/digitalgarden//icon.png><link href=https://carlos-gg.github.io/digitalgarden/styles.b3e1e36b0403ac565c9392b3e23ef3b6.min.css rel=stylesheet><link href=https://carlos-gg.github.io/digitalgarden/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://carlos-gg.github.io/digitalgarden/js/darkmode.18b7c6dfe67ae3bf2317338cf4189144.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/util.9825137f5e7825e8553c68ce39ac9e44.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/popover.abe6a51cc7138c5dff00f151dd627ad1.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/code-title.b35124ad8db0ba37162b886afb711cbc.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/clipboard.c20857734e53a3fb733b7443879efa61.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const BASE_URL="https://carlos-gg.github.io/digitalgarden/",fetchData=Promise.all([fetch("https://carlos-gg.github.io/digitalgarden/indices/linkIndex.fcb56eed4d98466ce4e3e3c2d06d318e.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://carlos-gg.github.io/digitalgarden/indices/contentIndex.b7844059db45f34a6ef0bb478d1ff07c.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://carlos-gg.github.io/digitalgarden",!0,!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!0;drawGraph("https://carlos-gg.github.io/digitalgarden",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/carlos-gg.github.io\/digitalgarden\/js\/router.9d4974281069e9ebb189f642ae1e3ca2.min.js"
    attachSPARouting(init, render)
  </script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-S103D50NQ0"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S103D50NQ0",{anonymize_ip:!1})}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://carlos-gg.github.io/digitalgarden/js/full-text-search.24827f874defbbc6d529926cbfcfb493.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://carlos-gg.github.io/digitalgarden/>CarlosGG's Knowledge Garden ü™¥</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Foundation models</h1><p class=meta>Last updated
Nov 14, 2024
<a href=https://github.com/jackyzha0/quartz/tree/hugo/content/AI/Generative%20AI/Foundation%20models.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details open><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#resources>Resources</a><ol><li><a href=#deployment>Deployment</a></li><li><a href=#language-models>Language models</a></li><li><a href=#vision-models>Vision Models</a></li></ol></li><li><a href=#courses>Courses</a></li><li><a href=#references>References</a></li></ol></nav></details></aside><blockquote><p>A foundation model is a large artificial intelligence model trained on a vast quantity of unlabeled data at scale (usually by <a href=/digitalgarden/AI/Self-supervised-learning rel=noopener class=internal-link data-src=/digitalgarden/AI/Self-supervised-learning>AI/Self-supervised learning</a>) resulting in a model that can be adapted to a wide range of downstream tasks</p></blockquote><blockquote><p>See:</p><ul><li><a href=/digitalgarden/AI/Deep-learning/Multimodal-learning rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Multimodal-learning>AI/Deep learning/Multimodal learning</a></li><li><a href=/digitalgarden/AI/Generative-AI/LLMs rel=noopener class=internal-link data-src=/digitalgarden/AI/Generative-AI/LLMs>LLMs</a></li><li><a href=/digitalgarden/AI/Generative-AI/GenAI rel=noopener class=internal-link data-src=/digitalgarden/AI/Generative-AI/GenAI>GenAI</a></li></ul></blockquote><a href=#resources><h2 id=resources><span class=hanchor arialabel=Anchor># </span>Resources</h2></a><ul><li><a href=https://en.wikipedia.org/wiki/Foundation_models rel=noopener>https://en.wikipedia.org/wiki/Foundation_models</a></li><li><a href=https://en.wikipedia.org/wiki/Large_language_model rel=noopener>https://en.wikipedia.org/wiki/Large_language_model</a></li><li><a href=https://hai.stanford.edu/news/introducing-center-research-foundation-models-crfm rel=noopener>Center for Research on Foundation Models (CRFM)</a></li><li><a href=https://lastweekin.ai/p/multi-modal-ai rel=noopener>Foundation Models and the Future of Multi-Modal AI</a></li><li><a href=https://venturebeat.com/ai/foundation-models-2022s-ai-paradigm-shift/ rel=noopener>Foundation models: 2022‚Äôs AI paradigm shift</a></li><li><a href=https://www.artificialintelligence.news/foundation-models-paradigm-shift-for-ai-or-mere-rebranding/ rel=noopener>Foundation Models: paradigm shift for AI or mere rebranding?</a></li><li><a href=https://betterprogramming.pub/chatgpt-llms-and-foundation-models-a-closer-look-into-the-hype-and-implications-for-startups-b2f1d82f4d46 rel=noopener>ChatGPT, LLMs, and Foundation models ‚Äî a closer look into the hype and implications for startups</a></li><li><a href=https://petapixel.com/2022/08/22/ai-image-generators-compared-side-by-side-reveals-stark-differences/ rel=noopener>AI Image Generators Compared Side-By-Side Reveals Stark Differences</a></li><li><a href=https://huggingface.co/blog/rlhf rel=noopener>Illustrating Reinforcement Learning from Human Feedback (RLHF)</a><ul><li><a href="https://www.youtube.com/watch?v=PBH2nImUM5c" rel=noopener>RLHF+CHATGPT: What you must know</a></li></ul></li><li><a href="https://www.youtube.com/watch?v=veV2I-NEjaM" rel=noopener>Langchain JS | How to Use GPT-3, GPT-4 to Reference your own Data | OpenAI Embeddings Intro</a></li></ul><a href=#deployment><h3 id=deployment><span class=hanchor arialabel=Anchor># </span>Deployment</h3></a><ul><li><a href=https://github.com/fal-ai/fal rel=noopener>Fal</a> - Generative media platform for developers<ul><li><a href=https://fal.ai/docs rel=noopener>https://fal.ai/docs</a></li><li><a href=https://fal.ai/models rel=noopener>https://fal.ai/models</a></li></ul></li></ul><a href=#language-models><h3 id=language-models><span class=hanchor arialabel=Anchor># </span>Language models</h3></a><ul><li>See <a href=/digitalgarden/AI/Generative-AI/LLMs rel=noopener class=internal-link data-src=/digitalgarden/AI/Generative-AI/LLMs>LLMs</a></li></ul><a href=#vision-models><h3 id=vision-models><span class=hanchor arialabel=Anchor># </span>Vision Models</h3></a><ul><li><a href=https://www.midjourney.com/home/ rel=noopener>Midjourney</a></li><li><a href=https://openai.com/dall-e-2/ rel=noopener>DALLE-2</a></li><li><a href=https://imagen.research.google/ rel=noopener>IMAGEN</a></li><li><a href=https://imagen.research.google/video/ rel=noopener>IMAGEN video</a></li><li><a href=https://stability.ai/blog/stable-diffusion-announcement rel=noopener>Stable Diffusion</a><ul><li>#CODE
<a href=https://github.com/CompVis/stable-diffusion rel=noopener>https://github.com/CompVis/stable-diffusion</a></li><li>#CODE
<a href=https://github.com/divamgupta/stable-diffusion-tensorflow rel=noopener>https://github.com/divamgupta/stable-diffusion-tensorflow</a></li><li><a href=https://beta.dreamstudio.ai rel=noopener>DreamStudio</a></li><li><a href="https://www.youtube.com/watch?v=nVhmFski3vg" rel=noopener>Two minute papers</a></li><li><a href="https://www.youtube.com/watch?v=f6PtJKdey8E" rel=noopener>Stable Diffusion: High-Resolution Image Synthesis with Latent Diffusion Models | ML Coding Series</a></li><li><a href=https://simonwillison.net/2022/Aug/29/stable-diffusion/ rel=noopener>Stable Diffusion is a really big deal</a></li></ul></li><li><a href=https://makeavideo.studio/ rel=noopener>Make-A-Video</a></li><li><a href=https://leonardo.ai/ rel=noopener>Leonardo.AI</a></li></ul><a href=#courses><h2 id=courses><span class=hanchor arialabel=Anchor># </span>Courses</h2></a><ul><li>#COURSE
<a href=https://stanford-cs324.github.io/winter2022/ rel=noopener>Large Language Models (Stanford CS324)</a></li></ul><a href=#references><h2 id=references><span class=hanchor arialabel=Anchor># </span>References</h2></a><ul><li>#PAPER
<a href=https://arxiv.org/abs/2108.07258 rel=noopener>On the Opportunities and Risks of Foundation Models (Bommasani 2021)</a><ul><li>A foundation model is any model that is trained on broad data at scale and can be adapted (e.g., fine-tuned) to a wide range of downstream tasks; current examples include BERT, GPT-3, and CLIP</li><li>Foundation models are based on deep neural networks and self-supervised learning</li><li>On a technical level, foundation models are enabled by transfer learning and scale</li><li>The idea of transfer learning is to take the ‚Äúknowledge‚Äù learned from one task (e.g., object recognition in images) and apply it to another task (e.g., activity recognition in videos).</li><li>Within deep learning, pretraining is the dominant approach to transfer learning: a model is trained on a surrogate task (often just as a means to an end) and then adapted to the downstream task of interest via fine-tuning</li><li>Transfer learning is what makes foundation models possible, but scale is what makes them powerful. Scale required three ingredients: improvements in computer hardware, the development of the Transformer model architecture that leverages the parallelism of the hardware to train much more expressive models than before and the availability of much more training data</li></ul></li><li>#PAPER
<a href=https://arxiv.org/pdf/2111.11432 rel=noopener>Florence: A New Foundation Model for Computer Vision (Yuan 2021)</a></li><li>#PAPER
<a href=https://arxiv.org/pdf/2210.06423 rel=noopener>Foundation Transformers (Wang 2022)</a></li><li>#PAPER
<a href=https://arxiv.org/pdf/2211.05778 rel=noopener>InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions (Wang 2022)</a></li><li>#PAPER
<a href=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9163040/ rel=noopener>Towards artificial general intelligence via a multimodal foundation model (Fei 2022)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/2203.02155 rel=noopener>InstructGPT - Training language models to follow instructions with human feedback (Ouyang 2022)</a><ul><li><a href=https://openai.com/blog/chatgpt rel=noopener>https://openai.com/blog/chatgpt</a></li><li><a href=https://en.wikipedia.org/wiki/ChatGPT rel=noopener>https://en.wikipedia.org/wiki/ChatGPT</a></li><li>See GPT-3 in <a href=/digitalgarden/AI/Deep-learning/Transformers rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Transformers>AI/Deep learning/Transformers</a></li><li>ChatGPT ‚Äì a generative pre-trained transformer (GPT) ‚Äì was fine-tuned on top of GPT-3.5 using <a class="internal-link broken">Supervised learning</a> as well as <a class="internal-link broken">Reinforcement learning</a>. Both approaches used human trainers to improve the model&rsquo;s performance. In the case of supervised learning, the model was provided with conversations in which the trainers played both sides: the user and the AI assistant. In the reinforcement learning step, human trainers first ranked responses that the model had created in a previous conversation. These rankings were used to create &lsquo;reward models&rsquo; that the model was further fine-tuned on using several iterations of Proximal Policy Optimization (PPO)</li></ul></li><li>#PAPER
<a href=https://arxiv.org/pdf/2301.04655 rel=noopener>ChatGPT is not all you need. A State of the Art Review of large Generative AI models (Gozalo-Brizuela 2023)</a></li><li>#PAPER
<a href=https://arxiv.org/pdf/2302.13971 rel=noopener>LLaMA: Open and Efficient Foundation Language Models (Touvron 2023)</a><ul><li><a href=https://ai.facebook.com/blog/large-language-model-llama-meta-ai/ rel=noopener>Introducing LLaMA: A foundational, 65-billion-parameter large language model</a></li><li><a href="https://www.youtube.com/watch?v=E5OnoYF2oAk" rel=noopener>Paper explained</a></li><li>#CODE
<a href=https://github.com/cocktailpeanut/dalai rel=noopener>Dalai</a> - The simplest way to run LLaMA on your local machine</li></ul></li><li>#PAPER
<a href=https://cdn.openai.com/papers/gpt-4.pdf rel=noopener>GPT-4 Technical Report (OpenAI 2023)</a><ul><li><a href=https://openai.com/product/gpt-4 rel=noopener>https://openai.com/product/gpt-4</a></li><li><a href="https://www.youtube.com/watch?v=2zW33LfffPc&t=8s" rel=noopener>Paper explained</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/pdf/2302.10724 rel=noopener>ChatGPT: Jack of all trades, master of none (Kocon 2023)</a></li><li>#PAPER
<a href=https://www.together.xyz/blog/openchatkit rel=noopener>OpenChatKit (Together 2023)</a><ul><li><a href=https://huggingface.co/spaces/togethercomputer/OpenChatKit rel=noopener>https://huggingface.co/spaces/togethercomputer/OpenChatKit</a></li><li><a href=https://www.marktechpost.com/2023/03/12/together-releases-the-first-open-source-chatgpt-alternative-called-openchatkit/ rel=noopener>Together Releases The First Open-Source ChatGPT Alternative Called OpenChatKit</a></li></ul></li><li>#PAPER
<a href=https://crfm.stanford.edu/2023/03/13/alpaca.html rel=noopener>Alpaca: A Strong, Replicable Instruction-Following Model (Taori 2023)</a><ul><li>Fine-tuned LLaMA 7B model on 52K instruction-following demonstrations produced using OpenAI API</li><li>Performance qualitatively similar to OpenAI‚Äôs text-davinci-003, while being surprisingly small and easy/cheap to reproduce (&lt;600$)</li><li><a href=https://huggingface.co/datasets/tatsu-lab/alpaca rel=noopener>Alpaca dataset</a></li><li><a href=https://the-decoder.com/stanfords-alpaca-shows-that-openai-may-have-a-problem/ rel=noopener>https://the-decoder.com/stanfords-alpaca-shows-that-openai-may-have-a-problem/</a></li><li><a href=https://github.com/tatsu-lab/stanford_alpaca#fine-tuning rel=noopener>https://github.com/tatsu-lab/stanford_alpaca#fine-tuning</a></li><li><a href="https://www.youtube.com/watch?v=LSoqyynKU9E" rel=noopener>How to finetune your own Alpaca 7B</a></li><li><a href=https://huggingface.co/mrm8488/Alpacoom rel=noopener>https://huggingface.co/mrm8488/Alpacoom</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/pdf/2303.04671 rel=noopener>Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models (Wu 2023)</a><ul><li>Visual ChatGPT opens the door of combining ChatGPT and Visual Foundation Models and enables ChatGPT to handle complex visual tasks</li></ul></li><li>#PAPER
<a href=https://arxiv.org/pdf/2303.12712 rel=noopener>Sparks of Artificial General Intelligence: Early experiments with GPT-4 (Bubeck 2023)</a><ul><li><a href="https://www.youtube.com/watch?v=Mqg3aTGNxZ0" rel=noopener>Paper explained</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/pdf/2304.01373 rel=noopener>Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling (Biderman 2023)</a></li></ul></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/digitalgarden/AI/AI/ data-ctx="AI/Generative AI/Foundation models" data-src=/AI/AI class=internal-link>Artificial Intelligence (AI)</a></li><li><a href=/digitalgarden/AI/AI-ML-DL-for-scientific-discovery/ data-ctx="AI/Generative AI/Foundation models" data-src=/AI/AI-ML-DL-for-scientific-discovery class=internal-link>AI-ML-DL for scientific discovery</a></li><li><a href=/digitalgarden/AI/Deep-learning/Multimodal-learning/ data-ctx="AI/Generative AI/Foundation models" data-src=/AI/Deep-learning/Multimodal-learning class=internal-link>Multimodal learning</a></li><li><a href=/digitalgarden/AI/Generative-AI/Agents/ data-ctx="AI/Generative AI/Foundation models" data-src=/AI/Generative-AI/Agents class=internal-link>Agents</a></li><li><a href=/digitalgarden/AI/Generative-AI/GenAI/ data-ctx="AI/Generative AI/Foundation models" data-src=/AI/Generative-AI/GenAI class=internal-link>GenAI</a></li><li><a href=/digitalgarden/AI/Generative-AI/LLMs/ data-ctx="Foundation models" data-src=/AI/Generative-AI/LLMs class=internal-link>LLMs</a></li><li><a href=/digitalgarden/AI/NLP/ data-ctx="AI/Generative AI/Foundation models" data-src=/AI/NLP class=internal-link>Natural Language Processing (NLP)</a></li><li><a href=/digitalgarden/AI/Transfer-learning/ data-ctx="AI/Generative AI/Foundation models" data-src=/AI/Transfer-learning class=internal-link>Transfer learning</a></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://carlos-gg.github.io/digitalgarden/js/graph.abd4bc2af3869a96524d7d23b76152c7.js></script></div></div><div id=contact_buttons><footer><p>Made by Carlos Alberto Gomez Gonzalez using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, ¬© 2024</p><ul><li><a href=https://carlos-gg.github.io/digitalgarden/>Home</a></li><li><a href=https://carlos-gg.github.io>Carlos'Homepage</a></li></ul></footer></div></div></body></html>