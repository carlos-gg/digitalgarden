<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Resources  https://en.wikipedia.org/wiki/Recurrent_neural_network https://github.com/kjw0612/awesome-rnn  Recurrent Neural Networks cheatsheet  Tensorflow, DL and RNNs without a PhD http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/ http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html http://karpathy.github.io/2015/05/21/rnn-effectiveness/ http://www."><title>Recurrent Neural Networks (RNNs)</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=/icon.png><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Source+Sans+Pro:wght@400;600;700&family=Fira+Code:wght@400;700&display=swap" rel=stylesheet><link href=https://quartz.jzhao.xyz/styles.fc271f84b56ab4040866b55bab91ea59.min.css rel=stylesheet><script src=https://quartz.jzhao.xyz/js/darkmode.46b07878b7f5d9e26ad7a3c40f8a0605.min.js></script><script>const BASE_URL="https://quartz.jzhao.xyz/",fetchData=Promise.all([fetch("https://quartz.jzhao.xyz/indices/linkIndex.410017b60b8b73753cf666d266220b8e.min.json").then(a=>a.json()).then(a=>({index:a.index,links:a.links})),fetch("https://quartz.jzhao.xyz/indices/contentIndex.ca952d62e8f6347669253dd0009b381e.min.json").then(a=>a.json())]).then(([{index:a,links:b},c])=>({index:a,links:b,content:c}))</script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-XYFD95KB4J"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-XYFD95KB4J',{anonymize_ip:!1})}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script><script defer src=https://quartz.jzhao.xyz/js/search.3c0dba36397221e42c2e229db4303dc2.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://quartz.jzhao.xyz/>ðŸª´ Quartz 3.1</a></h1><svg tabindex="0" id="search-icon" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg><div class=spacer></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Recurrent Neural Networks (RNNs)</h1><p class=meta>Last updated March 17, 2022</p><ul class=tags></ul><aside class=mainTOC><h3>Table of Contents</h3><nav id=TableOfContents><ol><li><a href=#resources>Resources</a><ol><li><a href=#long-short-term-memory-lstm>Long Short-Term Memory (LSTM)</a></li><li><a href=#gated-recurrent-units-gru>Gated recurrent units (GRU)</a></li><li><a href=#reservoir-computing>Reservoir computing</a></li></ol></li><li><a href=#references>References</a><ol><li><a href=#long-short-term-memory-lstm-1>Long Short-Term Memory (LSTM)</a></li><li><a href=#reservoir-computing-1>Reservoir computing</a></li></ol></li></ol></nav></aside><h2 id=resources>Resources</h2><ul><li><a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>https://en.wikipedia.org/wiki/Recurrent_neural_network</a></li><li><a href=https://github.com/kjw0612/awesome-rnn>https://github.com/kjw0612/awesome-rnn</a></li><li><a href=https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks rel=noopener>Recurrent Neural Networks cheatsheet</a></li><li><a href="https://docs.google.com/presentation/d/e/2PACX-1vRouwj_3cYsmLrNNI3Uq5gv5-hYp_QFdeoan2GlxKgIZRSejozruAbVV0IMXBoPsINB7Jw92vJo2EAM/pub?slide=id.p" rel=noopener>Tensorflow, DL and RNNs without a PhD</a></li><li><a href=http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/>http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/</a></li><li><a href=http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html>http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html</a></li><li><a href=http://karpathy.github.io/2015/05/21/rnn-effectiveness/>http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a></li><li><a href=http://www.kdnuggets.com/2017/04/build-recurrent-neural-network-tensorflow.html>http://www.kdnuggets.com/2017/04/build-recurrent-neural-network-tensorflow.html</a></li><li><a href=https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0>https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0</a></li><li><a href=https://www.kaggle.com/thebrownviking20/intro-to-recurrent-neural-networks-lstm-gru>https://www.kaggle.com/thebrownviking20/intro-to-recurrent-neural-networks-lstm-gru</a></li><li><a href=https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21>https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21</a></li><li><a href=https://hanxiao.github.io/2018/06/24/4-Encoding-Blocks-You-Need-to-Know-Besides-LSTM-RNN-in-Tensorflow/ rel=noopener>4 Sequence Encoding Blocks You Must Know Besides RNN/LSTM in Tensorflow</a></li><li><a href=http://www.offconvex.org/2018/07/27/approximating-recurrent/ rel=noopener>When Recurrent Models Don&rsquo;t Need to be Recurrent (recurrent vs feed-forward models)</a></li></ul><h3 id=long-short-term-memory-lstm>Long Short-Term Memory (LSTM)</h3><ul><li><a href=https://en.wikipedia.org/wiki/Long_short-term_memory>https://en.wikipedia.org/wiki/Long_short-term_memory</a></li><li>One of the most innovative works in the NLP space is LSTMs and their variations e.g. GRU</li><li>With a basic RNN cell, we see a massive drop in performance when it comes to long sequences and the network needs to remember patterns which have occurred way at the beginning to infer things correctly at a current time step. And this is because of exploding and vanishing gradients.</li><li>Then came Sepp Hochreiter and JÃ¼rgen Schmidhuber and invented LSTMs, which can remember information from the way past and also selectively forget stuff that is not required.</li><li>There are several architectures of LSTM units. A common architecture is composed of a cell (the memory part of the LSTM unit) and three &ldquo;regulators&rdquo;, usually called gates, of the flow of information inside the LSTM unit: an input gate, an output gate and a forget gate. Some variations of the LSTM unit do not have one or more of these gates or maybe have other gates (for instance, GRUs do not have an output gate).</li><li>The Forget gate decides what is relevant to keep from prior steps. The input gate decides what information is relevant to add from the current step. The output gate determines what the next hidden state should be.</li><li><a href=http://colah.github.io/posts/2015-08-Understanding-LSTMs/>http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></li><li><a href=http://machinelearningmastery.com/tune-lstm-hyperparameters-keras-time-series-forecasting/>http://machinelearningmastery.com/tune-lstm-hyperparameters-keras-time-series-forecasting/</a></li><li><a href=http://machinelearningmastery.com/use-features-lstm-networks-time-series-forecasting/>http://machinelearningmastery.com/use-features-lstm-networks-time-series-forecasting/</a></li><li><a href=http://blog.echen.me/2017/05/30/exploring-lstms/>http://blog.echen.me/2017/05/30/exploring-lstms/</a></li><li><a href=https://hackernoon.com/understanding-architecture-of-lstm-cell-from-scratch-with-code-8da40f0b71f4>https://hackernoon.com/understanding-architecture-of-lstm-cell-from-scratch-with-code-8da40f0b71f4</a></li><li><a href=https://rubikscode.net/2018/03/19/understanding-long-short-term-memory-networks-lstms/>https://rubikscode.net/2018/03/19/understanding-long-short-term-memory-networks-lstms/</a></li><li><a href=https://eli.thegreenplace.net/2018/minimal-character-based-lstm-implementation/>https://eli.thegreenplace.net/2018/minimal-character-based-lstm-implementation/</a></li></ul><h3 id=gated-recurrent-units-gru>Gated recurrent units (GRU)</h3><ul><li><a href=https://en.wikipedia.org/wiki/Gated_recurrent_unit>https://en.wikipedia.org/wiki/Gated_recurrent_unit</a></li><li><a href=https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be>https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be</a></li><li>GRU (Gated Recurrent Unit) aims to solve the vanishing gradient problem which comes with a standard recurrent neural network. GRU can also be considered as a variation on the LSTM. GRUâ€™s got rid of the cell state and used the hidden state to transfer information. It also only has two gates, a reset gate and update gate.</li><li>The update gate acts similar to the forget and input gate of an LSTM. It decides what information to throw away and what new information to add.</li><li>The reset gate is another gate is used to decide how much past information to forget.</li></ul><h3 id=reservoir-computing>Reservoir computing</h3><ul><li><a href=https://en.wikipedia.org/wiki/Reservoir_computing>https://en.wikipedia.org/wiki/Reservoir_computing</a></li><li>Reservoir computing is a framework for computation derived from recurrent neural network theory that maps input signals into higher dimensional computational spaces through the dynamics of a fixed, non-linear system called a reservoir. After the input signal is fed into the reservoir, which is treated as a &ldquo;black box,&rdquo; a simple readout mechanism is trained to read the state of the reservoir and map it to the desired output.</li></ul><h4 id=echo-state-networks-esn>Echo state networks (ESN)</h4><ul><li><a href=https://en.wikipedia.org/wiki/Echo_state_network>https://en.wikipedia.org/wiki/Echo_state_network</a></li><li>The ESN is a recurrent neural network with a sparsely connected hidden layer (with typically 1% connectivity)</li></ul><h2 id=references>References</h2><ul><li>#PAPER
<a href=http://arxiv.org/abs/1410.5401 rel=noopener>Neural Turing Machines (Graves 2014)</a></li><li>#PAPER
<a href=http://distill.pub/2016/augmented-rnns/ rel=noopener>Attention and Augmented Recurrent Neural Networks (Olah 2016)</a></li><li>#PAPER
<a href=https://eng.uber.com/neural-networks/ rel=noopener>Engineering Extreme Event Forecasting at Uber with Recurrent Neural Networks (Laptev 2017)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1709.01907 rel=noopener>Deep and Confident Prediction for Time Series at Uber (Zhu 2017)</a><ul><li><a href=https://eng.uber.com/neural-networks-uncertainty-estimation/>https://eng.uber.com/neural-networks-uncertainty-estimation/</a></li><li>introduced a new end-to-end Bayesian neural network (BNN) architecture that more accurately forecasts time series predictions and uncertainty estimations at scale</li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1409.0473 rel=noopener>Neural Machine Translation by Jointly Learning to Align and Translate (Bahdanau 2016)</a><ul><li><a href=https://medium.com/datadriveninvestor/attention-in-rnns-321fbcd64f05>https://medium.com/datadriveninvestor/attention-in-rnns-321fbcd64f05</a></li></ul></li></ul><h3 id=long-short-term-memory-lstm-1>Long Short-Term Memory (LSTM)</h3><ul><li>#PAPER
<a href=https://dl.acm.org/doi/10.1162/neco.1997.9.8.1735 rel=noopener>Long Short-Term Memory (Hochreiter 1997)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1903.08023 rel=noopener>IndyLSTMs: Independently Recurrent LSTMs (Gonnet 2019)</a><ul><li>Independently Recurrent Long Short-term Memory cells (IndyLSTMs) differ from regular LSTM cells in that the recurrent weights are not modeled as a full matrix, but as a diagonal matrix, i.e.\ the output and state of each LSTM cell depends on the inputs and its own output/state, as opposed to the input and the outputs/states of all the cells in the layer. The number of parameters per IndyLSTM layer, and thus the number of FLOPS per evaluation, is linear in the number of nodes in the layer, as opposed to quadratic for regular LSTM layers, resulting in potentially both smaller and faster models. IndyLSTMs, despite their smaller size, consistently outperform regular LSTMs both in terms of accuracy per parameter, and in best accuracy overall. We attribute this improved performance to the IndyLSTMs being less prone to overfitting.</li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1909.09586 rel=noopener>Understanding LSTM &ndash; a tutorial into Long Short-Term Memory Recurrent Neural Networks (Staudemeyer 2019)</a></li></ul><h3 id=reservoir-computing-1>Reservoir computing</h3><ul><li>#PAPER
<a href=https://www.nature.com/articles/s41467-021-25801-2 rel=noopener>Next generation reservoir computing (Gauthier 2021)</a></li></ul><h4 id=echo-state-networks-esn-1>Echo state networks (ESN)</h4><ul><li><p><a href=https://en.wikipedia.org/wiki/Echo_state_network>https://en.wikipedia.org/wiki/Echo_state_network</a></p></li><li><p>The ESN is a recurrent neural network with a sparsely connected hidden layer (with typically 1% connectivity)</p></li><li><p>#PAPER
<a href=https://pubmed.ncbi.nlm.nih.gov/15064413/ rel=noopener>Harnessing nonlinearity: predicting chaotic systems and saving energy in wireless communication (Jaeger 2004)</a></p><ul><li>#CODE <a href=https://github.com/cknd/pyESN>https://github.com/cknd/pyESN</a></li></ul></li><li><p>#PAPER
<a href=https://www.sciencedirect.com/science/article/pii/S0893608018302223 rel=noopener>Design of deep echo state networks (Gallicchio 2018)</a></p><ul><li>#CODE <a href=https://github.com/lucapedrelli/DeepESN>https://github.com/lucapedrelli/DeepESN</a></li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/abs/1710.07313 rel=noopener>Using Machine Learning to Replicate Chaotic Attractors and Calculate Lyapunov Exponents from Data (Pathak 2017)</a></p></li><li><p>#PAPER
<a href=https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.120.024102 rel=noopener>Model-Free Prediction of Large Spatiotemporally Chaotic Systems from Data: A Reservoir Computing Approach (Pathak 2018)</a></p></li><li><p>#PAPER
<a href=https://www.mdpi.com/1996-1073/11/3/526/htm rel=noopener>Wind Power Forecasting Based on Echo State Networks and Long Short-Term Memory (Lopez 2018)</a></p><ul><li>ESN + LSTM</li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/abs/1812.11527 rel=noopener>Comparison between DeepESNs and gatedRNNs on multivariate time-series prediction (Gallicchio 2019)</a></p></li><li><p>#PAPER
<a href=https://arxiv.org/abs/1812.11527 rel=noopener>Deep Echo State Network (DeepESN): A Brief Survey (Gallicchio 2020)</a></p></li><li><p>#PAPER
<a href=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7297597/#CR12 rel=noopener>Comparison of Recurrent Neural Networks for Wind Power Forecasting (Lopez 2020)</a></p><ul><li>ESN + LSTM</li></ul></li></ul></article><hr><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=https://quartz.jzhao.xyz//AI/Computer-Vision/Video-segmentation-and-prediction>Untitled Page</a></li><li><a href=https://quartz.jzhao.xyz//AI/Deep-learning/Deep-learning>Deep Learning (DL)</a></li><li><a href=https://quartz.jzhao.xyz//AI/Forecasting>Forecasting</a></li><li><a href=https://quartz.jzhao.xyz//AI/NLP>Natural Language Processing (NLP)</a></li><li><a href=https://quartz.jzhao.xyz//AI/Supervised-Learning/Regression>Regression</a></li><li><a href=https://quartz.jzhao.xyz//AI/Time-Series-analysis>Time series analysis</a></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://quartz.jzhao.xyz/js/graph.8aaeaccdaf2f67fa56e0d3e2d059ed5e.js></script><script>drawGraph("https://quartz.jzhao.xyz/AI/Deep-learning/RNNs","https://quartz.jzhao.xyz",[{"/moc":"#4388cc"}],-1,!0,!1,!0)</script></div></div><div id=contact_buttons><footer><p>Made by Jacky Zhao using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2022</p><a href=/>Home</a>
<a href=https://twitter.com/_jzhao>Twitter</a><a href=https://github.com/jackyzha0>Github</a></footer></div><script src=https://quartz.jzhao.xyz/js/popover.718bae159805a590cda4e9bcb14b9dcc.min.js></script><script>initPopover("https://quartz.jzhao.xyz")</script></div></body></html>