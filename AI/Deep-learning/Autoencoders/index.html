<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Resources  https://en.wikipedia.org/wiki/Autoencoder  Dimensionality Reduction The classical approach for unsupervised learning using neural networks. The basic version consists of a Multilayer Perceptron (MLP) where the input and output layer have the same size and a smaller hidden layer is trained to recover the input."><title>Autoencoders</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=/icon.png><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Source+Sans+Pro:wght@400;600;700&family=Fira+Code:wght@400;700&display=swap" rel=stylesheet><link href=https://carlos-gg.github.io/digitalgarden/styles.fc271f84b56ab4040866b55bab91ea59.min.css rel=stylesheet><script src=https://carlos-gg.github.io/digitalgarden/js/darkmode.46b07878b7f5d9e26ad7a3c40f8a0605.min.js></script><script>const BASE_URL="https://carlos-gg.github.io/digitalgarden/",fetchData=Promise.all([fetch("https://carlos-gg.github.io/digitalgarden/indices/linkIndex.238440fbb1e0efe7bd3997d9556c3b1e.min.json").then(a=>a.json()).then(a=>({index:a.index,links:a.links})),fetch("https://carlos-gg.github.io/digitalgarden/indices/contentIndex.42540bffa44359d7a230315e6970d9d3.min.json").then(a=>a.json())]).then(([{index:a,links:b},c])=>({index:a,links:b,content:c}))</script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-S103D50NQ0"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-S103D50NQ0',{anonymize_ip:!1})}</script><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://carlos-gg.github.io/digitalgarden/js/graph.8aaeaccdaf2f67fa56e0d3e2d059ed5e.js></script><script>drawGraph("https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/Autoencoders","https://carlos-gg.github.io/digitalgarden",[{"/moc":"#4388cc"}],-1,!0,!1,!0)</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script><script defer src=https://carlos-gg.github.io/digitalgarden/js/search.3c0dba36397221e42c2e229db4303dc2.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://carlos-gg.github.io/digitalgarden/>CarlosGG's Knowledge Garden ðŸª´</a></h1><svg tabindex="0" id="search-icon" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg><div class=spacer></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Autoencoders</h1><p class=meta>Last updated March 17, 2022</p><ul class=tags></ul><aside class=mainTOC><h3>Table of Contents</h3><nav id=TableOfContents><ol><li><a href=#resources>Resources</a><ol><li><a href=#vaes>VAEs</a></li></ol></li><li><a href=#references>References</a><ol><li><a href=#vaes-1>VAEs</a></li></ol></li></ol></nav></aside><h2 id=resources>Resources</h2><ul><li><a href=https://en.wikipedia.org/wiki/Autoencoder>https://en.wikipedia.org/wiki/Autoencoder</a></li><li><a href=https://www.cs.toronto.edu/~hinton/science.pdf rel=noopener>Dimensionality Reduction</a></li><li>The classical approach for unsupervised learning using neural networks. The basic version consists of a Multilayer Perceptron (MLP) where the input and output layer have the same size and a smaller hidden layer is trained to recover the input. Once trained, the output from the hidden layer corresponds to data representation that can be useful for clustering, dimensionality reduction, improving supervised classification and even for data compression.</li><li><a href=https://blog.keras.io/building-autoencoders-in-keras.html>https://blog.keras.io/building-autoencoders-in-keras.html</a></li><li><a href=https://blog.insightdatascience.com/isee-removing-eyeglasses-from-faces-using-deep-learning-d4e7d935376f>https://blog.insightdatascience.com/isee-removing-eyeglasses-from-faces-using-deep-learning-d4e7d935376f</a></li><li><a href=https://github.com/nanopony/keras-convautoencoder>https://github.com/nanopony/keras-convautoencoder</a></li></ul><h3 id=vaes>VAEs</h3><ul><li>Variational autoencoders are generative models. Traditional autoencoders that just do reconstruction donâ€™t have an obvious generative interpretation. There are some cases in between, like denoising autoencoders, where it is possible to construct a Markov chain that uses the autoencoder to sample from the data distribution, but the autoencoder doesnâ€™t give direct explicit access to an estimate of the density or the ability to sample directly.</li><li>VAE is a type of autoencoder with added constraints on the encoded representations being learned. More precisely, it is an autoencoder that learns a latent variable model for its input data. So instead of letting your neural network learn an arbitrary function, you are learning the parameters of a probability distribution modeling your data. If you sample points from this distribution, you can generate new input data samples: a VAE is a &ldquo;generative model&rdquo;.</li><li><a href=https://jmtomczak.github.io/blog/4/4_VAE.html rel=noopener>Introduction to deep generative modeling: Variational Auto-Encoders</a></li><li><a href=https://jmtomczak.github.io/blog/7/7_priors.html rel=noopener>Introduction to deep generative modeling: Priors in VAEs</a></li><li><a href=https://jmtomczak.github.io/blog/9/9_hierarchical_lvm_p1.html rel=noopener>Introduction to deep generative modeling: Hierarchical VAEs</a></li><li><a href=http://kvfrans.com/variational-autoencoders-explained/ rel=noopener>Variational Autoencoders Explained</a></li><li><a href=https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf rel=noopener>Intuitively understanding VAEs</a></li><li><a href=https://thilospinner.com/towards-an-interpretable-latent-space/ rel=noopener>An Intuitive Comparison of Autoencoders with Variational Autoencoders</a></li><li><a href=http://blog.fastforwardlabs.com/post/148842796218/introducing-variational-autoencoders-in-prose-and>http://blog.fastforwardlabs.com/post/148842796218/introducing-variational-autoencoders-in-prose-and</a></li><li><a href="https://www.youtube.com/watch?v=9zKuYvjFFS8" rel=noopener>Arxiv insights. Variational Autoencoders</a></li><li><a href=https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html rel=noopener>From Autoencoder to Beta-VAE</a></li></ul><h2 id=references>References</h2><ul><li>#PAPER
<a href=https://www.jmlr.org/papers/v11/vincent10a.html rel=noopener>Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion (Vincent 2010)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1812.04314 rel=noopener>Adversarial Autoencoders with Constant-Curvature Latent Manifolds (Grattarola 2018)</a></li><li>#PAPER
<a href=https://www.mdpi.com/2076-3417/9/22/4780/htm rel=noopener>Image-To-Image Translation Using a Cross-Domain Auto-Encoder and Decoder (Yoo 2019)</a><ul><li>Early image-to-image translation methods used convolutional neural networks (CNN), which learn to minimize the loss of a pixel value between the source domain image and the target domain image but had the limitation of failing to produce more photorealistic images</li><li>Unlike other approachesâ€¦ our method is not limited to a specific task, nor do we rely on predefined relationships between the source and target domains. Our method can be applied to make a general-domain solution for many image-to-image translation tasks.</li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2111.06377 rel=noopener>Masked Autoencoders Are Scalable Vision Learners (He 2021)</a><ul><li>#CODE <a href=https://github.com/facebookresearch/mae>https://github.com/facebookresearch/mae</a></li><li>#CODE <a href=https://github.com/ariG23498/mae-scalable-vision-learners>https://github.com/ariG23498/mae-scalable-vision-learners</a></li><li><a href=https://keras.io/examples/vision/masked_image_modeling/>https://keras.io/examples/vision/masked_image_modeling/</a></li><li>Masked autoencoder (MAE) is a simple autoencoding approach that reconstructs the original signal given its partial observation</li></ul></li></ul><h3 id=vaes-1>VAEs</h3><ul><li>#PAPER
<a href=https://arxiv.org/abs/1312.6114 rel=noopener>Auto-Encoding Variational Bayes (Kingma 2014)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1906.02691 rel=noopener>An Introduction to Variational Autoencoders (Kingma 2019)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/2007.03898 rel=noopener>NVAE: A Deep Hierarchical Variational Autoencoder (Vahdat 2020)</a><ul><li><a href="https://www.youtube.com/watch?v=x6T1zMSE4Ts" rel=noopener>Paper explained</a></li></ul></li></ul></article><hr><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=https://carlos-gg.github.io/digitalgarden//AI/Deep-learning/Deep-learning>Deep Learning (DL)</a></li><li><a href=https://carlos-gg.github.io/digitalgarden//AI/Deep-learning/Encoder-decoder-networks>Encoder-decoder networks</a></li><li><a href=https://carlos-gg.github.io/digitalgarden//AI/Deep-learning/Generative-modelling>Generative modeling</a></li></ul></div></div><div id=contact_buttons><footer><p>Made by Carlos Alberto Gomez Gonzalez using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2022</p><a href=https://carlos-gg.github.io/digitalgarden/>Root</a><a href=https://carlos-gg.github.io>CarlosGG</a></footer></div><script src=https://carlos-gg.github.io/digitalgarden/js/popover.718bae159805a590cda4e9bcb14b9dcc.min.js></script><script>initPopover("https://carlos-gg.github.io/digitalgarden")</script></div></body></html>