<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="A GAN consists of two networks; a generator (G) and a discriminator (D), given a set of training examples, G will generate outputs and D will classify them as either being from the same distribution as the training examples or not."><title>Generative Adversarial Networks (GANs)</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=/icon.png><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Source+Sans+Pro:wght@400;600;700&family=Fira+Code:wght@400;700&display=swap" rel=stylesheet><link href=https://carlos-gg.github.io/digitalgarden/styles.cd61336c89ed6e03702366ce4a492b75.min.css rel=stylesheet><script src=https://carlos-gg.github.io/digitalgarden/js/darkmode.46b07878b7f5d9e26ad7a3c40f8a0605.min.js></script>
<script>const BASE_URL="https://carlos-gg.github.io/digitalgarden/",fetchData=Promise.all([fetch("https://carlos-gg.github.io/digitalgarden/indices/linkIndex.245c4ea85f5863f9859f3efe24c1fc8b.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://carlos-gg.github.io/digitalgarden/indices/contentIndex.1a2ae984dedec26a398ab120cdfde996.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n}))</script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-S103D50NQ0"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S103D50NQ0",{anonymize_ip:!1})}</script><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://carlos-gg.github.io/digitalgarden/js/graph.27e8521c25c27c79dea35f434c486167.js></script>
<script>drawGraph("https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/GANs","https://carlos-gg.github.io/digitalgarden",[{"/moc":"#4388cc"}],-1,!0,!1,!0)</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://carlos-gg.github.io/digitalgarden/js/search.bc849b857f2c1b822264d40635bb67b6.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://carlos-gg.github.io/digitalgarden/>CarlosGG's Knowledge Garden ü™¥</a></h1><svg tabindex="0" id="search-icon" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg><div class=spacer></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Generative Adversarial Networks (GANs)</h1><p class=meta>Last updated July 29, 2022</p><ul class=tags></ul><aside class=mainTOC><details open><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#resources>Resources</a></li><li><a href=#courses>Courses</a></li><li><a href=#talks>Talks</a></li><li><a href=#code>Code</a></li><li><a href=#references>References</a></li><li><a href=#subtopics>Subtopics</a><ol><li><a href=#gans-for-super-resolution>GANs for super-resolution</a></li><li><a href=#gans-for-missing-data-imputation-and-inpainting>GANs for missing data, imputation and inpainting</a></li><li><a href=#image-to-image-translation-conditional-gans>Image-to-image translation. Conditional GANs</a></li><li><a href=#gans-for-spatio-temporal-data-generation>GANs for spatio-temporal data generation</a></li><li><a href=#gans-for-representation-learning-and-image-synthesis>GANs for representation learning and image synthesis</a></li><li><a href=#semi-supervised-gans>Semi-supervised GANs</a></li><li><a href=#fewone-shot-learning-gans>Few/one-shot learning GANs</a></li><li><a href=#gans-for-anomaly-detection>GANs for anomaly detection</a></li></ol></li></ol></nav></details></aside><blockquote><p>A GAN consists of two networks; a generator (G) and a discriminator (D), given a set of training examples, G will generate outputs and D will classify them as either being from the same distribution as the training examples or not. In doing so D is optimized so as to be able to discriminate between examples from the training example and from the generator network which in turn is optimized to fool D into classifying its output as being drawn from the training examples. After such training G can now generate samples with properties very similar to those of the training examples. GANs tend to be devilishly hard to train</p></blockquote><blockquote><p>See <a href=/digitalgarden/AI/Deep-learning/Generative-modelling rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Generative-modelling>AI/Deep learning/Generative modelling</a></p></blockquote><h2 id=resources>Resources</h2><ul><li><a href=https://github.com/pshams55/GAN-Case-Study rel=noopener>List of papers and other on Generative Adversarial Networks</a></li><li><a href=https://spectra.pub/ml/gans rel=noopener>Generative Adversarial Networks</a></li><li><a href=https://deepgenerativemodels.github.io/notes/gan/ rel=noopener>Generative adversarial networks</a></li><li><a href=https://jmtomczak.github.io/blog/12/12_gans.html rel=noopener>Introduction to deep generative modeling: Generative Adversarial Networks (GANs)</a></li><li><a href=https://www.oreilly.com/learning/generative-adversarial-networks-for-beginners rel=noopener>Generative adversarial networks for beginners</a></li><li><a href=https://tryolabs.com/blog/2016/12/06/major-advancements-deep-learning-2016/ rel=noopener>Intuitive explanation of GANs. Subtypes</a></li><li><a href=https://adeshpande3.github.io/adeshpande3.github.io/Deep-Learning-Research-Review-Week-1-Generative-Adversarial-Nets rel=noopener>https://adeshpande3.github.io/adeshpande3.github.io/Deep-Learning-Research-Review-Week-1-Generative-Adversarial-Nets</a></li><li><a href=http://www.openias.org/hybrid-generative-discriminative rel=noopener>http://www.openias.org/hybrid-generative-discriminative</a></li><li><a href=http://edwardlib.org/tutorials/gan rel=noopener>http://edwardlib.org/tutorials/gan</a></li><li><a href=https://poloclub.github.io/ganlab/ rel=noopener>Play with GANs in your browser</a></li><li><a href=http://www.offconvex.org/2017/07/06/GANs3/ rel=noopener>Do GANs actually do distribution learning?</a></li><li><a href=https://deephunt.in/the-gan-zoo-79597dc8c347 rel=noopener>The GAN Zoo - A list of all named GANs!</a></li><li><a href=https://beyondminds.ai/advances-in-generative-adversarial-networks-gans/ rel=noopener>Advances in Generative Adversarial Networks</a> ^advancesingans<ul><li>Drawbacks of using GANs: Mode collapse, Convergence, Quality evaluation, Metrics</li><li>Techniques for Improving Performance:<ul><li>Alternative Loss Functions: One of the most popular fixes to the shortcomings of GANs is the Wasserstein GAN. It essentially replaces the Jensen Shannon divergence of conventional GANs with the Earth Mover distance (Wasserstein-1 distance or EM distance)</li><li>Two Timescale Update Rule (TTUR): In this method, we use a different learning rate for the discriminator and the generator. Typically, a slower update rule is used for the generator and a faster update rule is used for the discriminator</li><li>Gradient Penalty: In the paper Wasserstein GAN GP, a simple gradient penalty was introduced which is added to the loss function to avoid exploding vanishing gradients and optimization issues (caused by weight clipping)</li><li>Spectral Normalization: weight normalization technique that is typically used on the Discriminator to enhance the training process</li><li><a href=http://aiden.nibali.org/blog/2017-01-18-mode-collapse-gans/ rel=noopener>Unrolling and Packing</a></li><li>Stacking GANs: use multiple GANs placed consecutively, where each GAN solves an easier version of the problem. For instance, FashionGAN used two GANs to perform localized image translation. Progressive GANs (ProGANs) can generate high quality images of excellent resolution.</li><li>Relativistic GANs: Conventional GANs measure the probability of the generated data being real. Relativistic GANs measure the probability of the generated data being ‚Äúmore realistic‚Äù than the real data. We can measure this ‚Äúrelative realism‚Äù using an appropriate distance measure, as mentioned in the RGAN paper</li><li>Self Attention Mechanism (SAGAN): The authors of Self Attention GANs claim that convolutions used for generating images look at information that are spread locally. That is, they miss out on relationships that span globally due to their restrictive receptive field. Self-Attention Generative Adversarial Network allows attention-driven, long-range dependency modeling for image generation tasks.</li><li>Miscellaneous Techniques: Feature Matching, Mini Batch Discrimination, Historical Averaging, One-sided Label Smoothing, Virtual Batch Normalization.</li></ul></li></ul></li></ul><h2 id=courses>Courses</h2><ul><li>#COURSE
<a href="https://www.youtube.com/watch?v=wFsI2WqUfdA&t=850s" rel=noopener>Generative Adversarial Networks ( DeepMind x UCL | Deep Learning Lectures | 9/12)</a></li></ul><h2 id=talks>Talks</h2><ul><li>#TALK
<a href="https://www.youtube.com/watch?v=9d4jmPmTWmc" rel=noopener>GANs for Good - A Virtual Expert Panel by DeepLearning.AI</a></li><li>#TALK
<a href="https://www.youtube.com/watch?v=8L11aMN5KY8" rel=noopener>A Friendly Introduction to Generative Adversarial Networks (GANs)</a></li></ul><h2 id=code>Code</h2><ul><li>#CODE
<a href=https://github.com/eriklindernoren/Keras-GAN rel=noopener>Keras-GAN - Collection of Keras implementations of GANs</a></li><li>#CODE
<a href=https://github.com/eriklindernoren/PyTorch-GAN rel=noopener>Pytorch-GAN - Collection of Pytorch implementations of GANs</a></li><li>#CODE
<a href=https://github.com/wiseodd/generative-models rel=noopener>Generative models in Tensorflow and Pytorch</a></li><li>#CODE
<a href=https://github.com/hwalsuklee/tensorflow-generative-model-collections rel=noopener>Tensorflow generative model collection</a></li><li>#CODE
<a href=https://github.com/ydataai/ydata-synthetic rel=noopener>ydata-synthetic</a><ul><li>This repository contains material related with GANs for synthetic data generation, in particular regular tabular data and time-series</li></ul></li></ul><h2 id=references>References</h2><p>Review papers:</p><ul><li><p>#PAPER
<a href=https://arxiv.org/abs/2006.05132 rel=noopener>A Survey on Generative Adversarial Networks: Variants, Applications,and Training (Jabbar 2020)</a></p></li><li><p>#PAPER
<a href=https://arxiv.org/abs/2001.06937 rel=noopener>A Review on Generative Adversarial Networks: Algorithms, Theory, and Applications (Gui 2020)</a></p></li><li><p>#PAPER
<a href=http://arxiv.org/abs/1406.2661 rel=noopener>Generative Adversarial Networks (Goodfellow 2014)</a></p><ul><li><a href="https://www.youtube.com/watch?v=eyxmSmjmNS0" rel=noopener>Paper explained</a></li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/abs/1605.05396 rel=noopener>GAN to convert text descriptions into images (Reed 2016)</a></p></li><li><p>#PAPER
<a href=https://arxiv.orga/abs/1511.06434v2 rel=noopener>Unsupervised representation learning with GANs (Radford 2016)</a></p><ul><li>Although GANs were already introduced in 2014 by Ian Goodfellow, it wasn&rsquo;t until the publication of this paper detailing a deep convolutional architecture (DCGAN) that GANs really took off</li><li><a href=https://www.tensorflow.org/tutorials/generative/dcgan rel=noopener>https://www.tensorflow.org/tutorials/generative/dcgan</a></li><li><a href=https://towardsdatascience.com/dcgans-deep-convolutional-generative-adversarial-networks-c7f392c2c8f8 rel=noopener>https://towardsdatascience.com/dcgans-deep-convolutional-generative-adversarial-networks-c7f392c2c8f8</a></li><li>#CODE
<a href=https://github.com/tensorflow/models/blob/master/research/slim/nets/dcgan.py rel=noopener>https://github.com/tensorflow/models/blob/master/research/slim/nets/dcgan.py</a></li></ul></li><li><p>#PAPER
<a href=https://distill.pub/2016/deconv-checkerboard/ rel=noopener>Deconvolution and Checkerboard Artifacts (Odena 2016)</a></p></li><li><p>#PAPER
<a href=https://arxiv.org/abs/1606.03657 rel=noopener>InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets (Chen 2016)</a></p><ul><li><a href=https://wiseodd.github.io/techblog/2017/01/29/infogan/ rel=noopener>https://wiseodd.github.io/techblog/2017/01/29/infogan/</a></li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/abs/1707.02937 rel=noopener>Checkerboard artifact free sub-pixel convolution: A note on sub-pixel convolution, resize convolution and convolution resize (Aitken 2017)</a></p></li><li><p>#PAPER
<a href=https://arxiv.org/abs/1701.07875 rel=noopener>Wasserstein GAN (Arjovsky 2017)</a></p><ul><li><a href=https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html#wasserstein-gan-wgan rel=noopener>From GAN to Wasserstein GAN</a></li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/abs/1704.00028 rel=noopener>Improved Training of Wasserstein GANs (Gulrajani 2017)</a> ^wgangp</p></li><li><p>#PAPER
<a href=https://arxiv.org/abs/1705.09558 rel=noopener>Bayesian GAN (Saatchi 2017)</a></p><ul><li>#CODE
<a href=https://github.com/andrewgordonwilson/bayesgan rel=noopener>https://github.com/andrewgordonwilson/bayesgan</a></li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/abs/1702.07392 rel=noopener>WaterGAN: Unsupervised Generative Network to Enable Real-time Color Correction of Monocular Underwater Images (Li 2017)</a></p></li><li><p>#PAPER
<a href=https://arxiv.org/abs/1812.04948 rel=noopener>A Style-Based Generator Architecture for Generative Adversarial Networks, StyleGAN (Karras 2018)</a></p><ul><li>#TALK
<a href=https://youtu.be/kSLJriaOumA rel=noopener>https://youtu.be/kSLJriaOumA</a></li><li>#CODE
<a href=https://github.com/NVlabs/stylegan rel=noopener>https://github.com/NVlabs/stylegan</a></li><li><a href=https://github.com/NVlabs/ffhq-dataset rel=noopener>FFHQ</a></li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/abs/1807.00734 rel=noopener>The relativistic discriminator: a key element missing from standard GAN (Jolicoeur-Martineau 2018)</a> ^190c58</p></li><li><p>#PAPER
<a href=https://arxiv.org/abs/1904.08994 rel=noopener>From GAN to WGAN (Wenb 2019)</a></p></li><li><p>#PAPER
<a href=https://arxiv.org/abs/1904.11419 rel=noopener>Time Series Simulation by Conditional Generative Adversarial Net (Fu 2019)</a></p></li><li><p>#PAPER
<a href=https://arxiv.org/abs/1904.01326 rel=noopener>HoloGAN: Unsupervised learning of 3D representations from natural images (Nguyen-Phuoc 2019)</a></p><ul><li><a href=https://www.monkeyoverflow.com/#/hologan-unsupervised-learning-of-3d-representations-from-natural-images/ rel=noopener>https://www.monkeyoverflow.com/#/hologan-unsupervised-learning-of-3d-representations-from-natural-images/</a></li><li>#TALK
<a href="https://www.youtube.com/watch?v=z2DnFOQNECM" rel=noopener>https://www.youtube.com/watch?v=z2DnFOQNECM</a></li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/abs/1910.05852 rel=noopener>Implicit competitive regularization in GANs (Schafer 2020)</a></p></li><li><p>#PAPER
<a href=https://arxiv.org/abs/2006.06676 rel=noopener>Training Generative Adversarial Networks with Limited Data (Karras 2020)</a></p></li><li><p>#PAPER
<a href=https://arxiv.org/abs/2103.03076 rel=noopener>Gradient-Guided Dynamic Efficient Adversarial Training (Waag 2021)</a></p><ul><li>#CODE
<a href=https://github.com/locuslab/fast_adversarial rel=noopener>https://github.com/locuslab/fast_adversarial</a></li><li>The goal of DEAT is to improve adversarial training while maintaining effectiveness. It begins by training one batch replay and gradually increases it during training</li><li>This method reduces large amount of computation when doing backpropagation and consequently achieves a more efficient training paradigm</li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/abs/2009.08454 rel=noopener>ExGAN: Adversarial Generation of Extreme Samples (Bathia 2021)</a></p><ul><li>#CODE
<a href=https://github.com/Stream-AD/ExGAN rel=noopener>https://github.com/Stream-AD/ExGAN</a></li><li>Existing approaches based on GANs excel at generating realistic samples, but seek to generate typical samples, rather than extreme samples</li><li>ExGAN is a GAN-based approach to generate realistic and extreme samples. To model the extremes of the training distribution in a principled way, our work draws from Extreme Value Theory (EVT), a probabilistic approach for modelling the extreme tails of distributions</li></ul></li></ul><h2 id=subtopics>Subtopics</h2><h3 id=gans-for-super-resolution>GANs for super-resolution</h3><p>See &ldquo;GAN-based&rdquo; section in <a href=/digitalgarden/AI/Computer-Vision/Super-resolution rel=noopener class=internal-link data-src=/digitalgarden/AI/Computer-Vision/Super-resolution>AI/Computer Vision/Super-resolution</a></p><h3 id=gans-for-missing-data-imputation-and-inpainting>GANs for missing data, imputation and inpainting</h3><p>See &ldquo;GAN-based&rdquo; section in <a href=/digitalgarden/AI/Computer-Vision/Inpainting-and-restoration rel=noopener class=internal-link data-src=/digitalgarden/AI/Computer-Vision/Inpainting-and-restoration>AI/Computer Vision/Inpainting and restoration</a></p><h3 id=image-to-image-translation-conditional-gans>Image-to-image translation. Conditional GANs</h3><p>See &ldquo;GAN-based&rdquo; section in <a href=/digitalgarden/AI/Computer-Vision/Image-to-image-translation rel=noopener class=internal-link data-src=/digitalgarden/AI/Computer-Vision/Image-to-image-translation>AI/Computer Vision/Image-to-image translation</a></p><h3 id=gans-for-spatio-temporal-data-generation>GANs for spatio-temporal data generation</h3><ul><li>#PAPER
<a href=https://arxiv.org/abs/2006.08571 rel=noopener>COT-GAN: Generating Sequential Data via Causal Optimal Transport (Xu 2020)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/2109.15044# rel=noopener>SPATE-GAN: Improved Generative Modeling of Dynamic Spatio-Temporal Patterns with an Autoregressive Embedding Loss (Klemmer 2021)</a> ^spate-gan<ul><li>#CODE
<a href=https://github.com/konstantinklemmer/spate-gan rel=noopener>https://github.com/konstantinklemmer/spate-gan</a></li></ul></li></ul><h3 id=gans-for-representation-learning-and-image-synthesis>GANs for representation learning and image synthesis</h3><ul><li>#PAPER
<a href=https://arxiv.org/abs/1506.05751 rel=noopener>Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks, Laplacian GAN (Denton 2015)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1605.09782 rel=noopener>Adversarial feature learning, BiGAN (Donahue 2017)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1907.02544 rel=noopener>Large Scale Adversarial Representation Learning, BigBiGAN (Donahue 2019)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1809.11096 rel=noopener>Large Scale GAN Training for High Fidelity Natural Image Synthesis, BigGAN (Brock 2019)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1805.08318 rel=noopener>Self-Attention GANs, SAGAN (Zhang 2019)</a> ^sagan<ul><li>#CODE
<a href=https://github.com/brain-research/self-attention-gan rel=noopener>https://github.com/brain-research/self-attention-gan</a></li></ul></li><li>#PAPER
<a href=https://genforce.github.io/idinvert/ rel=noopener>In-domain GAN Inversion for Real Image Editing (Zhu 2020)</a><ul><li><a href="https://www.youtube.com/watch?v=2qMw8sOsNg0" rel=noopener>Paper explained</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2006.09965 rel=noopener>High-Fidelity Generative Image Compression (Mentzer 2020)</a><ul><li><a href=https://hific.github.io/ rel=noopener>https://hific.github.io/</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2008.02793 rel=noopener>Generative Adversarial Networks for Image and Video Synthesis: Algorithms and Applications (Liu 2020)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/2012.13736 rel=noopener>Image Synthesis with Adversarial Networks: a Comprehensive Survey and Case Studies (Shamsolmoali 2020)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/2101.04702 rel=noopener>Cross-Modal Contrastive Learning for Text-to-Image Generation (Zhang 2021)</a><ul><li><a href=https://ai.googleblog.com/2021/05/cross-modal-contrastive-learning-for.html rel=noopener>https://ai.googleblog.com/2021/05/cross-modal-contrastive-learning-for.html</a></li><li>text-to-image generation by learning to maximize the mutual information between image and text using inter-modal (image-text) and intra-modal (image-image) contrastive losses</li></ul></li><li>#PAPER
<a href=https://link.springer.com/article/10.1007/s00138-020-01164-4 rel=noopener>TriGAN: image-to-image translation for multi-source domain adaptation (Roy 2021)</a><ul><li>approach for multi-source domain adaptation (MSDA) based on generative adversarial networks</li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2108.02774 rel=noopener>Sketch Your Own GAN (Wang 2021)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/2109.05070 rel=noopener>Instance-Conditioned GAN (Casanova 2021)</a><ul><li>#CODE
<a href="https://paperswithcode.com/paper/instance-conditioned-gan?from=n17" rel=noopener>https://paperswithcode.com/paper/instance-conditioned-gan?from=n17</a></li></ul></li></ul><h3 id=semi-supervised-gans>Semi-supervised GANs</h3><ul><li>#PAPER
<a href=https://arxiv.org/abs/1606.03498 rel=noopener>Improved Techniques for Training GANs (Saliman 2016)</a> ^improvedgans<ul><li><a href=https://towardsdatascience.com/semi-supervised-learning-with-gans-9f3cb128c5e rel=noopener>https://towardsdatascience.com/semi-supervised-learning-with-gans-9f3cb128c5e</a></li><li><a href=https://hjweide.github.io/semi-supervised-dcgan rel=noopener>https://hjweide.github.io/semi-supervised-dcgan</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1606.01583 rel=noopener>Semi-Supervised Learning with Generative Adversarial Networks (Odena 2016)</a><ul><li>#CODE
<a href=https://github.com/tryambak2019/SGAN rel=noopener>https://github.com/tryambak2019/SGAN</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1703.09695 rel=noopener>Semi and Weakly Supervised Semantic Segmentation Using Generative Adversarial Network (Suoly 2017)</a></li><li>#PAPER
<a href=https://farzadab.github.io/assets/projects/pdf/Review__SSL_in_GANs.pdf rel=noopener>Semi-supervised Learning in Generative Adversarial Networks, review (2018)</a><ul><li>The GAN framework can be integrated with almost any available neural network classifier in order to make use of unlabeled data</li></ul></li></ul><h3 id=fewone-shot-learning-gans>Few/one-shot learning GANs</h3><p>See &ldquo;Few one-shot learning GANs&rdquo; section in <a href=/digitalgarden/AI/One-few-shot-learning rel=noopener class=internal-link data-src=/digitalgarden/AI/One-few-shot-learning>AI/One, few-shot learning</a></p><h3 id=gans-for-anomaly-detection>GANs for anomaly detection</h3><ul><li>#PAPER
<a href=https://arxiv.org/abs/1906.11632 rel=noopener>A Survey on GANs for Anomaly Detection (Di Mattia 2019)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/2009.07769 rel=noopener>TadGAN: Time Series Anomaly Detection Using Generative Adversarial Networks (Geiger 2020)</a><ul><li>#CODE ^oriontfanomalies in
<a href=/digitalgarden/AI/Anomaly-and-Outlier-Detection rel=noopener class=internal-link data-src=/digitalgarden/AI/Anomaly-and-Outlier-Detection>Anomaly and Outlier Detection</a></li><li><a href=https://analyticsindiamag.com/hands-on-guide-to-tadgan-with-python-codes/ rel=noopener>https://analyticsindiamag.com/hands-on-guide-to-tadgan-with-python-codes/</a></li></ul></li></ul></article><hr><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/digitalgarden/AI/Anomaly-and-Outlier-Detection>Anomaly and Outlier Detection</a></li><li><a href=/digitalgarden/AI/Computer-Vision/Computer-vision>Computer Vision</a></li><li><a href=/digitalgarden/AI/Computer-Vision/Deep-CV>Deep CV</a></li><li><a href=/digitalgarden/AI/Computer-Vision/Image-to-image-translation>Image-to-image translation</a></li><li><a href=/digitalgarden/AI/Deep-learning/DL>Deep Learning (DL)</a></li><li><a href=/digitalgarden/AI/Deep-learning/Generative-modelling>Generative modeling</a></li><li><a href=/digitalgarden/AI4ES/Extremes-events>Extremes events</a></li></ul></div></div><div id=contact_buttons><footer><p>Made by Carlos Alberto Gomez Gonzalez using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, ¬© 2022</p><ul><li><a href=https://carlos-gg.github.io/digitalgarden/>Home</a></li><a href=https://carlos-gg.github.io>Carlos'Homepage</a></ul></footer></div><script src=https://carlos-gg.github.io/digitalgarden/js/popover.e57188d2e4c06b0654e020b3a734bb62.min.js></script>
<script>initPopover("https://carlos-gg.github.io/digitalgarden")</script></div></body></html>