<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="In brief, a GAN consists of two networks; a generator (G) and a discriminator (D), given a set of training examples, G will generate outputs and D will classify them as either being from the same distribution as the training examples or not."><title>Generative Adversarial Networks</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=/icon.png><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Source+Sans+Pro:wght@400;600;700&family=Fira+Code:wght@400;700&display=swap" rel=stylesheet><style>:root{--lt-colours-light:var(--light) !important;--lt-colours-lightgray:var(--lightgray) !important;--lt-colours-dark:var(--secondary) !important;--lt-colours-secondary:var(--tertiary) !important;--lt-colours-gray:var(--outlinegray) !important}h1,h2,h3,h4,h5,h6,ol,ul,thead{font-family:Inter;color:var(--dark);font-weight:revert;margin:revert;padding:revert}p,ul,text{font-family:source sans pro,sans-serif;color:var(--gray);fill:var(--gray);font-weight:revert;margin:revert;padding:revert}#TableOfContents>ol{counter-reset:section;margin-left:0;padding-left:1.5em}#TableOfContents>ol>li{counter-increment:section}#TableOfContents>ol>li>ol{counter-reset:subsection}#TableOfContents>ol>li>ol>li{counter-increment:subsection}#TableOfContents>ol>li>ol>li::marker{content:counter(section)"." counter(subsection)"  "}#TableOfContents>ol>li::marker{content:counter(section)"  "}#TableOfContents>ol>li::marker,#TableOfContents>ol>li>ol>li::marker{font-family:Source Sans Pro;font-weight:700}footer{margin-top:4em;text-align:center}table{width:100%}img{width:100%;border-radius:3px;margin:1em 0}p>img+em{display:block;transform:translateY(-1em)}sup{line-height:0}p,tbody,li{font-family:Source Sans Pro;color:var(--gray);line-height:1.5em}blockquote{margin-left:0;border-left:3px solid var(--secondary);padding-left:1em;transition:border-color .2s ease}blockquote:hover{border-color:var(--tertiary)}table{padding:1.5em}td,th{padding:.1em .5em}.footnotes p{margin:.5em 0}.pagination{list-style:none;padding-left:0;display:flex;margin-top:2em;gap:1.5em;justify-content:center}.pagination>li{text-align:center;display:inline-block}.pagination>li a{background-color:initial!important}.pagination>li a[href$="#"]{opacity:.2}.section h3>a{font-weight:700;font-family:Inter;margin:0}.section p{margin-top:0}article>.meta{margin:-1.5em 0 1em;opacity:.7}article>.tags{list-style:none;padding-left:0}article>.tags .meta>h1{margin:0}article>.tags .meta>p{margin:0}article>.tags>li{display:inline-block}article>.tags>li>a{border-radius:8px;border:var(--outlinegray)1px solid;padding:.2em .5em}article>.tags>li>a::before{content:"#";margin-right:.3em;color:var(--outlinegray)}article a{font-family:Source Sans Pro;font-weight:600}article a.internal-link{text-decoration:none;background-color:rgba(143,159,169,.15);padding:0 .1em;margin:auto -.1em;border-radius:3px}.backlinks a{font-weight:600;font-size:.9rem}sup>a{text-decoration:none;padding:0 .1em 0 .2em}a{font-family:Inter,sans-serif;font-size:1em;font-weight:700;text-decoration:none;transition:all .2s ease;color:var(--secondary)}a:hover{color:var(--tertiary)!important}pre{font-family:fira code;padding:.75em;border-radius:3px;overflow-x:scroll}code{font-family:fira code;font-size:.85em;padding:.15em .3em;border-radius:5px;background:var(--lightgray)}html{scroll-behavior:smooth}html:lang(ar) p,html:lang(ar) h1,html:lang(ar) h2,html:lang(ar) h3,html:lang(ar) article{direction:rtl;text-align:right}body{margin:0;height:100vh;width:100vw;overflow-x:hidden;background-color:var(--light)}@keyframes fadeIn{0%{opacity:0}100%{opacity:1}}footer{margin-top:4em}footer>a{font-size:1em;color:var(--secondary);padding:0 .5em 3em}hr{width:25%;margin:4em auto;height:2px;border-radius:1px;border-width:0;color:var(--dark);background-color:var(--dark)}.singlePage{margin:4em 30vw}@media all and (max-width:1200px){.singlePage{margin:25px 5vw}}.page-end{display:flex;flex-direction:row;gap:2em}@media all and (max-width:780px){.page-end{flex-direction:column}}.page-end>*{flex:1 0}.page-end>.backlinks-container>ul{list-style:none;padding-left:0}.page-end>.backlinks-container>ul>li{margin:.5em 0;padding:.25em 1em;border:var(--outlinegray)1px solid;border-radius:5px}.page-end #graph-container{border:var(--outlinegray)1px solid;border-radius:5px}.centered{margin-top:30vh}article>h1{font-size:2em}header{display:flex;flex-direction:row;align-items:center}header>h1{font-size:2em}@media all and (max-width:600px){header>nav{display:none}}header>.spacer{flex:auto}header>svg{cursor:pointer;width:18px;min-width:18px;margin:0 1em}header>svg:hover .search-path{stroke:var(--tertiary)}header>svg .search-path{stroke:var(--gray);stroke-width:2px;transition:stroke .5s ease}#search-container{position:fixed;z-index:9999;left:0;top:0;width:100vw;height:100%;overflow:scroll;display:none;backdrop-filter:blur(4px);-webkit-backdrop-filter:blur(4px)}#search-container>div{width:50%;margin-top:15vh;margin-left:auto;margin-right:auto}@media all and (max-width:1200px){#search-container>div{width:90%}}#search-container>div>*{width:100%;border-radius:4px;background:var(--light);box-shadow:0 14px 50px rgba(27,33,48,.12),0 10px 30px rgba(27,33,48,.16);margin-bottom:2em}#search-container>div>input{box-sizing:border-box;padding:.5em 1em;font-family:Inter,sans-serif;color:var(--dark);font-size:1.1em;border:1px solid var(--outlinegray)}#search-container>div>input:focus{outline:none}#search-container>div>#results-container>.result-card{padding:1em;cursor:pointer;transition:background .2s ease;border:1px solid var(--outlinegray);border-bottom:none;width:100%;font-family:inherit;font-size:100%;line-height:1.15;margin:0;overflow:visible;text-transform:none;text-align:left;background:var(--light);outline:none}#search-container>div>#results-container>.result-card:hover,#search-container>div>#results-container>.result-card:focus{background:rgba(180,180,180,.15)}#search-container>div>#results-container>.result-card:first-of-type{border-top-left-radius:5px;border-top-right-radius:5px}#search-container>div>#results-container>.result-card:last-of-type{border-bottom-left-radius:5px;border-bottom-right-radius:5px;border-bottom:1px solid var(--outlinegray)}#search-container>div>#results-container>.result-card>h3,#search-container>div>#results-container>.result-card>p{margin:0}#search-container>div>#results-container>.result-card .search-highlight{background-color:#afbfc966;padding:.05em .2em;border-radius:3px}.section-ul{list-style:none;padding-left:0}.section-ul>li{border:1px solid var(--outlinegray);border-radius:5px;padding:0 1em;margin-bottom:1em}.section-ul>li h3{opacity:1;font-weight:700;margin-bottom:0}.section-ul>li .meta{opacity:.6}.popover{z-index:999;position:absolute;width:20em;display:inline-block;visibility:hidden;background-color:var(--light);padding:1em;border:1px solid var(--outlinegray);border-radius:5px;transform:translate(-50%,40%);opacity:0;pointer-events:none;transition:opacity .2s ease,transform .2s ease;transition-delay:.3s;user-select:none}@media all and (max-width:600px){.popover{display:none}}.popover.visible{opacity:1;visibility:visible;transform:translate(-50%,20%)}.popover>h3{font-size:1rem;margin:.25em 0}.popover>.meta{margin-top:.25em;opacity:.5}.popover>p{margin:0;font-weight:400;user-select:none}</style><style>.darkmode{float:right;padding:1em;min-width:30px;position:relative}@media all and (max-width:450px){.darkmode{padding:1em}}.darkmode>.toggle{display:none;box-sizing:border-box}.darkmode svg{opacity:0;position:absolute;width:20px;height:20px;top:calc(50% - 10px);margin:0 7px;fill:var(--gray);transition:opacity .1s ease}.toggle:checked~label>#dayIcon{opacity:0}.toggle:checked~label>#nightIcon{opacity:1}.toggle:not(:checked)~label>#dayIcon{opacity:1}.toggle:not(:checked)~label>#nightIcon{opacity:0}</style><style>.chroma{color:#f8f8f2;background-color:#282a36;overflow:hidden}.chroma .lntd{vertical-align:top;padding:0;margin:0;border:0}.chroma .lntable{border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block}.chroma .hl{display:block;width:100%;background-color:#ffc}.chroma .lnt{margin-right:.4em;padding:0 .4em;color:#7f7f7f}.chroma .ln{margin-right:.4em;padding:0 .4em;color:#7f7f7f}.chroma .k{color:#ff79c6}.chroma .kc{color:#ff79c6}.chroma .kd{color:#8be9fd;font-style:italic}.chroma .kn{color:#ff79c6}.chroma .kp{color:#ff79c6}.chroma .kr{color:#ff79c6}.chroma .kt{color:#8be9fd}.chroma .na{color:#50fa7b}.chroma .nb{color:#8be9fd;font-style:italic}.chroma .nc{color:#50fa7b}.chroma .nf{color:#50fa7b}.chroma .nl{color:#8be9fd;font-style:italic}.chroma .nt{color:#ff79c6}.chroma .nv{color:#8be9fd;font-style:italic}.chroma .vc{color:#8be9fd;font-style:italic}.chroma .vg{color:#8be9fd;font-style:italic}.chroma .vi{color:#8be9fd;font-style:italic}.chroma .s{color:#f1fa8c}.chroma .sa{color:#f1fa8c}.chroma .sb{color:#f1fa8c}.chroma .sc{color:#f1fa8c}.chroma .dl{color:#f1fa8c}.chroma .sd{color:#f1fa8c}.chroma .s2{color:#f1fa8c}.chroma .se{color:#f1fa8c}.chroma .sh{color:#f1fa8c}.chroma .si{color:#f1fa8c}.chroma .sx{color:#f1fa8c}.chroma .sr{color:#f1fa8c}.chroma .s1{color:#f1fa8c}.chroma .ss{color:#f1fa8c}.chroma .m{color:#bd93f9}.chroma .mb{color:#bd93f9}.chroma .mf{color:#bd93f9}.chroma .mh{color:#bd93f9}.chroma .mi{color:#bd93f9}.chroma .il{color:#bd93f9}.chroma .mo{color:#bd93f9}.chroma .o{color:#ff79c6}.chroma .ow{color:#ff79c6}.chroma .c{color:#6272a4}.chroma .ch{color:#6272a4}.chroma .cm{color:#6272a4}.chroma .c1{color:#6272a4}.chroma .cs{color:#6272a4}.chroma .cp{color:#ff79c6}.chroma .cpf{color:#ff79c6}.chroma .gd{color:#8b080b}.chroma .ge{text-decoration:underline}.chroma .gh{font-weight:700}.chroma .gi{font-weight:700}.chroma .go{color:#44475a}.chroma .gu{font-weight:700}.chroma .gl{text-decoration:underline}.lntd:first-of-type>.chroma{padding-right:0}.chroma code{font-family:fira code!important;font-size:.85em;line-height:1em;background:0 0;padding:0}.chroma{border-radius:3px;margin:0}</style><style>:root{--light:#faf8f8;--dark:#141021;--secondary:#284b63;--tertiary:#84a59d;--visited:#afbfc9;--primary:#f28482;--gray:#4e4e4e;--lightgray:#f0f0f0;--outlinegray:#dadada}[saved-theme=dark]{--light:#1e1e21 !important;--dark:#fbfffe !important;--secondary:#6b879a !important;--visited:#4a575e !important;--tertiary:#84a59d !important;--primary:#f58382 !important;--gray:#d4d4d4 !important;--lightgray:#292633 !important;--outlinegray:#343434 !important}</style><script>const userPref=window.matchMedia('(prefers-color-scheme: light)').matches?'light':'dark',currentTheme=localStorage.getItem('theme')??userPref;currentTheme&&document.documentElement.setAttribute('saved-theme',currentTheme);const switchTheme=a=>{a.target.checked?(document.documentElement.setAttribute('saved-theme','dark'),localStorage.setItem('theme','dark')):(document.documentElement.setAttribute('saved-theme','light'),localStorage.setItem('theme','light'))};window.addEventListener('DOMContentLoaded',()=>{const a=document.querySelector('#darkmode-toggle');a.addEventListener('change',switchTheme,!1),currentTheme==='dark'&&(a.checked=!0)})</script><script>let saved=!1;const fetchData=async()=>{if(saved)return saved;const promises=[fetch("https://carlgogo.github.io/AIDigitalGarden/linkIndex.json").then(a=>a.json()).then(a=>({index:a.index,links:a.links})),fetch("https://carlgogo.github.io/AIDigitalGarden/contentIndex.json").then(a=>a.json())],[{index,links},content]=await Promise.all(promises),res={index,links,content};return saved=res,res};fetchData()</script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-S103D50NQ0"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-S103D50NQ0',{anonymize_ip:!1})}</script><script>function htmlToElement(a){const b=document.createElement('template');return a=a.trim(),b.innerHTML=a,b.content.firstChild}const baseUrl="https://carlgogo.github.io/AIDigitalGarden".replace(window.location.origin,"");document.addEventListener("DOMContentLoaded",()=>{fetchData().then(({content:a})=>{const b=[...document.getElementsByClassName("internal-link")];b.forEach(b=>{const c=a[b.dataset.src.replace(baseUrl,"")];if(c){const d=`<div class="popover">
    <h3>${c.title}</h3>
    <p>${removeMarkdown(c.content).split(" ",20).join(" ")}...</p>
    <p class="meta">${new Date(c.lastmodified).toLocaleDateString()}</p>
</div>`,a=htmlToElement(d);b.appendChild(a),b.addEventListener("mouseover",()=>{a.classList.add("visible")}),b.addEventListener("mouseout",()=>{a.classList.remove("visible")})}})})})</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/gh/nextapps-de/flexsearch@0.7.2/dist/flexsearch.bundle.js></script><script>const removeMarkdown=(c,b={listUnicodeChar:!1,stripListLeaders:!0,gfm:!0,useImgAltText:!1,preserveLinks:!1})=>{let a=c||"";a=a.replace(/^(-\s*?|\*\s*?|_\s*?){3,}\s*$/gm,"");try{b.stripListLeaders&&(b.listUnicodeChar?a=a.replace(/^([\s\t]*)([\*\-\+]|\d+\.)\s+/gm,b.listUnicodeChar+" $1"):a=a.replace(/^([\s\t]*)([\*\-\+]|\d+\.)\s+/gm,"$1")),b.gfm&&(a=a.replace(/\n={2,}/g,"\n").replace(/~{3}.*\n/g,"").replace(/~~/g,"").replace(/`{3}.*\n/g,"")),b.preserveLinks&&(a=a.replace(/\[(.*?)\][\[\(](.*?)[\]\)]/g,"$1 ($2)")),a=a.replace(/<[^>]*>/g,"").replace(/^[=\-]{2,}\s*$/g,"").replace(/\[\^.+?\](\: .*?$)?/g,"").replace(/\s{0,2}\[.*?\]: .*?$/g,"").replace(/\!\[(.*?)\][\[\(].*?[\]\)]/g,b.useImgAltText?"$1":"").replace(/\[(.*?)\][\[\(].*?[\]\)]/g,"$1").replace(/^\s{0,3}>\s?/g,"").replace(/(^|\n)\s{0,3}>\s?/g,"\n\n").replace(/^\s{1,2}\[(.*?)\]: (\S+)( ".*?")?\s*$/g,"").replace(/^(\n)?\s{0,}#{1,6}\s+| {0,}(\n)?\s{0,}#{0,} {0,}(\n)?\s{0,}$/gm,"$1$2$3").replace(/([\*_]{1,3})(\S.*?\S{0,1})\1/g,"$2").replace(/([\*_]{1,3})(\S.*?\S{0,1})\1/g,"$2").replace(/(`{3,})(.*?)\1/gm,"$2").replace(/`(.+?)`/g,"$1").replace(/\n{2,}/g,"\n\n")}catch(a){return console.error(a),c}return a}</script><script>async function run(){const g=new FlexSearch.Document({cache:!0,charset:"latin:extra",optimize:!0,worker:!0,document:{index:[{field:"content",tokenize:"strict",context:{resolution:5,depth:3,bidirectional:!0},suggest:!0},{field:"title",tokenize:"forward"}]}}),{content:d}=await fetchData();for(const[b,a]of Object.entries(d))g.add({id:b,title:a.title,content:removeMarkdown(a.content)});const j=(i,j)=>{const a=20,k=j.split(/\s+/).filter(a=>a!==""),b=i.split(/\s+/).filter(a=>a!==""),f=a=>k.some(b=>a.toLowerCase().startsWith(b.toLowerCase())),d=b.map(f);let e=0,g=0;for(let b=0;b<Math.max(d.length-a,0);b++){const f=d.slice(b,b+a),c=f.reduce((a,b)=>a+b,0);c>=e&&(e=c,g=b)}const c=Math.max(g-a,0),h=Math.min(c+2*a,b.length),l=b.slice(c,h).map(a=>{return f(a)?`<span class="search-highlight">${a}</span>`:a}).join(" ").replaceAll('</span> <span class="search-highlight">'," ");return`${c===0?"":"..."}${l}${h===b.length?"":"..."}`},m=({url:b,title:c,content:d,term:a})=>{const e=removeMarkdown(d),f=j(c,a),g=j(e,a);return`<button class="result-card" id="${b}">
        <h3>${f}</h3>
        <p>${g}</p>
    </button>`},h=(a,b)=>{window.location.href="https://carlgogo.github.io/AIDigitalGarden"+`${a}#:~:text=${encodeURIComponent(b)}`},l=a=>({id:a,url:a,title:d[a].title,content:d[a].content}),c=document.getElementById('search-bar'),e=document.getElementById("results-container");let b;c.addEventListener("keyup",a=>{if(a.key==="Enter"){const a=document.getElementsByClassName("result-card")[0];h(a.id,b)}}),c.addEventListener('input',a=>{b=a.target.value,g.search(b,[{field:"content",limit:10,suggest:!0},{field:"title",limit:5}]).then(d=>{const a=b=>{const a=d.filter(a=>a.field===b);return a.length===0?[]:[...a[0].result]},f=new Set([...a('title'),...a('content')]),c=[...f].map(l);if(c.length===0)e.innerHTML=`<button class="result-card">
                    <h3>No results.</h3>
                    <p>Try another search term?</p>
                </button>`;else{e.innerHTML=c.map(a=>m({...a,term:b})).join("\n");const a=document.getElementsByClassName("result-card");[...a].forEach(a=>{a.onclick=()=>h(a.id,b)})}})});const a=document.getElementById("search-container");function f(){a.style.display==="none"||a.style.display===""?(c.value="",e.innerHTML="",a.style.display="block",c.focus()):a.style.display="none"}function i(){a.style.display="none"}document.addEventListener('keydown',a=>{a.key==="/"&&(a.preventDefault(),f()),a.key==="Escape"&&(a.preventDefault(),i())});const k=document.getElementById("search-icon");k.addEventListener('click',a=>{f()}),k.addEventListener('keydown',a=>{f()}),a.addEventListener('click',a=>{i()}),document.getElementById("search-space").addEventListener('click',a=>{a.stopPropagation()})}run()</script><div class=singlePage><header><h1 id=page-title><a href=https://carlgogo.github.io/AIDigitalGarden>🪴 AI and DS Digital Garden</a></h1><svg tabindex="0" id="search-icon" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg><div class=spacer></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Generative Adversarial Networks</h1><p class=meta>Last updated March 7, 2022</p><ul class=tags></ul><aside class=mainTOC><h3>Table of Contents</h3><nav id=TableOfContents><ol><li><a href=#courses>Courses</a></li><li><a href=#code>Code</a></li><li><a href=#references>References</a></li></ol><ol><li><a href=#gans-for-super-resolution>GANs for super-resolution</a></li><li><a href=#gans-for-missing-data-imputation-and-inpainting>GANs for missing data, imputation and inpainting</a></li><li><a href=#image-to-image-translation-conditional-gans>Image-to-image translation. Conditional GANs</a></li><li><a href=#gans-for-spatio-temporal-data-generation>GANs for spatio-temporal data generation</a></li><li><a href=#gans-for-representation-learning-and-image-synthesis>GANs for representation learning and image synthesis</a></li><li><a href=#semi-supervised-gans>Semi-supervised GANs</a></li><li><a href=#fewone-shot-learning-gans>Few/one-shot learning GANs</a></li><li><a href=#gans-for-anomaly-detection>GANs for anomaly detection</a></li></ol></nav></aside><ul><li><p>In brief, a GAN consists of two networks; a generator (G) and a discriminator (D), given a set of training examples, G will generate outputs and D will classify them as either being from the same distribution as the training examples or not. In doing so D is optimized so as to be able to discriminate between examples from the training example and from the generator network which in turn is optimized to fool D into classifying its output as being drawn from the training examples. After such training G can now generate samples with properties very similar to those of the training examples. GANs tend to be devilishly hard to train.</p></li><li><p><a href=https://github.com/pshams55/GAN-Case-Study>https://github.com/pshams55/GAN-Case-Study</a></p></li><li><p>Generative Adversarial Networks: <a href=https://spectra.pub/ml/gans>https://spectra.pub/ml/gans</a></p></li><li><p>GANs for good: <a href="https://www.youtube.com/watch?v=9d4jmPmTWmc">https://www.youtube.com/watch?v=9d4jmPmTWmc</a></p></li><li><p>Generative adversarial networks: <a href=https://deepgenerativemodels.github.io/notes/gan/>https://deepgenerativemodels.github.io/notes/gan/</a></p></li><li><p>Generative adversarial networks for beginners: <a href=https://www.oreilly.com/learning/generative-adversarial-networks-for-beginners>https://www.oreilly.com/learning/generative-adversarial-networks-for-beginners</a></p></li><li><p>Intuitive explanation of GANs. Subtypes. <a href=https://tryolabs.com/blog/2016/12/06/major-advancements-deep-learning-2016/>https://tryolabs.com/blog/2016/12/06/major-advancements-deep-learning-2016/</a></p></li><li><p><a href=https://adeshpande3.github.io/adeshpande3.github.io/Deep-Learning-Research-Review-Week-1-Generative-Adversarial-Nets>https://adeshpande3.github.io/adeshpande3.github.io/Deep-Learning-Research-Review-Week-1-Generative-Adversarial-Nets</a></p></li><li><p><a href=http://www.openias.org/hybrid-generative-discriminative>http://www.openias.org/hybrid-generative-discriminative</a></p></li><li><p><a href=http://edwardlib.org/tutorials/gan>http://edwardlib.org/tutorials/gan</a></p></li><li><p>Play with GANs in your browser: <a href=https://poloclub.github.io/ganlab/>https://poloclub.github.io/ganlab/</a></p></li><li><p>Do GANs actually do distribution learning?: <a href=http://www.offconvex.org/2017/07/06/GANs3/>http://www.offconvex.org/2017/07/06/GANs3/</a></p></li><li><p>The GAN Zoo - A list of all named GANs! <a href=https://deephunt.in/the-gan-zoo-79597dc8c347>https://deephunt.in/the-gan-zoo-79597dc8c347</a></p></li><li><p>Advances in Generative Adversarial Networks: <a href=https://beyondminds.ai/advances-in-generative-adversarial-networks-gans/>https://beyondminds.ai/advances-in-generative-adversarial-networks-gans/</a> ^advancesingans</p><ul><li>Drawbacks of using GANs: Mode collapse, Convergence, Quality evaluation, Metrics</li><li>Techniques for Improving Performance:<ul><li>Alternative Loss Functions: One of the most popular fixes to the shortcomings of GANs is the Wasserstein GAN. It essentially replaces the Jensen Shannon divergence of conventional GANs with the Earth Mover distance (Wasserstein-1 distance or EM distance)</li><li>Two Timescale Update Rule (TTUR): In this method, we use a different learning rate for the discriminator and the generator. Typically, a slower update rule is used for the generator and a faster update rule is used for the discriminator</li><li>Gradient Penalty: In the paper [[GANs#^wgangp]], a simple gradient penalty was introduced which is added to the loss function to avoid exploding vanishing gradients and optimization issues (caused by weight clipping)</li><li>Spectral Normalization: weight normalization technique that is typically used on the Discriminator to enhance the training process</li><li>Unrolling and Packing: <a href=http://aiden.nibali.org/blog/2017-01-18-mode-collapse-gans/>http://aiden.nibali.org/blog/2017-01-18-mode-collapse-gans/</a></li><li>Stacking GANs: use multiple GANs placed consecutively, where each GAN solves an easier version of the problem. For instance, FashionGAN used two GANs to perform localized image translation. Progressive GANs (ProGANs) can generate high quality images of excellent resolution.</li><li>Relativistic GANs: Conventional GANs measure the probability of the generated data being real. Relativistic GANs measure the probability of the generated data being “more realistic” than the real data. We can measure this “relative realism” using an appropriate distance measure, as mentioned in the RGAN [[GANs#^190c58]] paper</li><li>Self Attention Mechanism [[GANs#^sagan]]: The authors of Self Attention GANs claim that convolutions used for generating images look at information that are spread locally. That is, they miss out on relationships that span globally due to their restrictive receptive field. Self-Attention Generative Adversarial Network allows attention-driven, long-range dependency modeling for image generation tasks.</li><li>Miscellaneous Techniques: Feature Matching, Mini Batch Discrimination, Historical Averaging, One-sided Label Smoothing, Virtual Batch Normalization. See [[GANs#^improvedgans]]</li></ul></li></ul></li></ul><h2 id=courses>Courses</h2><ul><li>#COURSE Generative Adversarial Networks ( DeepMind x UCL | Deep Learning Lectures | 9/12): <a href="https://www.youtube.com/watch?v=wFsI2WqUfdA&t=850s">https://www.youtube.com/watch?v=wFsI2WqUfdA&t=850s</a></li></ul><h2 id=code>Code</h2><ul><li>#CODE Keras-GAN - Collection of Keras implementations of GANs: <a href=https://github.com/eriklindernoren/Keras-GAN>https://github.com/eriklindernoren/Keras-GAN</a></li><li>#CODE Pytorch-GAN - Collection of Pytorch implementations of GANs: <a href=https://github.com/eriklindernoren/PyTorch-GAN>https://github.com/eriklindernoren/PyTorch-GAN</a></li><li>#CODE Generative models in Tensorflow and Pytorch: <a href=https://github.com/wiseodd/generative-models>https://github.com/wiseodd/generative-models</a></li><li>#CODE Tensorflow generative model collection: <a href=https://github.com/hwalsuklee/tensorflow-generative-model-collections>https://github.com/hwalsuklee/tensorflow-generative-model-collections</a></li><li>#CODE ydata-synthetic: <a href=https://github.com/ydataai/ydata-synthetic>https://github.com/ydataai/ydata-synthetic</a><ul><li>This repository contains material related with Generative Adversarial Networks for synthetic data generation, in particular regular tabular data and time-series</li></ul></li></ul><h2 id=references>References</h2><p>Review papers:</p><ul><li><p>#PAPER A Survey on Generative Adversarial Networks: Variants, Applications,and Training (Jabbar 2020): <a href=https://arxiv.org/abs/2006.05132>https://arxiv.org/abs/2006.05132</a></p></li><li><p>#PAPER A Review on Generative Adversarial Networks: Algorithms, Theory, and Applications (Gui 2020): <a href=https://arxiv.org/abs/2001.06937>https://arxiv.org/abs/2001.06937</a></p></li><li><p>#PAPER Generative Adversarial Networks (Goodfellow 2014): <a href=http://arxiv.org/abs/1406.2661>http://arxiv.org/abs/1406.2661</a></p><ul><li>Paper explained: <a href="https://www.youtube.com/watch?v=eyxmSmjmNS0">https://www.youtube.com/watch?v=eyxmSmjmNS0</a></li></ul></li><li><p>#PAPER GAN to convert text descriptions into images (Reed 2016): <a href=https://arxiv.org/abs/1605.05396>https://arxiv.org/abs/1605.05396</a></p></li><li><p>#PAPER Unsupervised representation learning with GANs (Radford 2016): <a href=https://arxiv.orga/abs/1511.06434v2>https://arxiv.orga/abs/1511.06434v2</a></p><ul><li>Although GANs were already introduced in 2014 by Ian Goodfellow, it wasn&rsquo;t until the publication of this paper detailing a deep convolutional architecture (DCGAN) that GANs really took off</li><li><a href=https://www.tensorflow.org/tutorials/generative/dcgan>https://www.tensorflow.org/tutorials/generative/dcgan</a></li><li><a href=https://towardsdatascience.com/dcgans-deep-convolutional-generative-adversarial-networks-c7f392c2c8f8>https://towardsdatascience.com/dcgans-deep-convolutional-generative-adversarial-networks-c7f392c2c8f8</a></li><li>#CODE <a href=https://github.com/tensorflow/models/blob/master/research/slim/nets/dcgan.py>https://github.com/tensorflow/models/blob/master/research/slim/nets/dcgan.py</a></li></ul></li><li><p>#PAPER Deconvolution and Checkerboard Artifacts (Odena 2016): <a href=https://distill.pub/2016/deconv-checkerboard/>https://distill.pub/2016/deconv-checkerboard/</a></p></li><li><p>#PAPER InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets (Chen 2016): <a href=https://arxiv.org/abs/1606.03657>https://arxiv.org/abs/1606.03657</a></p><ul><li><a href=https://wiseodd.github.io/techblog/2017/01/29/infogan/>https://wiseodd.github.io/techblog/2017/01/29/infogan/</a></li></ul></li><li><p>#PAPER Checkerboard artifact free sub-pixel convolution: A note on sub-pixel convolution, resize convolution and convolution resize (Aitken 2017): <a href=https://arxiv.org/abs/1707.02937>https://arxiv.org/abs/1707.02937</a></p></li><li><p>#PAPER Wasserstein GAN (Arjovsky 2017): <a href=https://arxiv.org/abs/1701.07875>https://arxiv.org/abs/1701.07875</a></p><ul><li>From GAN to Wasserstein GAN: <a href=https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html#wasserstein-gan-wgan>https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html#wasserstein-gan-wgan</a></li></ul></li><li><p>#PAPER Improved Training of Wasserstein GANs (Gulrajani 2017): <a href=https://arxiv.org/abs/1704.00028>https://arxiv.org/abs/1704.00028</a> ^wgangp</p></li><li><p>#PAPER Bayesian GAN (Saatchi 2017): <a href=https://arxiv.org/abs/1705.09558>https://arxiv.org/abs/1705.09558</a></p><ul><li>#CODE <a href=https://github.com/andrewgordonwilson/bayesgan>https://github.com/andrewgordonwilson/bayesgan</a></li></ul></li><li><p>#PAPER WaterGAN: Unsupervised Generative Network to Enable Real-time Color Correction of Monocular Underwater Images (Li 2017): <a href=https://arxiv.org/abs/1702.07392>https://arxiv.org/abs/1702.07392</a></p></li><li><p>#PAPER A Style-Based Generator Architecture for Generative Adversarial Networks, StyleGAN (Karras 2018): <a href=https://arxiv.org/abs/1812.04948>https://arxiv.org/abs/1812.04948</a></p><ul><li>#TALK <a href=https://youtu.be/kSLJriaOumA>https://youtu.be/kSLJriaOumA</a></li><li>#CODE <a href=https://github.com/NVlabs/stylegan>https://github.com/NVlabs/stylegan</a></li><li>FFHQ: <a href=https://github.com/NVlabs/ffhq-dataset>https://github.com/NVlabs/ffhq-dataset</a></li></ul></li><li><p>#PAPER The relativistic discriminator: a key element missing from standard GAN (Jolicoeur-Martineau 2018): <a href=https://arxiv.org/abs/1807.00734>https://arxiv.org/abs/1807.00734</a> ^190c58</p></li><li><p>#PAPER From GAN to WGAN (Wenb 2019): <a href=https://arxiv.org/abs/1904.08994>https://arxiv.org/abs/1904.08994</a></p></li><li><p>#PAPER Time Series Simulation by Conditional Generative Adversarial Net (Fu 2019): <a href=https://arxiv.org/abs/1904.11419>https://arxiv.org/abs/1904.11419</a></p></li><li><p>#PAPER HoloGAN: Unsupervised learning of 3D representations from natural images (Nguyen-Phuoc 2019): <a href=https://arxiv.org/abs/1904.01326>https://arxiv.org/abs/1904.01326</a></p><ul><li><a href=https://www.monkeyoverflow.com/#/hologan-unsupervised-learning-of-3d-representations-from-natural-images/>https://www.monkeyoverflow.com/#/hologan-unsupervised-learning-of-3d-representations-from-natural-images/</a></li><li>#TALK <a href="https://www.youtube.com/watch?v=z2DnFOQNECM">https://www.youtube.com/watch?v=z2DnFOQNECM</a></li></ul></li><li><p>#PAPER Implicit competitive regularization in GANs (Schafer 2020): <a href=https://arxiv.org/abs/1910.05852>https://arxiv.org/abs/1910.05852</a></p></li><li><p>#PAPER Training Generative Adversarial Networks with Limited Data (Karras 2020): <a href=https://arxiv.org/abs/2006.06676>https://arxiv.org/abs/2006.06676</a></p></li><li><p>#PAPER Gradient-Guided Dynamic Efficient Adversarial Training (Waag 2021): <a href=https://arxiv.org/abs/2103.03076>https://arxiv.org/abs/2103.03076</a></p><ul><li>#CODE <a href=https://github.com/locuslab/fast_adversarial>https://github.com/locuslab/fast_adversarial</a></li><li>The goal of DEAT is to improve adversarial training while maintaining effectiveness. It begins by training one batch replay and gradually increases it during training</li><li>This method reduces large amount of computation when doing backpropagation and consequently achieves a more efficient training paradigm</li></ul></li></ul><h1 id=subtopics>Subtopics</h1><h2 id=gans-for-super-resolution>GANs for super-resolution</h2><p>See [[Super-resolution#GAN-based]]</p><h2 id=gans-for-missing-data-imputation-and-inpainting>GANs for missing data, imputation and inpainting</h2><p>See [[Inpainting#GAN-based]]</p><h2 id=image-to-image-translation-conditional-gans>Image-to-image translation. Conditional GANs</h2><p>See [[Image-to-image translation#GAN-based]]</p><h2 id=gans-for-spatio-temporal-data-generation>GANs for spatio-temporal data generation</h2><ul><li>#PAPER COT-GAN: Generating Sequential Data via Causal Optimal Transport (Xu 2020): <a href=https://arxiv.org/abs/2006.08571>https://arxiv.org/abs/2006.08571</a></li><li>#PAPER SPATE-GAN: Improved Generative Modeling of Dynamic Spatio-Temporal Patterns with an Autoregressive Embedding Loss (Klemmer 2021): <a href=https://arxiv.org/abs/2109.15044#>https://arxiv.org/abs/2109.15044#</a> ^spate-gan<ul><li>#CODE <a href=https://github.com/konstantinklemmer/spate-gan>https://github.com/konstantinklemmer/spate-gan</a></li></ul></li></ul><h2 id=gans-for-representation-learning-and-image-synthesis>GANs for representation learning and image synthesis</h2><ul><li>#PAPER Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks, Laplacian GAN (Denton 2015): <a href=https://arxiv.org/abs/1506.05751>https://arxiv.org/abs/1506.05751</a></li><li>#PAPER Adversarial feature learning, BiGAN (Donahue 2017): <a href=https://arxiv.org/abs/1605.09782>https://arxiv.org/abs/1605.09782</a></li><li>#PAPER Large Scale Adversarial Representation Learning, BigBiGAN (Donahue 2019): <a href=https://arxiv.org/abs/1907.02544>https://arxiv.org/abs/1907.02544</a></li><li>#PAPER Large Scale GAN Training for High Fidelity Natural Image Synthesis, BigGAN (Brock 2019): <a href=https://arxiv.org/abs/1809.11096>https://arxiv.org/abs/1809.11096</a></li><li>#PAPER Self-Attention GANs, SAGAN (Zhang 2019): <a href=https://arxiv.org/abs/1805.08318>https://arxiv.org/abs/1805.08318</a> ^sagan<ul><li>#CODE <a href=https://github.com/brain-research/self-attention-gan>https://github.com/brain-research/self-attention-gan</a></li></ul></li><li>#PAPER In-domain GAN Inversion for Real Image Editing (Zhu 2020): <a href=https://genforce.github.io/idinvert/>https://genforce.github.io/idinvert/</a><ul><li>Paper explained: <a href="https://www.youtube.com/watch?v=2qMw8sOsNg0">https://www.youtube.com/watch?v=2qMw8sOsNg0</a></li></ul></li><li>#PAPER High-Fidelity Generative Image Compression (Mentzer 2020): <a href=https://arxiv.org/abs/2006.09965>https://arxiv.org/abs/2006.09965</a><ul><li><a href=https://hific.github.io/>https://hific.github.io/</a></li></ul></li><li>#PAPER Generative Adversarial Networks for Image and Video Synthesis: Algorithms and Applications (Liu 2020): <a href=https://arxiv.org/abs/2008.02793>https://arxiv.org/abs/2008.02793</a></li><li>#PAPER Image Synthesis with Adversarial Networks: a Comprehensive Survey and Case Studies (Shamsolmoali 2020): <a href=https://arxiv.org/abs/2012.13736>https://arxiv.org/abs/2012.13736</a></li><li>#PAPER Cross-Modal Contrastive Learning for Text-to-Image Generation (Zhang 2021): <a href=https://arxiv.org/abs/2101.04702>https://arxiv.org/abs/2101.04702</a><ul><li><a href=https://ai.googleblog.com/2021/05/cross-modal-contrastive-learning-for.html>https://ai.googleblog.com/2021/05/cross-modal-contrastive-learning-for.html</a></li><li>text-to-image generation by learning to maximize the mutual information between image and text using inter-modal (image-text) and intra-modal (image-image) contrastive losses</li></ul></li><li>#PAPER TriGAN: image-to-image translation for multi-source domain adaptation (Roy 2021): <a href=https://link.springer.com/article/10.1007/s00138-020-01164-4>https://link.springer.com/article/10.1007/s00138-020-01164-4</a><ul><li>approach for multi-source domain adaptation (MSDA) based on generative adversarial networks</li></ul></li><li>#PAPER Sketch Your Own GAN (Wang 2021): <a href=https://arxiv.org/abs/2108.02774>https://arxiv.org/abs/2108.02774</a></li><li>#PAPER Instance-Conditioned GAN (Casanova 2021): <a href=https://arxiv.org/abs/2109.05070>https://arxiv.org/abs/2109.05070</a><ul><li>#CODE <a href="https://paperswithcode.com/paper/instance-conditioned-gan?from=n17">https://paperswithcode.com/paper/instance-conditioned-gan?from=n17</a></li></ul></li></ul><h2 id=semi-supervised-gans>Semi-supervised GANs</h2><ul><li>#PAPER Improved Techniques for Training GANs (Saliman 2016): <a href=https://arxiv.org/abs/1606.03498>https://arxiv.org/abs/1606.03498</a> ^improvedgans<ul><li><a href=https://towardsdatascience.com/semi-supervised-learning-with-gans-9f3cb128c5e>https://towardsdatascience.com/semi-supervised-learning-with-gans-9f3cb128c5e</a></li><li><a href=https://hjweide.github.io/semi-supervised-dcgan>https://hjweide.github.io/semi-supervised-dcgan</a></li></ul></li><li>#PAPER Semi-Supervised Learning with Generative Adversarial Networks (Odena 2016): <a href=https://arxiv.org/abs/1606.01583>https://arxiv.org/abs/1606.01583</a><ul><li>#CODE <a href=https://github.com/tryambak2019/SGAN>https://github.com/tryambak2019/SGAN</a></li></ul></li><li>#PAPER Semi and Weakly Supervised Semantic Segmentation Using Generative Adversarial Network (Suoly 2017): <a href=https://arxiv.org/abs/1703.09695>https://arxiv.org/abs/1703.09695</a></li><li>#PAPER Semi-supervised Learning in Generative Adversarial Networks, review (2018): <a href=https://farzadab.github.io/assets/projects/pdf/Review__SSL_in_GANs.pdf>https://farzadab.github.io/assets/projects/pdf/Review__SSL_in_GANs.pdf</a><ul><li>The GAN framework can be integrated with almost any available neural network classifier in order to make use of unlabeled data</li></ul></li></ul><h2 id=fewone-shot-learning-gans>Few/one-shot learning GANs</h2><p>See [[One, few-shot learning#Few one-shot learning GANs]].</p><h2 id=gans-for-anomaly-detection>GANs for anomaly detection</h2><ul><li><p>#CODE Orion - A machine learning library for detecting anomalies in signals: <a href=https://github.com/signals-dev/Orion>https://github.com/signals-dev/Orion</a> ^oriontfanomalies</p><ul><li><a href=https://signals-dev.github.io/Orion/>https://signals-dev.github.io/Orion/</a></li></ul></li><li><p>#PAPER A Survey on GANs for Anomaly Detection (Di Mattia 2019): <a href=https://arxiv.org/abs/1906.11632>https://arxiv.org/abs/1906.11632</a></p></li><li><p>#PAPER TadGAN: Time Series Anomaly Detection Using Generative Adversarial Networks (Geiger 2020): <a href=https://arxiv.org/abs/2009.07769>https://arxiv.org/abs/2009.07769</a></p><ul><li>#CODE [[#^oriontfanomalies]]</li><li><a href=https://analyticsindiamag.com/hands-on-guide-to-tadgan-with-python-codes/>https://analyticsindiamag.com/hands-on-guide-to-tadgan-with-python-codes/</a></li></ul></li></ul></article><hr><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script>async function run(){const{index:a,links:p,content:u}=await fetchData(),j="https://carlgogo.github.io/AIDigitalGarden/AI/Deep-learning/GANs".replace("https://carlgogo.github.io/AIDigitalGarden",""),n=[{"/moc":"#4388cc"}];let h=-1;const m=a=>[...new Set(a.flatMap(a=>[a.source,a.target]))],e=new Set,f=[j||"/","__SENTINEL"];if(h>=0)while(h>=0&&f.length>0){const b=f.shift();if(b==="__SENTINEL")h--,f.push("__SENTINEL");else{e.add(b);const c=a.links[b]||[],d=a.backlinks[b]||[];f.push(...c.map(a=>a.target),...d.map(a=>a.source))}}else m(p).forEach(a=>e.add(a));const g={nodes:[...e].map(a=>({id:a})),links:p.filter(a=>e.has(a.source)&&e.has(a.target))},k=a=>{if(a.id===j||a.id==="/"&&j==="")return"var(--g-node-active)";for(const b of n){const c=Object.keys(b)[0],d=b[c];if(a.id.startsWith(c))return d}return"var(--g-node)"},l=c=>{function d(b,a){b.active||c.alphaTarget(1).restart(),a.fx=a.x,a.fy=a.y}function e(a,b){b.fx=a.x,b.fy=a.y}function f(b,a){b.active||c.alphaTarget(0),a.fx=null,a.fy=null}const a=!0,b=()=>{};return d3.drag().on("start",a?d:b).on("drag",a?e:b).on("end",a?f:b)},c=250,b=document.getElementById("graph-container").offsetWidth,i=d3.forceSimulation(g.nodes).force("charge",d3.forceManyBody().strength(-30)).force("link",d3.forceLink(g.links).id(a=>a.id)).force("center",d3.forceCenter()),d=d3.select('#graph-container').append('svg').attr('width',b).attr('height',c).attr("viewBox",[-b/2,-c/2,b,c]),t=!1;if(t){const a=[{Current:"var(--g-node-active)"},{Note:"var(--g-node)"},...n];a.forEach((a,e)=>{const f=Object.keys(a)[0],g=a[f];d.append("circle").attr("cx",-b/2+20).attr("cy",c/2-30*(e+1)).attr("r",6).style("fill",g),d.append("text").attr("x",-b/2+40).attr("y",c/2-30*(e+1)).text(f).style("font-size","15px").attr("alignment-baseline","middle")})}const r=d.append("g").selectAll("line").data(g.links).join("line").attr("class","link").attr("stroke","var(--g-link)").attr("stroke-width",2).attr("data-source",a=>a.source.id).attr("data-target",a=>a.target.id),s=d.append("g").selectAll("g").data(g.nodes).enter().append("g"),q=s.append("circle").attr("class","node").attr("id",a=>a.id).attr("r",b=>{const c=a.links[b.id]?.length||0,d=a.backlinks[b.id]?.length||0;return 3+(c+d)/4}).attr("fill",k).style("cursor","pointer").on("click",(b,a)=>{window.location.href="https://carlgogo.github.io/AIDigitalGarden"+decodeURI(a.id).replace(/\s+/g,'-')}).on("mouseover",function(g,b){d3.selectAll(".node").transition().duration(100).attr("fill","var(--g-node-inactive)");const d=m([...a.links[b.id]||[],...a.backlinks[b.id]||[]]),e=d3.selectAll(".node").filter(a=>d.includes(a.id)),c=b.id,f=d3.selectAll(".link").filter(a=>a.source.id===c||a.target.id===c);e.transition().duration(200).attr("fill",k),f.transition().duration(200).attr("stroke","var(--g-link-active)"),d3.select(this.parentNode).select("text").raise().transition().duration(200).style("opacity",1)}).on("mouseleave",function(d,b){d3.selectAll(".node").transition().duration(200).attr("fill",k);const a=b.id,c=d3.selectAll(".link").filter(b=>b.source.id===a||b.target.id===a);c.transition().duration(200).attr("stroke","var(--g-link)"),d3.select(this.parentNode).select("text").transition().duration(200).style("opacity",0)}).call(l(i)),o=s.append("text").attr("dx",12).attr("dy",".35em").text(a=>u[decodeURI(a.id).replace(/\s+/g,'-')]?.title||"Untitled").style("opacity",0).style("pointer-events","none").call(l(i)),v=!0;v&&d.call(d3.zoom().extent([[0,0],[b,c]]).scaleExtent([.25,4]).on("zoom",({transform:a})=>{r.attr("transform",a),q.attr("transform",a),o.attr("transform",a)})),i.on("tick",()=>{r.attr("x1",a=>a.source.x).attr("y1",a=>a.source.y).attr("x2",a=>a.target.x).attr("y2",a=>a.target.y),q.attr("cx",a=>a.x).attr("cy",a=>a.y),o.attr("x",a=>a.x).attr("y",a=>a.y)})}run()</script></div></div><div id=contact_buttons><footer><p>Made by Carlos Alberto Gomez Gonzalez using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2022</p><a href=/>Home</a>
<a href=https://carlgogo.github.io>Website</a><a href=https://github.com/carlgogo>Github</a></footer></div></div></body></html>