<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Resources  https://en.wikipedia.org/wiki/Residual_neural_network  Training and investigating Residual Nets  References  #PAPER Deep Residual Learning for Image Recognition, Resnet-50 (He 2015) ^resnet  #CODE https://github."><title>Residual and dense neural networks</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=/icon.png><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Source+Sans+Pro:wght@400;600;700&family=Fira+Code:wght@400;700&display=swap" rel=stylesheet><link href=https://carlos-gg.github.io/digitalgarden/styles.fc271f84b56ab4040866b55bab91ea59.min.css rel=stylesheet><script src=https://carlos-gg.github.io/digitalgarden/js/darkmode.46b07878b7f5d9e26ad7a3c40f8a0605.min.js></script><script>const BASE_URL="https://carlos-gg.github.io/digitalgarden/",fetchData=Promise.all([fetch("https://carlos-gg.github.io/digitalgarden/indices/linkIndex.c1f8366dc3550ffa3aa4d48131960e26.min.json").then(a=>a.json()).then(a=>({index:a.index,links:a.links})),fetch("https://carlos-gg.github.io/digitalgarden/indices/contentIndex.412d83a72358f67bfac8b25762e6c05a.min.json").then(a=>a.json())]).then(([{index:a,links:b},c])=>({index:a,links:b,content:c}))</script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-XYFD95KB4J"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-XYFD95KB4J',{anonymize_ip:!1})}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script><script defer src=https://carlos-gg.github.io/digitalgarden/js/search.3c0dba36397221e42c2e229db4303dc2.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://carlos-gg.github.io/digitalgarden/>ü™¥ Quartz 3.1</a></h1><svg tabindex="0" id="search-icon" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg><div class=spacer></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Residual and dense neural networks</h1><p class=meta>Last updated March 17, 2022</p><ul class=tags></ul><aside class=mainTOC><h3>Table of Contents</h3><nav id=TableOfContents><ol><li><a href=#resources>Resources</a></li><li><a href=#references>References</a></li></ol></nav></aside><h2 id=resources>Resources</h2><ul><li><a href=https://en.wikipedia.org/wiki/Residual_neural_network>https://en.wikipedia.org/wiki/Residual_neural_network</a></li><li><a href=http://torch.ch/blog/2016/02/04/resnets.html rel=noopener>Training and investigating Residual Nets</a></li></ul><h2 id=references>References</h2><ul><li>#PAPER
<a href=http://arxiv.org/abs/1512.03385 rel=noopener>Deep Residual Learning for Image Recognition, Resnet-50 (He 2015)</a> ^resnet<ul><li>#CODE <a href=https://github.com/raghakot/keras-resnet>https://github.com/raghakot/keras-resnet</a></li><li><a href=https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf>https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf</a></li><li><a href="https://www.youtube.com/watch?v=GWt6Fu05voI" rel=noopener>Explained paper</a></li><li>Deep convolutional neural networks have led to a series of breakthroughs for image classification. Many other visual recognition tasks have also greatly benefited from very deep models. So, over the years there is a trend to go more deeper, to solve more complex tasks and to also increase /improve the classification/recognition accuracy. But, as we go deeper; the training of neural network becomes difficult and also the accuracy starts saturating and then degrades also. Residual Learning tries to solve both these problems.</li><li>What is Residual Learning?<ul><li>In general, in a deep convolutional neural network, several layers are stacked and are trained to the task at hand. The network learns several low/mid/high level features at the end of its layers. In residual learning, instead of trying to learn some features, we try to learn some residual. Residual can be simply understood as subtraction of feature learned from input of that layer. ResNet does this using shortcut connections (directly connecting input of nth layer to some (n+x)th layer. It has proved that training this form of networks is easier than training simple deep convolutional neural networks and also the problem of degrading accuracy is resolved.</li><li>The architecture is similar to the VGGNet consisting mostly of 3X3 filters. From the VGGNet, shortcut connection as described above is inserted to form a residual network.</li></ul></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1611.05431 rel=noopener>Aggregated Residual Transformations for Deep Neural Networks, ResNeXt (Xie 2016)</a> ^resnext</li><li>#PAPER
<a href=https://arxiv.org/abs/1608.06993 rel=noopener>Densely Connected Convolutional Networks, DenseNet (Huang 2016)</a> ^densenet<ul><li>#CODE <a href=https://github.com/liuzhuang13/DenseNet>https://github.com/liuzhuang13/DenseNet</a></li><li>#BOOK
<a href=https://d2l.ai/chapter_convolutional-modern/densenet.html rel=noopener>DenseNet</a></li><li>For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages such as alleviating the vanishing-gradient problem, strengthening the feature propagation, encouraging feature reuse, and substantially reducing the number of parameters. DenseNets outperformed ResNets whilst requiring less memory and computation to achieve high performance.</li><li><a href=https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803>https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803</a></li><li>In DenseNet, each layer obtains additional inputs from all preceding layers and passes on its own feature-maps to all subsequent layers. Concatenation is used (while element-wise addition for ResNets). Each layer is receiving a ‚Äúcollective knowledge‚Äù from all preceding layers.</li><li><a href=https://arthurdouillard.com/post/densenet/>https://arthurdouillard.com/post/densenet/</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1605.07146 rel=noopener>Wide Residual Networks (Zagoruyko 2016)</a><ul><li>#CODE <a href=https://github.com/szagoruyko/wide-residual-networks>https://github.com/szagoruyko/wide-residual-networks</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1704.06904 rel=noopener>Residual Attention Network for Image Classification (Wang 2017)</a><ul><li>Residual Attention Network, a convolutional neural network using attention mechanism which can incorporate with state-of-art feed forward network architecture in an end-to-end training fashion. The attention residual learning is used to train very deep Residual Attention Networks which can be easily scaled up to hundreds of layers.</li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2110.00476 rel=noopener>ResNet strikes back: An improved training procedure in timm (Wightman 2021)</a><ul><li><a href="https://www.youtube.com/watch?v=Gl0s0GDqN3c" rel=noopener>Paper explained</a></li></ul></li></ul></article><hr><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=https://carlos-gg.github.io/digitalgarden//AI/Computer-Vision/Object-classification-image-recognition>Object classification, image recognition</a></li><li><a href=https://carlos-gg.github.io/digitalgarden//AI/Deep-learning/Deep-learning>Deep Learning (DL)</a></li><li><a href=https://carlos-gg.github.io/digitalgarden//AI/Deep-learning/Neural-ODEs>Neural Ordinary Differential Equations</a></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://carlos-gg.github.io/digitalgarden/js/graph.8aaeaccdaf2f67fa56e0d3e2d059ed5e.js></script><script>drawGraph("https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/Residual-and-dense-neural-networks","https://carlos-gg.github.io/digitalgarden",[{"/moc":"#4388cc"}],-1,!0,!1,!0)</script></div></div><div id=contact_buttons><footer><p>Made by Jacky Zhao using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, ¬© 2022</p><a href=/>Home</a>
<a href=https://twitter.com/_jzhao>Twitter</a><a href=https://github.com/jackyzha0>Github</a></footer></div><script src=https://carlos-gg.github.io/digitalgarden/js/popover.718bae159805a590cda4e9bcb14b9dcc.min.js></script><script>initPopover("https://carlos-gg.github.io/digitalgarden")</script></div></body></html>