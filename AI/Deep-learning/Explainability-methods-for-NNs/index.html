<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Resources   Using ML to Explore Neural Network Architecture  The Building Blocks of Interpretability  Feature Visualization  Applying deep learning to real-world problems (labeled data, imbalance, black box models)  Unblackboxing webinar (deepsense."><title>Explainability methods for NNs</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://carlos-gg.github.io/digitalgarden//icon.png><link href=https://carlos-gg.github.io/digitalgarden/styles.b3e1e36b0403ac565c9392b3e23ef3b6.min.css rel=stylesheet><link href=https://carlos-gg.github.io/digitalgarden/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://carlos-gg.github.io/digitalgarden/js/darkmode.18b7c6dfe67ae3bf2317338cf4189144.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/util.9825137f5e7825e8553c68ce39ac9e44.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/popover.abe6a51cc7138c5dff00f151dd627ad1.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/code-title.b35124ad8db0ba37162b886afb711cbc.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/clipboard.c20857734e53a3fb733b7443879efa61.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const BASE_URL="https://carlos-gg.github.io/digitalgarden/",fetchData=Promise.all([fetch("https://carlos-gg.github.io/digitalgarden/indices/linkIndex.1c608135a6e919b45cca326c94fa1dcf.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://carlos-gg.github.io/digitalgarden/indices/contentIndex.4d5d3ca667f49f3bf2aae96f5bc86ff7.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://carlos-gg.github.io/digitalgarden",!0,!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!0;drawGraph("https://carlos-gg.github.io/digitalgarden",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/carlos-gg.github.io\/digitalgarden\/js\/router.9d4974281069e9ebb189f642ae1e3ca2.min.js"
    attachSPARouting(init, render)
  </script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-S103D50NQ0"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S103D50NQ0",{anonymize_ip:!1})}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://carlos-gg.github.io/digitalgarden/js/full-text-search.24827f874defbbc6d529926cbfcfb493.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://carlos-gg.github.io/digitalgarden/>CarlosGG's Knowledge Garden 🪴</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Explainability methods for NNs</h1><p class=meta>Last updated
Sep 29, 2022
<a href=https://github.com/jackyzha0/quartz/tree/hugo/content/AI/Deep%20learning/Explainability%20methods%20for%20NNs.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details open><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#resources>Resources</a><ol><li><a href=#saliency-maps>Saliency maps</a></li><li><a href=#layer-wise-relevance-propagation-lrp>Layer-wise Relevance Propagation (LRP)</a></li></ol></li><li><a href=#courses>Courses</a></li><li><a href=#code>Code</a></li><li><a href=#references>References</a><ol><li><a href=#layer-wise-relevance-propagation-lrp-1>Layer-wise Relevance Propagation (LRP)</a></li></ol></li></ol></nav></details></aside><a href=#resources><h2 id=resources><span class=hanchor arialabel=Anchor># </span>Resources</h2></a><ul><li><a href=https://research.googleblog.com/2017/05/using-machine-learning-to-explore.html rel=noopener>Using ML to Explore Neural Network Architecture</a></li><li><a href=https://distill.pub/2018/building-blocks/ rel=noopener>The Building Blocks of Interpretability</a></li><li><a href=https://distill.pub/2017/feature-visualization/ rel=noopener>Feature Visualization</a></li><li><a href=https://medium.com/merantix/applying-deep-learning-to-real-world-problems-ba2d86ac5837 rel=noopener>Applying deep learning to real-world problems (labeled data, imbalance, black box models)</a></li><li><a href=https://github.com/deepsense-io/unblackboxing_webinar rel=noopener>Unblackboxing webinar (deepsense.io)</a></li><li><a href=https://www.technologyreview.com/s/604087/the-dark-secret-at-the-heart-of-ai/amp/ rel=noopener>The Dark Secret at the Heart of AI</a></li><li><a href=http://www.sciencemag.org/news/2017/07/how-ai-detectives-are-cracking-open-black-box-deep-learning rel=noopener>How AI detectives are cracking open the black box of deep learning</a></li><li><a href=https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html rel=noopener>Visualization of activations and filters</a><ul><li><a href=https://github.com/jacobgil/keras-filter-visualization rel=noopener>https://github.com/jacobgil/keras-filter-visualization</a></li></ul></li><li><a href=https://towardsdatascience.com/understanding-your-convolution-network-with-visualizations-a4883441533b rel=noopener>https://towardsdatascience.com/understanding-your-convolution-network-with-visualizations-a4883441533b</a></li><li><a href=https://imatge.upc.edu/web/publications/visual-saliency-prediction-using-deep-learning-techniques rel=noopener>https://imatge.upc.edu/web/publications/visual-saliency-prediction-using-deep-learning-techniques</a></li><li><a href=http://www.unofficialgoogledatascience.com/2017/03/attributing-deep-networks-prediction-to.html rel=noopener>Attributing a deep network’s prediction to its input features</a><ul><li>Integrated gradients method</li><li>It involves a few calls to a gradient operator yielding insightful results for a variety of deep networks</li></ul></li></ul><a href=#saliency-maps><h3 id=saliency-maps><span class=hanchor arialabel=Anchor># </span>Saliency maps</h3></a><ul><li><a href=https://en.wikipedia.org/wiki/Saliency_map rel=noopener>https://en.wikipedia.org/wiki/Saliency_map</a></li><li>Saliency map is a broader term from the field of computer vision. The first reference of saliency maps applied to the predictions of DNNs is Morch et al 1995. Simonyan et al (2014) first proposed a method to produce saliency maps using back-propagation through a CNN, but note that you could compute &ldquo;saliency&rdquo; from an image in many ways that do not deal with back-propagating the prediction scores of DNNs.</li><li><a href=https://christophm.github.io/interpretable-ml-book/pixel-attribution.html rel=noopener>Pixel Attribution (Saliency Maps)</a></li></ul><a href=#layer-wise-relevance-propagation-lrp><h3 id=layer-wise-relevance-propagation-lrp><span class=hanchor arialabel=Anchor># </span>Layer-wise Relevance Propagation (LRP)</h3></a><ul><li>LRP is an inverse method which calculates the contribution of a single pixel to the prediction made by a DNN in an image classification task</li><li><a href=http://heatmapping.org/ rel=noopener>http://heatmapping.org/</a></li><li><a href=https://lrpserver.hhi.fraunhofer.de/image-classification rel=noopener>Interactive demo</a></li><li><a href=https://medium.com/@ODSC/layer-wise-relevance-propagation-means-more-interpretable-deep-learning-219ff5158914 rel=noopener>https://medium.com/@ODSC/layer-wise-relevance-propagation-means-more-interpretable-deep-learning-219ff5158914</a></li><li><a href=https://towardsdatascience.com/indepth-layer-wise-relevance-propagation-340f95deb1ea rel=noopener>https://towardsdatascience.com/indepth-layer-wise-relevance-propagation-340f95deb1ea</a></li><li>There are several approaches for calculating attributions by back-propagating the prediction score through each layer of the network, back to the input features /pixels (DeConvNet, SmoothGrad, GradCam, LRP, XRAI). LRP is just one of them. In the first LRP paper, they talk about heatmaps or relevance maps, probably to avoid confusion with older saliency map techniques</li></ul><a href=#courses><h2 id=courses><span class=hanchor arialabel=Anchor># </span>Courses</h2></a><ul><li>#COURSE
<a href=https://interpretablevision.github.io/index_cvpr2020.html rel=noopener>Interpretable Machine Learning for Computer Vision (CVPR 2020)</a></li><li>#COURSE
<a href=https://sites.google.com/view/emnn-ws-2020/home rel=noopener>Explainability Methods for Neural Networks (2021)</a></li></ul><a href=#code><h2 id=code><span class=hanchor arialabel=Anchor># </span>Code</h2></a><ul><li>#CODE
<a href=https://github.com/deel-ai/xplique rel=noopener>Xplique</a><ul><li>Python toolkit dedicated to explainability, currently based on Tensorflow</li><li><a href=https://deel-ai.github.io/xplique/ rel=noopener>https://deel-ai.github.io/xplique/</a></li><li>#PAPER
<a href=https://arxiv.org/pdf/2206.04394v1 rel=noopener>Xplique: A Deep Learning Explainability Toolbox (Fel 2022)</a></li></ul></li><li>#CODE
<a href=https://github.com/understandable-machine-intelligence-lab/Quantus rel=noopener>Quantus</a><ul><li>Quantus is an eXplainable AI toolkit for responsible evaluation of neural network explanations</li></ul></li><li>#CODE
<a href=https://github.com/truera/trulens rel=noopener>TruLens (tf.keras and pytorch): Explainability for Neural Networks</a><ul><li><a href=https://www.trulens.org/ rel=noopener>https://www.trulens.org/</a></li></ul></li><li>#CODE
<a href=https://github.com/pytorch/captum rel=noopener>Captum (pytorch)</a><ul><li>Interpretability of models across modalities including vision, text, and more</li><li><a href=https://captum.ai/ rel=noopener>https://captum.ai/</a></li><li><a href=https://captum.ai/api/ rel=noopener>https://captum.ai/api/</a></li></ul></li><li>#CODE
<a href=https://github.com/ashutosh1919/explainable-cnn rel=noopener>Explainable-cnn</a><ul><li>PyTorch based visualization package for generating layer-wise explanations for CNNs</li></ul></li><li>#CODE
<a href=https://github.com/PAIR-code/saliency rel=noopener>Saliency</a><ul><li>XRAI, SmoothGrad, Vanilla Gradients, Guided Backpropogation, Integrated Gradients, Occlusion, Grad-CAM, Blur IG</li></ul></li><li>#CODE
<a href=https://github.com/albermax/innvestigate rel=noopener>iNNvestigate</a><ul><li>Vanilla gradient, SmoothGrad, DeConvNet, Guided BackProp, PatternNet, DeepTaylor, PatternAttribution, LRP, IntegratedGradients, DeepLIFT</li></ul></li><li>#CODE
<a href=https://github.com/sicara/tf-explain rel=noopener>TF-explain</a><ul><li>implements interpretability methods as Tensorflow 2.x callbacks to ease neural network&rsquo;s understanding</li></ul></li><li>#CODE
<a href=https://github.com/tensorspace-team/tensorspace rel=noopener>TensorSpace (Tensorflow.js)</a><ul><li>Neural network 3D visualization framework</li><li><a href=https://tensorspace.org rel=noopener>https://tensorspace.org</a></li></ul></li><li>#CODE
<a href=https://github.com/tensorflow/lucid rel=noopener>Lucid (Tensorflow 1) - A collection of infrastructure and tools for research in neural network interpretability</a></li><li>#CODE
<a href=https://github.com/keisen/tf-keras-vis rel=noopener>tf-keras-vis</a><ul><li>Neural network visualization toolkit for tf.keras</li><li>Activation Maximization</li><li>Class Activation Maps (GradCAM, GradCAM++, ScoreCAM, Faster-ScoreCAM)</li><li>Saliency Maps (Vanilla Saliency, SmoothGrad)</li></ul></li><li>#CODE
<a href=https://github.com/raghakot/keras-vis rel=noopener>Keras-vis</a><ul><li><a href=https://raghakot.github.io/keras-vis/ rel=noopener>https://raghakot.github.io/keras-vis/</a></li><li>Activation maximization, Saliency maps, Class activation maps</li></ul></li><li>#CODE
<a href=https://github.com/marcoancona/DeepExplain rel=noopener>DeepExplain (TensorFlow 1)</a><ul><li>Saliency maps, Gradient * Input, Integrated Gradients, DeepLIFT, ε-LRP</li></ul></li><li>#CODE
<a href=https://github.com/sebastian-lapuschkin/lrp_toolbox rel=noopener>LRP toolbox</a></li></ul><a href=#references><h2 id=references><span class=hanchor arialabel=Anchor># </span>References</h2></a><ul><li>#PAPER
<a href=https://www.researchgate.net/publication/3623243_Visualization_of_neural_networks_using_saliency_maps rel=noopener>Visualization of neural networks using saliency maps (Morch 1995)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1312.6034 rel=noopener>Deep inside CNNs: Visualising Image Classification Models and Saliency Maps (Simonyan 2014)</a><ul><li>Presented two visualisation techniques for deep classification ConvNets<ul><li>The first generates an artificial image, which is representative of a class of interest</li><li>The second computes an image-specific class saliency map, highlighting the areas of the given image, discriminative wrt the given class</li></ul></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1506.06579 rel=noopener>Understanding Neural Networks Through Deep Visualization (Yosinski 2015)</a><ul><li><a href=http://yosinski.com/deepvis rel=noopener>http://yosinski.com/deepvis</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1706.05806 rel=noopener>SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability (Raghu 2017)</a><ul><li><a href=https://ai.googleblog.com/2017/11/interpreting-deep-neural-networks-with.html rel=noopener>Interpreting Deep Neural Networks with SVCCA</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1703.01365 rel=noopener>Axiomatic Attribution for Deep Networks (Sundararajan 2017)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1706.03825 rel=noopener>SmoothGrad: removing noise by adding noise (Smilkov 2017)</a><ul><li><a href=https://pair-code.github.io/saliency/ rel=noopener>https://pair-code.github.io/saliency/</a></li></ul></li><li>#PAPER
<a href=http://arxiv.org/abs/1808.04260 rel=noopener>iNNvestigate Neural Networks! (Alber 2018)</a><ul><li>#CODE See Code section</li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1906.02825 rel=noopener>XRAI: Better Attributions Through Regions (Kapishnikov 2019)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1704.02685 rel=noopener>DeepLIFT - Learning Important Features Through Propagating Activation Differences (Shrikumar 2019)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1810.03716 rel=noopener>Saliency Prediction in the Deep Learning Era: Successes, Limitations, and Future Challenges (Borji 2019)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/2012.05766 rel=noopener>DAX: Deep Argumentative eXplanation for Neural Networks (Albini 2020)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1901.09839 rel=noopener>Interpreting Deep Neural Networks Through Variable Importance (Ish-Horowicz 2020)</a><ul><li>Their strategy is specifically designed to leverage partial covariance structures and incorporate variable interactions into our proposed feature ranking.</li><li>Extended the recently proposed “RelATive cEntrality” (RATE) measure (Crawford et al., 2019) to the Bayesian deep learning setting</li><li>Given a trained network, RATE applies an information theoretic criterion to the posterior distribution of effect sizes to assess feature significance</li></ul></li><li>#PAPER
<a href=https://link.springer.com/chapter/10.1007%2F978-3-030-58574-7_20 rel=noopener>Determining the Relevance of Features for Deep Neural Networks (Reimers 2020)</a><ul><li>Their approach builds upon concepts from causal inference</li><li>Interpret machine learning in a structural causal model and use Reichenbach’s common cause principle to infer whether a feature is relevant</li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2005.13799 rel=noopener>Explainable Deep Learning Models in Medical Image Analysis (Singh 2020)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1911.11293 rel=noopener>Efficient Saliency Maps for Explainable AI (Mundhenk 2020)</a></li><li>#PAPER
<a href=https://ieeexplore.ieee.org/document/9369420 rel=noopener>Explaining Deep Neural Networks and Beyond: A Review of Methods and Applications (Samek 2021)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/2108.05149 rel=noopener>Logic Explained Networks (Ciravegna 2021)</a><ul><li><a href=https://syncedreview.com/2021/08/18/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-85/ rel=noopener>https://syncedreview.com/2021/08/18/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-85/</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2112.11407 rel=noopener>Toward Explainable AI for Regression Models (Letzgus 2021)</a></li><li>#PAPER
<a href=https://explaining-in-style.github.io/ rel=noopener>Explaining in Style: Training a GAN to explain a classifier in StyleSpace (Lang 2021)</a><ul><li><a href=https://ai.googleblog.com/2022/01/introducing-stylex-new-approach-for.html rel=noopener>https://ai.googleblog.com/2022/01/introducing-stylex-new-approach-for.html</a></li></ul></li><li>#PAPER
<a href=https://www.nature.com/articles/s42256-021-00308-z rel=noopener>Variable selection with false discovery rate control in deep neural networks (Song 2021)</a><ul><li><a href=https://techxplore.com/news/2021-05-survnet-procedure-variable-deep-neural.html rel=noopener>SurvNet: A backward elimination procedure to enhance variable selection for deep neural networks</a></li></ul></li></ul><a href=#layer-wise-relevance-propagation-lrp-1><h3 id=layer-wise-relevance-propagation-lrp-1><span class=hanchor arialabel=Anchor># </span>Layer-wise Relevance Propagation (LRP)</h3></a><ul><li>#PAPER
<a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140" rel=noopener>On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation (Bach 2015)</a><ul><li>#CODE
<a href=https://git.tu-berlin.de/gmontavon/lrp-tutorial rel=noopener>Tutorial: Implementing Layer-Wise Relevance Propagation</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1812.02100 rel=noopener>Understanding Individual Decisions of CNNs via Contrastive Backpropagation (Gu 2019)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1712.08268 rel=noopener>Beyond saliency: understanding convolutional neural networks from saliency prediction on layer-wise relevance propagation (Li 2019)</a><ul><li>proposed a novel two-step understanding method, namely Salient Relevance (SR) map, which aims to shed light on how deep CNNs recognize images and learn features from attention areas</li><li>starts out with a layer-wise relevance propagation (LRP) step which estimates a pixel-wise relevance map over the input image. Following, we construct a context-aware saliency map, SR map, from the LRP-generated map which predicts areas close to the foci of attention instead of isolated pixels that LRP reveals</li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1910.09840 rel=noopener>Towards Best Practice in Explaining Neural Network Decisions with LRP (Kohlbrenner 2020)</a></li></ul></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/digitalgarden/AI/AI/ data-ctx="AI/Deep learning/Explainability methods for NNs" data-src=/AI/AI class=internal-link>Artificial Intelligence</a></li><li><a href=/digitalgarden/AI/Deep-learning/DL/ data-ctx="AI/Deep learning/Explainability methods for NNs" data-src=/AI/Deep-learning/DL class=internal-link>Deep Learning (DL)</a></li><li><a href=/digitalgarden/AI/XAI/ data-ctx="AI/Deep learning/Explainability methods for NNs" data-src=/AI/XAI class=internal-link>XAI</a></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://carlos-gg.github.io/digitalgarden/js/graph.abd4bc2af3869a96524d7d23b76152c7.js></script></div></div><div id=contact_buttons><footer><p>Made by Carlos Alberto Gomez Gonzalez using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2023</p><ul><li><a href=https://carlos-gg.github.io/digitalgarden/>Home</a></li><li><a href=https://carlos-gg.github.io>Carlos'Homepage</a></li></ul></footer></div></div></body></html>