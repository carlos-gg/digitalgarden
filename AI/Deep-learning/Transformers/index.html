<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Resources   https://github.com/IDEACVR/awesome-detection-transformer  https://github.com/Yangzhangcst/Transformer-in-Computer-Vision  https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/ (from RNNs with attention to Transformers)  A Complete Learning Path To Transformers (With Guide To 23 Architectures)  Investigating Vision Transformer representations  #CODE https://github."><title>Transformers</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://carlos-gg.github.io/digitalgarden//icon.png><link href=https://carlos-gg.github.io/digitalgarden/styles.b3e1e36b0403ac565c9392b3e23ef3b6.min.css rel=stylesheet><link href=https://carlos-gg.github.io/digitalgarden/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://carlos-gg.github.io/digitalgarden/js/darkmode.18b7c6dfe67ae3bf2317338cf4189144.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/util.9825137f5e7825e8553c68ce39ac9e44.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/popover.abe6a51cc7138c5dff00f151dd627ad1.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/code-title.b35124ad8db0ba37162b886afb711cbc.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/clipboard.c20857734e53a3fb733b7443879efa61.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const BASE_URL="https://carlos-gg.github.io/digitalgarden/",fetchData=Promise.all([fetch("https://carlos-gg.github.io/digitalgarden/indices/linkIndex.1c608135a6e919b45cca326c94fa1dcf.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://carlos-gg.github.io/digitalgarden/indices/contentIndex.4d5d3ca667f49f3bf2aae96f5bc86ff7.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://carlos-gg.github.io/digitalgarden",!0,!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!0;drawGraph("https://carlos-gg.github.io/digitalgarden",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/carlos-gg.github.io\/digitalgarden\/js\/router.9d4974281069e9ebb189f642ae1e3ca2.min.js"
    attachSPARouting(init, render)
  </script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-S103D50NQ0"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S103D50NQ0",{anonymize_ip:!1})}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://carlos-gg.github.io/digitalgarden/js/full-text-search.24827f874defbbc6d529926cbfcfb493.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://carlos-gg.github.io/digitalgarden/>CarlosGG's Knowledge Garden 🪴</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Transformers</h1><p class=meta>Last updated
Oct 4, 2022
<a href=https://github.com/jackyzha0/quartz/tree/hugo/content/AI/Deep%20learning/Transformers.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details open><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#resources>Resources</a></li><li><a href=#courses>Courses</a></li><li><a href=#code>Code</a></li><li><a href=#references>References</a><ol><li><a href=#for-nlp>For NLP</a></li><li><a href=#for-computer-vision>For Computer Vision</a><ol><li><a href=#self-supervised-vision-transformers>Self-supervised vision transformers</a></li><li><a href=#vision-transformers-with-convolutions>Vision transformers with convolutions</a></li></ol></li><li><a href=#multi-modal-transformers>Multi-modal transformers</a></li><li><a href=#for-rl>For RL</a></li></ol></li></ol></nav></details></aside><a href=#resources><h2 id=resources><span class=hanchor arialabel=Anchor># </span>Resources</h2></a><ul><li><a href=https://github.com/IDEACVR/awesome-detection-transformer rel=noopener>https://github.com/IDEACVR/awesome-detection-transformer</a></li><li><a href=https://github.com/Yangzhangcst/Transformer-in-Computer-Vision rel=noopener>https://github.com/Yangzhangcst/Transformer-in-Computer-Vision</a></li><li><a href=https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/ rel=noopener>https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/</a> (from RNNs with attention to Transformers)</li><li><a href=https://analyticsindiamag.com/a-complete-learning-path-to-transformers/ rel=noopener>A Complete Learning Path To Transformers (With Guide To 23 Architectures)</a></li><li><a href=https://keras.io/examples/vision/probing_vits/ rel=noopener>Investigating Vision Transformer representations</a><ul><li>#CODE
<a href=https://github.com/sayakpaul/probing-vits rel=noopener>https://github.com/sayakpaul/probing-vits</a></li></ul></li><li><a href=https://towardsdatascience.com/self-supervised-learning-in-vision-transformers-30ff9be928c rel=noopener>Self-Supervised Learning in Vision Transformers</a></li><li><a href=https://pyimagesearch.com/2022/09/05/a-deep-dive-into-transformers-with-tensorflow-and-keras-part-1/ rel=noopener>A Deep Dive into Transformers with TensorFlow and Keras</a></li></ul><a href=#courses><h2 id=courses><span class=hanchor arialabel=Anchor># </span>Courses</h2></a><ul><li>#COURSE
<a href=https://web.stanford.edu/class/cs25/ rel=noopener>CS25: Transformers United (Stanford)</a><ul><li><a href="https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM" rel=noopener>Videos</a></li></ul></li></ul><a href=#code><h2 id=code><span class=hanchor arialabel=Anchor># </span>Code</h2></a><ul><li>#CODE
<a href=https://github.com/huggingface/transformers rel=noopener>Transformers</a><ul><li>thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio</li><li>JAX, PyTorch and TensorFlow</li></ul></li><li>#CODE
<a href=https://github.com/facebookresearch/xformers rel=noopener>Xformers</a></li><li>#CODE
<a href=https://github.com/IbrahimSobh/Transformers rel=noopener>Transformers: from NLP to CV</a></li><li>#CODE
<a href=https://github.com/google-research/big_vision rel=noopener>Big vision</a><ul><li>This codebase is designed for training large-scale vision models on Cloud TPU VMs. It is based on Jax/Flax libraries, and uses tf.data and TensorFlow Datasets for scalable input pipelines in the Cloud</li></ul></li></ul><a href=#references><h2 id=references><span class=hanchor arialabel=Anchor># </span>References</h2></a><a href=#for-nlp><h3 id=for-nlp><span class=hanchor arialabel=Anchor># </span>For NLP</h3></a><ul><li>#PAPER
<a href=https://arxiv.org/abs/1706.03762 rel=noopener>Attention is all you need (Vaswani 2017)</a><ul><li><a href=https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html rel=noopener>https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html</a></li><li><a href="https://www.youtube.com/watch?v=iDulhoQ2pro" rel=noopener>Paper explained</a></li><li>The Transformer is a novel neural network architecture based on a self-attention mechanism that is well suited for language understanding.</li><li>It outperforms both recurrent and convolutional models on academic English to German and English to French translation benchmarks. On top of higher translation quality, the Transformer requires less computation to train and is a much better fit for modern machine learning hardware, speeding up training by up to an order of magnitude.</li><li>Self-attention is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing. If you’re familiar with RNNs, think of how maintaining a hidden state allows an RNN to incorporate its representation of previous words/vectors it has processed with the current one it’s processing.</li><li>The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.</li><li><a href=http://jalammar.github.io/illustrated-transformer/ rel=noopener>http://jalammar.github.io/illustrated-transformer/</a></li><li><a href="https://www.youtube.com/watch?v=rBCqOTEfxvg" rel=noopener>Attention is all you need, attentional neural network models (Łukasz Kaiser)</a></li><li><a href=https://sea-adl.org/2019/12/03/lstm-is-dead-long-live-transformers/ rel=noopener>LSTM is dead, long live Transformers</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1803.07416 rel=noopener>Tensor2tensor (Vaswani 2018)</a><ul><li>Tensor2Tensor, or T2T for short, is a library of deep learning models and datasets designed to make deep learning more accessible and accelerate ML research. T2T is actively used and maintained by researchers and engineers within the Google Brain team and a community of users. We&rsquo;re eager to collaborate with you too, so feel free to open an issue on GitHub or send along a pull request (see our contribution doc). You can chat with us on Gitter and join the T2T Google Group.</li><li>It includes the reference implementation of the state-of-the-art Transformer model.</li></ul></li><li>#PAPER
<a href=https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf rel=noopener>Improving Language Understanding by Generative Pre-Training, GPT (Radford 2018)</a><ul><li><a href=https://openai.com/blog/language-unsupervised/ rel=noopener>https://openai.com/blog/language-unsupervised/</a></li></ul></li><li>#PAPER
<a href=https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf rel=noopener>Language Models are Unsupervised Multitask Learners, GPT-2 (Radford 2018)</a><ul><li>#CODE
<a href=https://github.com/openai/gpt-2 rel=noopener>https://github.com/openai/gpt-2</a></li><li><a href=https://openai.com/blog/better-language-models/ rel=noopener>https://openai.com/blog/better-language-models/</a></li><li>Paper explained<ul><li><a href="https://www.youtube.com/watch?v=u1_qMdb0kYU" rel=noopener>https://www.youtube.com/watch?v=u1_qMdb0kYU</a></li><li><a href="https://www.youtube.com/watch?v=UULqu7LQoHs" rel=noopener>https://www.youtube.com/watch?v=UULqu7LQoHs</a></li><li><a href="https://www.youtube.com/watch?v=8ypnLjwpzK8" rel=noopener>https://www.youtube.com/watch?v=8ypnLjwpzK8</a></li></ul></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1810.04805 rel=noopener>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin 2019)</a><ul><li>#CODE
<a href=https://github.com/google-research/bert rel=noopener>TensorFlow code and pre-trained models for BERT</a></li><li><a href="https://www.youtube.com/watch?v=-9evrZnBorM" rel=noopener>Paper explained</a></li><li><a href=https://github.com/hanxiao/bert-as-service rel=noopener>BERT as a service</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2005.14165 rel=noopener>Language Models are Few-Shot Learners, GPT-3 (Brown 2020)</a><ul><li>Paper explained:<ul><li><a href="https://www.youtube.com/watch?v=SY5PvZrJhLE" rel=noopener>https://www.youtube.com/watch?v=SY5PvZrJhLE</a></li><li><a href="https://www.youtube.com/watch?v=_x9AwxfjxvE" rel=noopener>https://www.youtube.com/watch?v=_x9AwxfjxvE</a></li></ul></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2009.07118 rel=noopener>It&rsquo;s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners (Schick 2020)</a><ul><li>#CODE
<a href=https://github.com/timoschick/pet rel=noopener>https://github.com/timoschick/pet</a></li><li><a href=https://www.infoq.com/news/2020/10/training-exceeds-gpt3/ rel=noopener>https://www.infoq.com/news/2020/10/training-exceeds-gpt3/</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2009.14794 rel=noopener>Rethinking Attention with Performers (Choromanski 2020)</a><ul><li><a href=https://syncedreview.com/2020/10/02/google-cambridge-deepmind-alan-turing-institutes-performer-transformer-slashes-compute-costs/ rel=noopener>https://syncedreview.com/2020/10/02/google-cambridge-deepmind-alan-turing-institutes-performer-transformer-slashes-compute-costs/</a></li><li><a href=https://ai.googleblog.com/2020/10/rethinking-attention-with-performers.html rel=noopener>https://ai.googleblog.com/2020/10/rethinking-attention-with-performers.html</a></li><li>#CODE
<a href=https://github.com/google-research/google-research/tree/master/performer/fast_self_attention rel=noopener>https://github.com/google-research/google-research/tree/master/performer/fast_self_attention</a></li><li><a href="https://www.youtube.com/watch?v=xJrKIPwVwGM" rel=noopener>Paper explained</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2006.11316 rel=noopener>SqueezeBERT: What can computer vision teach NLP about efficient neural networks? (Iandola 2020)</a><ul><li>#TALK
<a href="https://www.youtube.com/watch?v=kPMaEYSywdI" rel=noopener>From SqueezeNet to SqueezeBERT: Developing Efficient Deep Neural Networks</a></li><li><a href=https://www.microsoft.com/en-us/research/video/from-squeezenet-to-squeezebert-developing-efficient-deep-neural-networks/ rel=noopener>https://www.microsoft.com/en-us/research/video/from-squeezenet-to-squeezebert-developing-efficient-deep-neural-networks/</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2105.03824 rel=noopener>FNet: Mixing Tokens with Fourier Transforms (Lee-Thorp 2021)</a><ul><li><a href=https://syncedreview.com/2021/05/14/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-19/ rel=noopener>https://syncedreview.com/2021/05/14/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-19/</a></li><li>#CODE
<a href="https://paperswithcode.com/paper/fnet-mixing-tokens-with-fourier-transforms?from=n10" rel=noopener>https://paperswithcode.com/paper/fnet-mixing-tokens-with-fourier-transforms?from=n10</a></li><li>Transformer architectures can be massively sped up, with limited accuracy costs, by replacing self-attention sublayers with linear transformations that &ldquo;mix&rdquo; input tokens</li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2012.15355 rel=noopener>Optimizing Deeper Transformers on Small Datasets (Xu 2021)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/2109.00301 rel=noopener>Infinity-former: Infinite Memory Transformer (Martins 2022)</a><ul><li><a href="https://www.youtube.com/watch?v=0JlB9gufTw8" rel=noopener>Paper explained</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2112.11446 rel=noopener>Scaling Language Models: Methods, Analysis & Insights from Training Gopher (Rae 2022)</a><ul><li><a href="https://www.youtube.com/watch?v=aPiHhJjN3hI" rel=noopener>Paper explained</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/pdf/2201.08239 rel=noopener>LaMDA: Language Models for Dialog Applications (Thoppilan 2022)</a></li><li>#PAPER
<a href=https://cdn.openai.com/papers/whisper.pdf rel=noopener>Whisper - Robust Speech Recognition via Large-Scale Weak Supervision (Radford 2022)</a><ul><li>#CODE
<a href=https://github.com/openai/whisper rel=noopener>https://github.com/openai/whisper</a></li><li><a href="https://www.youtube.com/watch?v=AwJf8aQfChE" rel=noopener>Paper explained</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/pdf/2105.03824 rel=noopener>FNet: Mixing Tokens with Fourier Transforms (Lee-Thorp 2022)</a><ul><li><a href="https://www.youtube.com/watch?v=JJR3pBl78zw" rel=noopener>Paper explained</a></li></ul></li></ul><a href=#for-computer-vision><h3 id=for-computer-vision><span class=hanchor arialabel=Anchor># </span>For Computer Vision</h3></a><ul><li>#PAPER
<a href=https://arxiv.org/abs/1506.02025 rel=noopener>Spatial Transformer Networks (Jaderberg 2016)</a><ul><li>the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, result-ing in state-of-the-art performance on several benchmarks, and for a number of classes of transformations</li><li><a href="https://www.youtube.com/watch?v=6NnearestOQC_fl1hQ" rel=noopener>https://www.youtube.com/watch?v=6NnearestOQC_fl1hQ</a></li><li>#CODE
<a href=https://github.com/oarriaga/paz/tree/master/examples/spatial_transfomer_networks rel=noopener>https://github.com/oarriaga/paz/tree/master/examples/spatial_transfomer_networks</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1802.05751 rel=noopener>Image Transformer (Parmar 2018)</a></li><li>#PAPER
<a href=https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf rel=noopener>Generative Pretraining from Pixels (Chen 2020)</a> ^imagegpt<ul><li><a href=https://openai.com/blog/image-gpt/ rel=noopener>https://openai.com/blog/image-gpt/</a></li><li>#CODE
<a href=https://github.com/openai/image-gpt rel=noopener>https://github.com/openai/image-gpt</a></li><li><a href="https://www.youtube.com/watch?v=YBlNQK0Ao6g" rel=noopener>Paper explained</a></li><li><a href="https://www.youtube.com/watch?v=7rFLnQdl22c" rel=noopener>https://www.youtube.com/watch?v=7rFLnQdl22c</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2005.12872 rel=noopener>DETR - End-to-End Object Detection with Transformers (Carion 2020)</a><ul><li>#CODE
<a href=https://paperswithcode.com/paper/end-to-end-object-detection-with-transformers rel=noopener>https://paperswithcode.com/paper/end-to-end-object-detection-with-transformers</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2012.09841v1 rel=noopener>Taming Transformers for High-Resolution Image Synthesis (Esser 2020)</a> ^tamingtransformers<ul><li><a href=https://compvis.github.io/taming-transformers/ rel=noopener>https://compvis.github.io/taming-transformers/</a></li><li><a href=https://github.com/CompVis/taming-transformers rel=noopener>https://github.com/CompVis/taming-transformers</a></li><li><a href=https://www.marktechpost.com/2020/12/28/a-new-method-to-code-inductive-image-biases-into-models-using-cnn-and-transformers/ rel=noopener>https://www.marktechpost.com/2020/12/28/a-new-method-to-code-inductive-image-biases-into-models-using-cnn-and-transformers/</a></li></ul></li><li>#PAPER
<a href="https://openreview.net/forum?id=YicbFdNTTy" rel=noopener>ViT - An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (2020)</a><ul><li>While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer can perform very well on image classification tasks when applied directly to sequences of image patches</li><li><a href="https://www.youtube.com/watch?v=TrdevFK_am4" rel=noopener>Paper explained</a></li><li>#CODE
<a href=https://github.com/google-research/vision_transformer rel=noopener>https://github.com/google-research/vision_transformer</a></li><li>#CODE
<a href=https://keras.io/examples/vision/image_classification_with_vision_transformer/ rel=noopener>https://keras.io/examples/vision/image_classification_with_vision_transformer/</a></li><li>#CODE
<a href=https://github.com/ashishpatel26/Vision-Transformer-Keras-Tensorflow-Pytorch-Examples rel=noopener>https://github.com/ashishpatel26/Vision-Transformer-Keras-Tensorflow-Pytorch-Examples</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2012.12877 rel=noopener>Training data-efficient image transformers & distillation through attention (Touvron 2021)</a><ul><li>#CODE
<a href=https://github.com/facebookresearch/deit rel=noopener>https://github.com/facebookresearch/deit</a></li><li>Propose a competitive convolution-free transformer by training on Imagenet only</li><li>Introduced a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention</li><li><a href=https://ai.facebook.com/blog/data-efficient-image-transformers-a-promising-new-technique-for-image-classification/ rel=noopener>https://ai.facebook.com/blog/data-efficient-image-transformers-a-promising-new-technique-for-image-classification/</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2102.12122 rel=noopener>PVT - Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions (Wang 2021)</a> ^pvt<ul><li>#CODE
<a href=https://paperswithcode.com/paper/pyramid-vision-transformer-a-versatile rel=noopener>https://paperswithcode.com/paper/pyramid-vision-transformer-a-versatile</a></li><li>#CODE
<a href=https://github.com/wangermeng2021/PVT-tensorflow2 rel=noopener>https://github.com/wangermeng2021/PVT-tensorflow2</a></li><li>PVT inherits the advantages from both CNN and Transformer, making it a unified backbone in various vision tasks without convolutions by simply replacing CNN backbones</li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2103.13413v1 rel=noopener>Vision Transformers for Dense Prediction (Ranftl 2021)</a><ul><li>#CODE
<a href=https://paperswithcode.com/paper/vision-transformers-for-dense-prediction rel=noopener>https://paperswithcode.com/paper/vision-transformers-for-dense-prediction</a></li><li>Model with an encoder-decoder design, leveraging the vision transformer (ViT) as the building block of the encoder</li><li>The representations produced by the transformer are reassembled into image-like feature representations at various resolutions and are progressively combined into the final dense prediction using a convolutional decoder</li><li>The transformer downsamples operations and keeps a representation with a constant dimensionality throughout the processing stages while keeping a global receptive field at every stage</li><li>These properties allows DPT to provide fine-grained and globally coherent predictions as compared to fully-convolutional networks</li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2103.14586v1 rel=noopener>Understanding Robustness of Transformers for Image Classification (Bhojanapalli 2021)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/2102.10662 rel=noopener>Medical Transformer: Gated Axial-Attention for Medical Image Segmentation (Valanarasu 2021)</a><ul><li>#CODE
<a href=https://github.com/jeya-maria-jose/Medical-Transformer rel=noopener>https://github.com/jeya-maria-jose/Medical-Transformer</a></li><li><a href=https://analyticsindiamag.com/guide-to-medical-transformer-attention-for-medical-image-segmentation/ rel=noopener>https://analyticsindiamag.com/guide-to-medical-transformer-attention-for-medical-image-segmentation/</a></li><li>Trains with less data thanks to the Gated Axial-Attention model which extends the existing architectures by introducing an additional control mechanism in the self-attention module</li><li>To train the model effectively on medical images, we propose a Local-Global training strategy (LoGo) which further improves the performance. Specifically, we operate on the whole image and patches to learn global and local features, respectively</li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2102.07074v2 rel=noopener>TransGAN: Two Transformers Can Make One Strong GAN (Jiang 2021)</a><ul><li>#CODE
<a href=https://paperswithcode.com/paper/transgan-two-transformers-can-make-one-strong rel=noopener>https://paperswithcode.com/paper/transgan-two-transformers-can-make-one-strong</a></li><li>first pilot study in building a GAN completely free of convolutions, using only pure transformer-based architectures</li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2103.01209v2 rel=noopener>Gansformer - Generative Adversarial Transformers (Hudson 2021)</a><ul><li>#CODE
<a href=https://paperswithcode.com/paper/generative-adversarial-transformers rel=noopener>https://paperswithcode.com/paper/generative-adversarial-transformers</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2103.14030 rel=noopener>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows (Liu 2021)</a><ul><li>#CODE
<a href=https://github.com/microsoft/Swin-Transformer rel=noopener>https://github.com/microsoft/Swin-Transformer</a></li><li>#CODE
<a href=https://github.com/rishigami/Swin-Transformer-TF rel=noopener>https://github.com/rishigami/Swin-Transformer-TF</a></li><li>#CODE
<a href=https://github.com/yingkaisha/keras-vision-transformer rel=noopener>https://github.com/yingkaisha/keras-vision-transformer</a></li><li>Swin Transformer serves as a general-purpose backbone for computer vision. Works for tasks such as image classification, object detection and semantic segmentation</li><li>involves a hierarchical Transformer whose representation is computed through a shifted windowing mechanism which limits the self-attention computation to non-overlapping local windows while still allowing for cross-window connection</li><li>the benefits of this hierarchical architecture are greater efficiency and flexibility to model at various scales. In addition, this model has linear computational complexity with respect to image size</li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2106.10270v1 rel=noopener>How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers (Steiner 2021)</a><ul><li>Results show that models using a combination of AugReg (model regularization) and increased compute can attain similar performance as models trained on an order of magnitude more training data</li><li>ViT models of various sizes, trained on ImageNet-21k, match or outperform counterparts trained on a larger dataset (JFT-300M)</li><li>#CODE
<a href=https://paperswithcode.com/paper/how-to-train-your-vit-data-augmentation-and rel=noopener>https://paperswithcode.com/paper/how-to-train-your-vit-data-augmentation-and</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2106.02584v1 rel=noopener>Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning (Kossen 2021)</a><ul><li>#CODE
<a href="https://paperswithcode.com/paper/self-attention-between-datapoints-going?from=n11" rel=noopener>https://paperswithcode.com/paper/self-attention-between-datapoints-going?from=n11</a></li><li>Authors challenge a common assumption underlying most supervised deep learning: that a model makes a prediction depending only on its parameters and the features of a single input</li><li>Introduced a general-purpose deep learning architecture that takes as input the entire dataset instead of processing one datapoint at a time</li><li>The approach uses self-attention to reason about relationships between datapoints explicitly, which can be seen as realizing non-parametric models using parametric attention mechanisms</li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2105.05633 rel=noopener>Segmenter: Transformer for Semantic Segmentation (Strudel 2021)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/2107.00641 rel=noopener>Focal Self-attention for Local-Global Interactions in Vision Transformers (Yang 2021)</a><ul><li><a href=https://www.marktechpost.com/2021/08/24/microsoft-ai-open-source-the-code-for-its-focal-transformer/ rel=noopener>https://www.marktechpost.com/2021/08/24/microsoft-ai-open-source-the-code-for-its-focal-transformer/</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2108.08810 rel=noopener>Do Vision Transformers See Like Convolutional Neural Networks (Raghu 2021)</a><ul><li><a href="https://www.youtube.com/watch?v=rk9bhIRInC0" rel=noopener>Paper explained</a></li></ul></li><li>#PAPER
<a href=https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00538-8 rel=noopener>DECIMER 1.0: deep learning for chemical image recognition using transformers (Rajan 2021)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/2104.11227 rel=noopener>Multiscale Vision Transformers (Fan 2021)</a><ul><li>#CODE
<a href=https://github.com/facebookresearch/SlowFast rel=noopener>https://github.com/facebookresearch/SlowFast</a></li><li><a href=https://ai.facebook.com/blog/multiscale-vision-transformers-an-architecture-for-modeling-visual-data/ rel=noopener>https://ai.facebook.com/blog/multiscale-vision-transformers-an-architecture-for-modeling-visual-data/</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2111.09883v1 rel=noopener>Swin Transformer V2: Scaling Up Capacity and Resolution (Liu 2021)</a><ul><li>#CODE
<a href=https://paperswithcode.com/paper/swin-transformer-v2-scaling-up-capacity-and rel=noopener>https://paperswithcode.com/paper/swin-transformer-v2-scaling-up-capacity-and</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2201.09873v1 rel=noopener>Transformers in Medical Imaging: A Survey (Shamshad 2022)</a><ul><li>#CODE
<a href="https://paperswithcode.com/paper/transformers-in-medical-imaging-a-survey?from=n24" rel=noopener>https://paperswithcode.com/paper/transformers-in-medical-imaging-a-survey?from=n24</a></li><li><a href=https://github.com/fahadshamshad/awesome-transformers-in-medical-imaging rel=noopener>https://github.com/fahadshamshad/awesome-transformers-in-medical-imaging</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2202.06709v2 rel=noopener>How Do Vision Transformers Work? (Park 2022)</a><ul><li>#CODE
<a href="https://paperswithcode.com/paper/how-do-vision-transformers-work-1?from=n26" rel=noopener>https://paperswithcode.com/paper/how-do-vision-transformers-work-1?from=n26</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/pdf/2206.01191v3 rel=noopener>EfficientFormer: Vision Transformers at MobileNet Speed (Li 2022)</a><ul><li>#CODE
<a href=https://github.com/snap-research/efficientformer rel=noopener>https://github.com/snap-research/efficientformer</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/pdf/2103.13413 rel=noopener>Vision Transformers for Dense Prediction (Ranftl 2022)</a> ^dpt<ul><li>#CODE
<a href=https://github.com/isl-org/DPT rel=noopener>https://github.com/isl-org/DPT</a></li><li>DPT is a dense prediction architecture that is based on an encoder-decoder design that leverages a transformer as the basic computational building block of the encoder</li><li>used the recently proposed VIT as a backbone architecture reassembling the bag-of-words representation that is provided by ViT into image-like feature representations at various resolutions and progressively combine the feature representations into the final dense prediction using a convolutional decoder</li><li>it has a global receptive field at every stage</li></ul></li><li>#PAPER
<a href=https://proceedings.neurips.cc/paper/2021/hash/3bbfdde8842a5c44a0323518eec97cbe-Abstract.html rel=noopener>HRFormer: High-Resolution Vision Transformer for Dense Predict (Yuan 2021)</a><ul><li>#CODE
<a href=https://github.com/HRNet/HRFormer rel=noopener>https://github.com/HRNet/HRFormer</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/pdf/2203.16015 rel=noopener>ITTR: Unpaired Image-to-Image Translation with Transformers (Zheng 2022)</a></li></ul><a href=#self-supervised-vision-transformers><h4 id=self-supervised-vision-transformers><span class=hanchor arialabel=Anchor># </span>Self-supervised vision transformers</h4></a><ul><li>#PAPER
<a href=https://arxiv.org/abs/2104.03602 rel=noopener>SiT: Self-supervised vIsion Transformer (Atito 2021)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/2104.14294 rel=noopener>DINO - Emerging Properties in Self-Supervised Vision Transformers (Caron 2021)</a><ul><li><a href=https://towardsdatascience.com/on-dino-self-distillation-with-no-labels-c29e9365e382 rel=noopener>https://towardsdatascience.com/on-dino-self-distillation-with-no-labels-c29e9365e382</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2205.03892v2 rel=noopener>ConvMAE: Masked Convolution Meets Masked Autoencoders (Gao 2022)</a><ul><li>#CODE
<a href=https://github.com/Alpha-VL/ConvMAE rel=noopener>https://github.com/Alpha-VL/ConvMAE</a></li></ul></li></ul><a href=#vision-transformers-with-convolutions><h4 id=vision-transformers-with-convolutions><span class=hanchor arialabel=Anchor># </span>Vision transformers with convolutions</h4></a><ul><li>#PAPER
<a href=https://arxiv.org/abs/2103.11816v1 rel=noopener>CeiT - Incorporating Convolution Designs into Visual Transformers (Yan 2021)</a><ul><li>#CODE
<a href=https://paperswithcode.com/paper/incorporating-convolution-designs-into-visual rel=noopener>https://paperswithcode.com/paper/incorporating-convolution-designs-into-visual</a></li><li>CeiT combines the advantages of CNNs in extracting low-level features, strengthening locality, and the advantages of Transformers in establishing long-range dependencies</li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2102.04306v1 rel=noopener>TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation (Chen 2021)</a><ul><li>#CODE
<a href=https://paperswithcode.com/paper/transunet-transformers-make-strong-encoders rel=noopener>https://paperswithcode.com/paper/transunet-transformers-make-strong-encoders</a></li><li>due to the intrinsic locality of convolution operations, U-Net generally demonstrates limitations in explicitly modeling long-range dependency</li><li>TransUNet merits both Transformers and U-Net, as a strong alternative for medical image segmentation</li><li>transformer encodes tokenized image patches from a convolution neural network (CNN) feature map as the input sequence for extracting global contexts</li><li>on the other hand, the decoder upsamples the encoded features which are then combined with the high-resolution CNN feature maps to enable precise localization</li><li>#TALK
<a href="https://www.youtube.com/watch?v=jKBJITQ8xJY" rel=noopener>Paper explained</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2104.05704 rel=noopener>Escaping the Big Data Paradigm with Compact Transformers (Hassani 2021)</a> ^cctransformer<ul><li>Compact Convolutional Transformer (CCT)</li><li>#CODE
<a href=https://github.com/SHI-Labs/Compact-Transformers rel=noopener>https://github.com/SHI-Labs/Compact-Transformers</a></li><li>#CODE
<a href=https://keras.io/examples/vision/cct/ rel=noopener>https://keras.io/examples/vision/cct/</a></li><li>ViTs (or a typical Transformer-based architecture) do not have well-informed inductive biases (such as convolutions for processing images)</li><li>Attempt to combine the benefits of convolution and the benefits of Transformers in a single network architecture</li><li>These benefits include parameter-efficiency, and self-attention to process long-range and global dependencies (interactions between different regions in an image)</li><li>Patching (VIT-like) and embedding is already a convolution in itself, albeit non-overlapping. This is replaced with overlapping (regular) convolutions</li><li>Better performance on CIFAR-10 than VIT, so it&rsquo;s efficient on smaller datasets, comparable with SOTA CNNs</li><li>#TALK
<a href="https://www.youtube.com/watch?v=AEWhf_hMBgs" rel=noopener>Escaping the Big Data Paradigm with Compact Transformers (Humphrey Shi)</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2103.15808 rel=noopener>CvT: Introducing Convolutions to Vision Transformers (Wu 2021)</a><ul><li>#CODE
<a href=https://github.com/leoxiaobin/CvT rel=noopener>https://github.com/leoxiaobin/CvT</a></li><li>Convolutional vision Transformers (CvT) improves ViT in performance and efficienty by introducing convolutions into ViT to yield the best of both disignes</li><li>This is accomplished through two primary modifications: a hierarchy of Transformers containing a new convolutional token embedding, and a convolutional Transformer block leveraging a convolutional projection</li><li>These changes introduce desirable properties of CNNs to the ViT architecture (e.g. shift, scale, and distortion invariance) while maintaining the merits of Transformers (e.g. dynamic attention, global context, and better generalization)</li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2107.02612 rel=noopener>Combining EfficientNet and Vision Transformers for Video Deepfake Detection (Coccomini 2021)</a><ul><li><a href=https://towardsdatascience.com/vision-transformers-or-convolutional-neural-networks-both-de1a2c3c62e4 rel=noopener>Vision Transformers or Convolutional Neural Networks? Both!</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2106.14881 rel=noopener>Early Convolutions Help Transformers See Better (Xiao 2021)</a><ul><li><a href=https://syncedreview.com/2021/07/06/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-55/ rel=noopener>https://syncedreview.com/2021/07/06/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-55/</a></li><li>replacing the ViT patchify stem with a standard convolutional stem in early visual processing results in marked improvements in terms of optimizer stability and final model accuracy</li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2103.10697 rel=noopener>ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases (d&rsquo;ascoli 2021)</a><ul><li>#CODE
<a href=https://github.com/facebookresearch/convit rel=noopener>https://github.com/facebookresearch/convit</a></li><li><a href=https://ai.facebook.com/blog/computer-vision-combining-transformers-and-convolutional-neural-networks/ rel=noopener>https://ai.facebook.com/blog/computer-vision-combining-transformers-and-convolutional-neural-networks/</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2107.06263 rel=noopener>CMT: Convolutional Neural Networks Meet Vision Transformers (Guo 2021)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/2106.04803 rel=noopener>CoAtNet: Marrying Convolution and Attention for All Data Sizes (Dai 2021)</a><ul><li><a href=https://ai.googleblog.com/2021/09/toward-fast-and-accurate-neural.html rel=noopener>https://ai.googleblog.com/2021/09/toward-fast-and-accurate-neural.html</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2201.09450v1 rel=noopener>UniFormer: Unifying Convolution and Self-attention for Visual Recognition (Li 2022)</a><ul><li>#CODE
<a href="https://paperswithcode.com/paper/uniformer-unifying-convolution-and-self?from=n24" rel=noopener>https://paperswithcode.com/paper/uniformer-unifying-convolution-and-self?from=n24</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2201.10271v1 rel=noopener>Convolutional Xformers for Vision (Jeevan 2022)</a><ul><li>#CODE
<a href=https://github.com/pranavphoenix/CXV rel=noopener>https://github.com/pranavphoenix/CXV</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/pdf/2201.09792 rel=noopener>Patches Are All You Need? (Trockman 2022)</a><ul><li>#CODE
<a href=https://github.com/locuslab/convmixer rel=noopener>https://github.com/locuslab/convmixer</a></li><li><a href=https://scoste.fr/posts/convmixer/index.html rel=noopener>https://scoste.fr/posts/convmixer/index.html</a></li><li><a href=https://medium.com/codex/an-overview-on-convmixer-patches-are-all-you-need-8502a8d87011 rel=noopener>https://medium.com/codex/an-overview-on-convmixer-patches-are-all-you-need-8502a8d87011</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2206.00566v1 rel=noopener>The Fully Convolutional Transformer for Medical Image Segmentation (Tragakis 2022)</a><ul><li>#CODE
<a href=https://github.com/Thanos-DB/FullyConvolutionalTransformer rel=noopener>https://github.com/Thanos-DB/FullyConvolutionalTransformer</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/pdf/2208.08900 rel=noopener>Conviformers: Convolutionally guided Vision Transformer (Vaishnav 2022)</a></li></ul><a href=#multi-modal-transformers><h3 id=multi-modal-transformers><span class=hanchor arialabel=Anchor># </span>Multi-modal transformers</h3></a><p>See <a href=/digitalgarden/AI/Deep-learning/Multimodal-learning rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Multimodal-learning>AI/Deep learning/Multimodal learning</a></p><a href=#for-rl><h3 id=for-rl><span class=hanchor arialabel=Anchor># </span>For RL</h3></a><p>See ^decisiontransformer in <a href=/digitalgarden/AI/Reinforcement-learning rel=noopener class=internal-link data-src=/digitalgarden/AI/Reinforcement-learning>AI/Reinforcement learning</a></p></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/digitalgarden/AI/Computer-Vision/Computer-vision/ data-ctx="AI/Deep learning/Transformers" data-src=/AI/Computer-Vision/Computer-vision class=internal-link>Computer Vision</a></li><li><a href=/digitalgarden/AI/Computer-Vision/Deep-CV/ data-ctx="AI/Deep learning/Transformers" data-src=/AI/Computer-Vision/Deep-CV class=internal-link>Deep CV</a></li><li><a href=/digitalgarden/AI/Computer-Vision/Super-resolution/ data-ctx="AI/Deep learning/Transformers" data-src=/AI/Computer-Vision/Super-resolution class=internal-link>Super-resolution</a></li><li><a href=/digitalgarden/AI/Deep-learning/Autoencoders/ data-ctx="AI/Deep learning/Transformers" data-src=/AI/Deep-learning/Autoencoders class=internal-link>Autoencoders</a></li><li><a href=/digitalgarden/AI/Deep-learning/DL/ data-ctx="AI/Deep learning/Transformers" data-src=/AI/Deep-learning/DL class=internal-link>Deep Learning (DL)</a></li><li><a href=/digitalgarden/AI/Deep-learning/Generative-modelling/ data-ctx="AI/Deep learning/Transformers" data-src=/AI/Deep-learning/Generative-modelling class=internal-link>Generative modeling</a></li><li><a href=/digitalgarden/AI/Deep-learning/RNNs/ data-ctx="AI/Deep learning/Transformers" data-src=/AI/Deep-learning/RNNs class=internal-link>Recurrent Neural Networks (RNNs)</a></li><li><a href=/digitalgarden/AI/Forecasting/ data-ctx="AI/Deep learning/Transformers" data-src=/AI/Forecasting class=internal-link>Forecasting</a></li><li><a href=/digitalgarden/AI/NLP/ data-ctx="AI/Deep learning/Transformers" data-src=/AI/NLP class=internal-link>Natural Language Processing (NLP)</a></li><li><a href=/digitalgarden/AI/Self-supervised-learning/ data-ctx="AI/Deep learning/Transformers" data-src=/AI/Self-supervised-learning class=internal-link>Self-supervised learning</a></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://carlos-gg.github.io/digitalgarden/js/graph.abd4bc2af3869a96524d7d23b76152c7.js></script></div></div><div id=contact_buttons><footer><p>Made by Carlos Alberto Gomez Gonzalez using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2023</p><ul><li><a href=https://carlos-gg.github.io/digitalgarden/>Home</a></li><li><a href=https://carlos-gg.github.io>Carlos'Homepage</a></li></ul></footer></div></div></body></html>