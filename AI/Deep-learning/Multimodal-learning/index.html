<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="General-purpose neural networks capable of handling diverse inputs and output tasks
 Resources   Multimodal Deep Learning  https://paperswithcode."><title>Multimodal learning, Foundation models</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=/icon.png><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Source+Sans+Pro:wght@400;600;700&family=Fira+Code:wght@400;700&display=swap" rel=stylesheet><link href=https://carlos-gg.github.io/digitalgarden/styles.cd61336c89ed6e03702366ce4a492b75.min.css rel=stylesheet><script src=https://carlos-gg.github.io/digitalgarden/js/darkmode.46b07878b7f5d9e26ad7a3c40f8a0605.min.js></script>
<script>const BASE_URL="https://carlos-gg.github.io/digitalgarden/",fetchData=Promise.all([fetch("https://carlos-gg.github.io/digitalgarden/indices/linkIndex.df2e6b04180e593e75d813f259756446.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://carlos-gg.github.io/digitalgarden/indices/contentIndex.9498363b22e868e129f212d88449d869.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n}))</script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-S103D50NQ0"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S103D50NQ0",{anonymize_ip:!1})}</script><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://carlos-gg.github.io/digitalgarden/js/graph.27e8521c25c27c79dea35f434c486167.js></script>
<script>drawGraph("https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/Multimodal-learning","https://carlos-gg.github.io/digitalgarden",[{"/moc":"#4388cc"}],-1,!0,!1,!0)</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://carlos-gg.github.io/digitalgarden/js/search.bc849b857f2c1b822264d40635bb67b6.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://carlos-gg.github.io/digitalgarden/>CarlosGG's Knowledge Garden ðŸª´</a></h1><svg tabindex="0" id="search-icon" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg><div class=spacer></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Multimodal learning, Foundation models</h1><p class=meta>Last updated August 22, 2022</p><ul class=tags></ul><aside class=mainTOC><details open><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#resources>Resources</a></li><li><a href=#code>Code</a></li><li><a href=#courses>Courses</a></li><li><a href=#references>References</a><ol><li><a href=#vision-and-language-models>Vision and language models</a></li></ol></li></ol></nav></details></aside><blockquote><p>General-purpose neural networks capable of handling diverse inputs and output tasks</p></blockquote><h2 id=resources>Resources</h2><ul><li><a href=https://multimodal-dl.mpi-inf.mpg.de/ rel=noopener>Multimodal Deep Learning</a></li><li><a href=https://paperswithcode.com/methods/category/vision-and-language-pre-trained-models rel=noopener>https://paperswithcode.com/methods/category/vision-and-language-pre-trained-models</a></li><li><a href=https://theaisummer.com/vision-language-models/ rel=noopener>Vision Language models: towards multi-modal deep learning</a></li></ul><h2 id=code>Code</h2><ul><li>#CODE
<a href=https://github.com/pykale/pykale rel=noopener>Pykale (in pytorch)</a></li></ul><h2 id=courses>Courses</h2><ul><li>#COURSE
<a href="https://www.youtube.com/watch?v=VIq5r7mCAyw&list=PL-Fhd_vrvisNup9YQs_TdLW7DQz-lda0G" rel=noopener>Multimodal Machine Learning (Carnegie Mellon University)</a><ul><li><a href=https://cmu-multicomp-lab.github.io/mmml-course/fall2020/ rel=noopener>https://cmu-multicomp-lab.github.io/mmml-course/fall2020/</a></li></ul></li></ul><h2 id=references>References</h2><p>Review papers:</p><ul><li><p>#PAPER
<a href=https://arxiv.org/abs/2105.11087 rel=noopener>Recent Advances and Trends in Multimodal Deep Learning: A Review (Summaira 2021)</a></p></li><li><p>#PAPER
<a href=https://arxiv.org/abs/2007.10639 rel=noopener>Multi-modal Transformer for Video Retrieval (Gabeur 2020)</a></p></li><li><p>#PAPER
<a href=https://arxiv.org/abs/2103.03206 rel=noopener>Perceiver: General Perception with Iterative Attention (Jaegle 2021)</a></p><ul><li><a href=https://www.zdnet.com/article/googles-supermodel-deepmind-perceiver-is-a-step-on-the-road-to-an-ai-machine-that-could-process-everything/ rel=noopener>https://www.zdnet.com/article/googles-supermodel-deepmind-perceiver-is-a-step-on-the-road-to-an-ai-machine-that-could-process-everything/</a></li><li>Multi-model with image, audio, video, 3d point clouds</li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/abs/2106.09756v1 rel=noopener>PyKale: Knowledge-Aware Machine Learning from Multiple Sources in Python (Lu 2021)</a> ^pykale</p></li><li><p>#PAPER
<a href=https://arxiv.org/abs/2107.14795v2 rel=noopener>Perceiver IO: A General Architecture for Structured Inputs & Outputs (Jaegle 2021)</a></p><ul><li>#CODE
<a href=https://paperswithcode.com/paper/perceiver-io-a-general-architecture-for rel=noopener>https://paperswithcode.com/paper/perceiver-io-a-general-architecture-for</a></li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/abs/2104.11178v2 rel=noopener>VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text (Akbari 2021)</a></p><ul><li>#CODE
<a href=https://paperswithcode.com/paper/vatt-transformers-for-multimodal-self rel=noopener>https://paperswithcode.com/paper/vatt-transformers-for-multimodal-self</a></li><li>VATT is trained to learn multimodal representations from unlabeled data using Transformer architectures</li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/abs/2111.12417v1 rel=noopener>NÃœWA: Visual Synthesis Pre-training for Neural visUal World creAtion (Wu 2021)</a></p><ul><li>#CODE
<a href=https://paperswithcode.com/paper/nuwa-visual-synthesis-pre-training-for-neural rel=noopener>https://paperswithcode.com/paper/nuwa-visual-synthesis-pre-training-for-neural</a></li><li><a href="https://www.youtube.com/watch?v=InhMx1h0N40&list=WL&index=50" rel=noopener>Paper explained</a></li><li>NÃœWAÂ consists of an adaptive encoder that takes either text or visual input, and a pre-trained decoder shared by 8 visual tasks</li><li>3D Nearby Attention mechanism (3DNA) is proposed to reduce computational complexity and improve visual quality of results, by considering the locality characteristics for both spatial and temporal axes to better deal with the nature of the visual data</li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/abs/2202.03555 rel=noopener>data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language (Baevski 2022)</a></p><ul><li>#CODE
<a href=https://github.com/pytorch/fairseq/tree/main/examples/data2vec rel=noopener>https://github.com/pytorch/fairseq/tree/main/examples/data2vec</a></li><li><a href=https://ai.facebook.com/blog/the-first-high-performance-self-supervised-algorithm-that-works-for-speech-vision-and-text/ rel=noopener>https://ai.facebook.com/blog/the-first-high-performance-self-supervised-algorithm-that-works-for-speech-vision-and-text/</a></li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/abs/2205.06175v1 rel=noopener>A Generalist Agent (Reed 2022)</a></p><ul><li><a href="https://www.youtube.com/watch?v=wSQJZHfAg18" rel=noopener>Paper explained</a></li><li>New approach, inspired by large-scale language models, that acts a single generalist agent. The agent, called Gato, is built to work as a multi-modal, multi-task, multi-embodiment generalist policy</li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/pdf/2206.06336v1 rel=noopener>Language Models are General-Purpose Interfaces (Hao 2022)</a></p><ul><li>#CODE
<a href=https://github.com/microsoft/unilm rel=noopener>https://github.com/microsoft/unilm</a></li></ul></li></ul><h3 id=vision-and-language-models>Vision and language models</h3><p>Review papers:</p><ul><li><p>#PAPER
<a href=https://arxiv.org/pdf/2202.10936 rel=noopener>A Survey of Vision-Language Pre-Trained Models (Du 2022)</a></p></li><li><p>#PAPER
<a href=https://openai.com/blog/dall-e/ rel=noopener>DALL-E - Creating Images from Text (Ramesh 2021)</a> ^dall-e</p><ul><li><a href=https://www.technologyreview.com/2021/01/05/1015754/avocado-armchair-future-ai-openai-deep-learning-nlp-gpt3-computer-vision-common-sense/ rel=noopener>https://www.technologyreview.com/2021/01/05/1015754/avocado-armchair-future-ai-openai-deep-learning-nlp-gpt3-computer-vision-common-sense/</a></li><li><a href="https://www.youtube.com/watch?v=j4xgkjWlfL4" rel=noopener>Blogpost explained</a></li><li>#CODE
<a href=https://github.com/EleutherAI/DALLE-mtf rel=noopener>https://github.com/EleutherAI/DALLE-mtf</a></li><li>Multi-modal text and speech</li><li><a href=https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-mini--Vmlldzo4NjIxODA rel=noopener>DALL-E mini</a></li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/pdf/2103.00020v1 rel=noopener>Learning Transferable Visual Models From Natural Language Supervision (Radford 2021)</a></p><ul><li>#CODE
<a href=https://paperswithcode.com/paper/learning-transferable-visual-models-from#code rel=noopener>https://paperswithcode.com/paper/learning-transferable-visual-models-from#code</a></li><li>#CODE
<a href=https://github.com/openai/CLIP rel=noopener>https://github.com/openai/CLIP</a></li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/pdf/2108.10904v2 rel=noopener>SimVLM: Simple Visual Language Model Pretraining with Weak Supervision (Wang 2022)</a></p></li><li><p>#PAPER
<a href=https://arxiv.org/pdf/2202.03052v1 rel=noopener>Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework (Wang 2022)</a></p></li><li><p>#PAPER
<a href=https://arxiv.org/abs/2204.14198v1 rel=noopener>Flamingo: a Visual Language Model for Few-Shot Learning (Alayrac 2022)</a></p><ul><li>#CODE
<a href=https://github.com/lucidrains/flamingo-pytorch rel=noopener>https://github.com/lucidrains/flamingo-pytorch</a></li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/abs/2206.14718v1 rel=noopener>LViT: Language meets Vision Transformer in Medical Image Segmentation (Li 2022)</a> ^lvit</p><ul><li>#CODE
<a href=https://github.com/HUANGLIZI/LViT rel=noopener>https://github.com/HUANGLIZI/LViT</a></li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/pdf/2206.10789v1 rel=noopener>Scaling Autoregressive Models for Content-Rich Text-to-Image Generation (Yu 2022)</a></p><ul><li>#CODE
<a href=https://github.com/google-research/parti rel=noopener>https://github.com/google-research/parti</a></li><li><a href=https://parti.research.google/ rel=noopener>https://parti.research.google/</a></li><li>Pathways Autoregressive Text-to-Image model (Parti), an autoregressive text-to-image generation model that achieves high-fidelity photorealistic image generation and supports content-rich synthesis involving complex compositions and world knowledge</li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/pdf/2204.14217 rel=noopener>CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers (Ding 2022)</a></p><ul><li>#CODE
<a href=https://github.com/THUDM/CogView2 rel=noopener>https://github.com/THUDM/CogView2</a></li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/pdf/2205.11487v1 rel=noopener>Imagen - Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding (Saharia 2022)</a></p><ul><li><a href=https://imagen.research.google/ rel=noopener>https://imagen.research.google/</a></li><li>Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation</li><li>#CODE
<a href=https://paperswithcode.com/paper/photorealistic-text-to-image-diffusion-models rel=noopener>https://paperswithcode.com/paper/photorealistic-text-to-image-diffusion-models</a></li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/pdf/2206.08916 rel=noopener>Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks (Lu 2022)</a></p><ul><li><a href=https://unified-io.allenai.org/ rel=noopener>https://unified-io.allenai.org/</a></li></ul></li></ul></article><hr><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/digitalgarden/AI/Deep-learning/DL>Deep Learning (DL)</a></li><li><a href=/digitalgarden/AI/Deep-learning/Diffusion-models>Diffusion models</a></li><li><a href=/digitalgarden/AI/Deep-learning/Transformers>Transformers</a></li><li><a href=/digitalgarden/AI/Multi-task-learning>Multi-task learning</a></li></ul></div></div><div id=contact_buttons><footer><p>Made by Carlos Alberto Gomez Gonzalez using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2022</p><ul><li><a href=https://carlos-gg.github.io/digitalgarden/>Home</a></li><a href=https://carlos-gg.github.io>Carlos'Homepage</a></ul></footer></div><script src=https://carlos-gg.github.io/digitalgarden/js/popover.e57188d2e4c06b0654e020b3a734bb62.min.js></script>
<script>initPopover("https://carlos-gg.github.io/digitalgarden")</script></div></body></html>