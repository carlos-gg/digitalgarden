<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="A multilayer perceptron (MLP) is a feed forward artificial neural network model that maps sets of input data onto a set of appropriate outputs."><title>Multilayer perceptrons (MLPs)</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://carlos-gg.github.io/digitalgarden//icon.png><link href=https://carlos-gg.github.io/digitalgarden/styles.b3e1e36b0403ac565c9392b3e23ef3b6.min.css rel=stylesheet><link href=https://carlos-gg.github.io/digitalgarden/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://carlos-gg.github.io/digitalgarden/js/darkmode.18b7c6dfe67ae3bf2317338cf4189144.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/util.9825137f5e7825e8553c68ce39ac9e44.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/popover.abe6a51cc7138c5dff00f151dd627ad1.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/code-title.b35124ad8db0ba37162b886afb711cbc.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/clipboard.c20857734e53a3fb733b7443879efa61.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const BASE_URL="https://carlos-gg.github.io/digitalgarden/",fetchData=Promise.all([fetch("https://carlos-gg.github.io/digitalgarden/indices/linkIndex.0b9ba37b3815f025016c0e5f1fba0cf7.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://carlos-gg.github.io/digitalgarden/indices/contentIndex.91f2ce950593b1ee3e6b28f0d95812fd.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://carlos-gg.github.io/digitalgarden",!0,!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!0;drawGraph("https://carlos-gg.github.io/digitalgarden",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/carlos-gg.github.io\/digitalgarden\/js\/router.9d4974281069e9ebb189f642ae1e3ca2.min.js"
    attachSPARouting(init, render)
  </script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-S103D50NQ0"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S103D50NQ0",{anonymize_ip:!1})}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://carlos-gg.github.io/digitalgarden/js/full-text-search.24827f874defbbc6d529926cbfcfb493.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://carlos-gg.github.io/digitalgarden/>CarlosGG's Knowledge Garden ðŸª´</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Multilayer perceptrons (MLPs)</h1><p class=meta>Last updated
Apr 10, 2022
<a href=https://github.com/jackyzha0/quartz/tree/hugo/content/AI/Deep%20learning/MLPs.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details open><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#resources>Resources</a></li><li><a href=#code>Code</a></li><li><a href=#perceptron>Perceptron</a></li><li><a href=#mlps-for-vision-and-language>MLPs for vision and language</a></li></ol></nav></details></aside><blockquote><p>A multilayer perceptron (MLP) is a feed forward artificial neural network model that maps sets of input data onto a set of appropriate outputs. An MLP consists of multiple layers of nodes in a directed graph, with each layer fully connected to the next one. Except for the input nodes, each node is a neuron (or processing element) with a non linear activation function</p></blockquote><a href=#resources><h2 id=resources><span class=hanchor arialabel=Anchor># </span>Resources</h2></a><ul><li>MLP utilizes a supervised learning technique called back propagation for training the network</li><li>MLP is a modification of the standard linear perceptron and can distinguish data that is not linearly separable</li><li><a href=https://medium.com/data-science-bootcamp/multilayer-perceptron-mlp-vs-convolutional-neural-network-in-deep-learning-c890f487a8f1 rel=noopener>Multilayer Perceptron (MLP) vs Convolutional Neural Network in Deep Learning</a></li></ul><a href=#code><h2 id=code><span class=hanchor arialabel=Anchor># </span>Code</h2></a><ul><li>#CODE
<a href=https://github.com/fchollet/keras/blob/master/examples/mnist_mlp.py rel=noopener>MLP for MNIST</a></li><li>#CODE Sklearn MLP implementation:<ul><li><a href=https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html rel=noopener>https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html</a></li><li><a href=https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor rel=noopener>https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor</a></li></ul></li></ul><a href=#perceptron><h2 id=perceptron><span class=hanchor arialabel=Anchor># </span>Perceptron</h2></a><ul><li><a href=https://medium.com/@Jaconda/a-concise-history-of-neural-networks-2070655d3fec rel=noopener>History of the Perceptron</a></li><li><a href=https://www.neuraldesigner.com/blog/perceptron-the-main-component-of-neural-networks rel=noopener>https://www.neuraldesigner.com/blog/perceptron-the-main-component-of-neural-networks</a></li><li><a href=https://fr.mathworks.com/help/nnet/ug/perceptron-neural-networks.html rel=noopener>https://fr.mathworks.com/help/nnet/ug/perceptron-neural-networks.html</a></li><li><a href=http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning/ rel=noopener>http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning/</a></li><li><a href=http://sebastianraschka.com/Articles/2015_singlelayer_neurons.html rel=noopener>http://sebastianraschka.com/Articles/2015_singlelayer_neurons.html</a></li></ul><a href=#mlps-for-vision-and-language><h2 id=mlps-for-vision-and-language><span class=hanchor arialabel=Anchor># </span>MLPs for vision and language</h2></a><ul><li>#PAPER
<a href=https://arxiv.org/abs/2105.01601v2 rel=noopener>MLP-Mixer: An all-MLP Architecture for Vision (Tolstikhin 2021)</a><ul><li>#CODE
<a href="https://paperswithcode.com/paper/mlp-mixer-an-all-mlp-architecture-for-vision?from=n9" rel=noopener>https://paperswithcode.com/paper/mlp-mixer-an-all-mlp-architecture-for-vision?from=n9</a></li><li>CNNs are widely regarded as the go-to model for dealing with computer vision tasks. Attention-based architectures have also emerged as promising approaches that produce good performance on a variety of vision tasks. Despite this trend and the successes of attention and CNN architectures, this paper proposes a simple alternative architecture, MLP-Mixer, based on multi-layer perceptions, that produces competitive results on image classification benchmarks</li><li>MLP-Mixer contains two types of layers. One layer of MLPs applied independently to image patches and another layer of MLPs applied across patches. These layers achieve the effect of mixing per-location features and mixing spatial information, respectively</li><li>MLP-Mixer achieves competitive results on the ImageNet benchmark</li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2105.01883v1 rel=noopener>RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition (Ding 2021)</a><ul><li>#CODE
<a href=https://github.com/DingXiaoH/RepMLP rel=noopener>https://github.com/DingXiaoH/RepMLP</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2105.03404 rel=noopener>ResMLP: Feedforward networks for image classification with data-efficient training (Touvron 2021)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/2105.08050v2 rel=noopener>Pay Attention to MLPs (Liu 2021)</a><ul><li>#CODE
<a href="https://paperswithcode.com/paper/pay-attention-to-mlps?from=n10" rel=noopener>https://paperswithcode.com/paper/pay-attention-to-mlps?from=n10</a></li><li><a href=https://www.infoq.com/news/2021/10/google-mlp-vision-language/ rel=noopener>https://www.infoq.com/news/2021/10/google-mlp-vision-language/</a></li><li>Researchers at Google Brain have announced Gated Multi-Layer Perceptron (gMLP), a deep-learning model that contains only basic multi-layer perceptrons</li><li>gMLP aims to show that these simplified architectures can perform as well as Transformers on key vision and language applications. According to the authors, the results and comparisons show that attention is not critical for Vision Transformers</li><li>In fine-tuning tasks, gMLP can close the gap on Transformers by simply making the model substantially larger</li><li>gMLP can scale as well as Transformers over increased data and compute</li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2201.02973v1 rel=noopener>MAXIM: Multi-Axis MLP for Image Processing (Tu 2022)</a><ul><li>#CODE
<a href="https://paperswithcode.com/paper/maxim-multi-axis-mlp-for-image-processing?from=n23" rel=noopener>https://paperswithcode.com/paper/maxim-multi-axis-mlp-for-image-processing?from=n23</a></li></ul></li></ul></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/digitalgarden/AI/Computer-Vision/Computer-vision/ data-ctx="AI/Deep learning/MLPs" data-src=/AI/Computer-Vision/Computer-vision class=internal-link>Computer Vision</a></li><li><a href=/digitalgarden/AI/Deep-learning/DL/ data-ctx="AI/Deep learning/MLPs" data-src=/AI/Deep-learning/DL class=internal-link>Deep Learning (DL)</a></li><li><a href=/digitalgarden/AI4ES/Parameterizations/ data-ctx="AI/Deep learning/MLPs" data-src=/AI4ES/Parameterizations class=internal-link>Parameterizations</a></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://carlos-gg.github.io/digitalgarden/js/graph.abd4bc2af3869a96524d7d23b76152c7.js></script></div></div><div id=contact_buttons><footer><p>Made by Carlos Alberto Gomez Gonzalez using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2023</p><ul><li><a href=https://carlos-gg.github.io/digitalgarden/>Home</a></li><li><a href=https://carlos-gg.github.io>Carlos'Homepage</a></li></ul></footer></div></div></body></html>