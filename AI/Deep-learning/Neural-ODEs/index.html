<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Resources  https://github.com/Zymrael/awesome-neural-ode  Understanding Neural ODE&rsquo;s  Neural Ordinary Differential Equations and Dynamics Models  ODEs are often used to describe the time derivatives of a physical situation, referred to as the dynamics."><title>Neural Ordinary Differential Equations</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=/icon.png><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Source+Sans+Pro:wght@400;600;700&family=Fira+Code:wght@400;700&display=swap" rel=stylesheet><link href=https://carlos-gg.github.io/digitalgarden/styles.cd61336c89ed6e03702366ce4a492b75.min.css rel=stylesheet><script src=https://carlos-gg.github.io/digitalgarden/js/darkmode.46b07878b7f5d9e26ad7a3c40f8a0605.min.js></script><script>const BASE_URL="https://carlos-gg.github.io/digitalgarden/",fetchData=Promise.all([fetch("https://carlos-gg.github.io/digitalgarden/indices/linkIndex.4662b1c06ac21499279f532949abefdb.min.json").then(a=>a.json()).then(a=>({index:a.index,links:a.links})),fetch("https://carlos-gg.github.io/digitalgarden/indices/contentIndex.5091725e6bbe4053e9db7cc5ceea8619.min.json").then(a=>a.json())]).then(([{index:a,links:b},c])=>({index:a,links:b,content:c}))</script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-S103D50NQ0"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-S103D50NQ0',{anonymize_ip:!1})}</script><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://carlos-gg.github.io/digitalgarden/js/graph.27e8521c25c27c79dea35f434c486167.js></script><script>drawGraph("https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/Neural-ODEs","https://carlos-gg.github.io/digitalgarden",[{"/moc":"#4388cc"}],-1,!0,!1,!0)</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script><script defer src=https://carlos-gg.github.io/digitalgarden/js/search.bc849b857f2c1b822264d40635bb67b6.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://carlos-gg.github.io/digitalgarden/>CarlosGG's Knowledge Garden ðŸª´</a></h1><svg tabindex="0" id="search-icon" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg><div class=spacer></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Neural Ordinary Differential Equations</h1><p class=meta>Last updated April 9, 2022</p><ul class=tags></ul><aside class=mainTOC><h3>Table of Contents</h3><nav id=TableOfContents><ol><li><a href=#resources>Resources</a></li><li><a href=#references>References</a></li></ol></nav></aside><h2 id=resources>Resources</h2><ul><li><a href=https://github.com/Zymrael/awesome-neural-ode>https://github.com/Zymrael/awesome-neural-ode</a></li><li><a href=https://jontysinai.github.io/jekyll/update/2019/01/18/understanding-neural-odes.html rel=noopener>Understanding Neural ODE&rsquo;s</a></li><li><a href=https://medium.com/@ml.at.berkeley/neural-ordinary-differential-equations-and-dynamics-models-1a4277fbb80 rel=noopener>Neural Ordinary Differential Equations and Dynamics Models</a><ul><li>ODEs are often used to describe the time derivatives of a physical situation, referred to as the dynamics. Knowing the dynamics allows us to model the change of an environment, like a physics simulation, unlocking the ability to take any starting condition and model how it will change. With Neural ODEs, we donâ€™t define explicit ODEs to document the dynamics, but learn them via ML.</li><li>Strong connection with
<a href=/digitalgarden/AI/Deep-learning/Residual-and-dense-neural-networks rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Residual-and-dense-neural-networks>Residual and dense neural networks</a>. Why do residual layers help networks achieve higher accuracies and grow deeper? Firstly, skip connections help information flow through the network by sending the hidden state, h(t), along with the transformation by the layer, f(h(t)), to layer t+1, preventing important information from being discarded by f. Secondly, residual layers can be stacked, forming very deep networks.</li><li>However, ResNets still employ many layers of weights and biases requiring much time and data to train. On top of this, the backpropagation algorithm on such a deep network incurs a high memory cost to store intermediate values.</li><li>Continuous depth ODENets are evaluated using black box ODE solvers, but first the parameters of the model must be optimized via gradient descent. To do this, we need to know the gradient of the loss with respect to the parameters, or how the loss function depends on the parameters in the ODENet.</li><li>In deep learning, backpropagation is the workhorse for finding this gradient, but this algorithm incurs a high memory costs to store the intermediate values of the network. On top of this, the sheer number of chain rule applications produces numerical error. Since an ODENet models a differential equation, these issues can be circumvented using sensitivity analysis methods developed for calculating gradients of a loss function with respect to the parameters of the system producing its input.</li></ul></li></ul><h2 id=references>References</h2><ul><li>#PAPER
<a href=https://arxiv.org/abs/1806.07366 rel=noopener>Neural Ordinary Differential Equations (TQ Chen 2018)</a><ul><li>#CODE <a href=https://github.com/JSeam2/Neural-Ordinary-Differential-Equations>https://github.com/JSeam2/Neural-Ordinary-Differential-Equations</a></li><li>#CODE <a href=https://github.com/jason71995/Keras_ODENet>https://github.com/jason71995/Keras_ODENet</a></li><li><a href="https://www.youtube.com/watch?v=V6nGT0Gakyg" rel=noopener>Neural Ordinary Differential Equations - Best Paper Awards NeurIPS 2018</a></li><li><a href=https://towardsdatascience.com/paper-summary-neural-ordinary-differential-equations-37c4e52df128>https://towardsdatascience.com/paper-summary-neural-ordinary-differential-equations-37c4e52df128</a></li><li><a href=https://braindump.jethro.dev/posts/neural_ode/>https://braindump.jethro.dev/posts/neural_ode/</a></li><li><a href=https://msurtsukov.github.io/Neural-ODE/>https://msurtsukov.github.io/Neural-ODE/</a></li><li><a href=https://github.com/msurtsukov/neural-ode/blob/master/Neural%20ODEs.ipynb>https://github.com/msurtsukov/neural-ode/blob/master/Neural%20ODEs.ipynb</a></li><li><a href="https://www.youtube.com/watch?v=jltgNGt8Lpg">https://www.youtube.com/watch?v=jltgNGt8Lpg</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1904.01681 rel=noopener>Augmented Neural ODEs (Dupont 2019)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1912.00796 rel=noopener>Differential Bayesian Neural Nets (Look 2020)</a><ul><li>Neural Ordinary Differential Equations (N-ODEs) are a powerful building block for learning systems, which extend residual networks to a continuous-time dynamical system. Propose a Bayesian version of N-ODEs that enables well-calibrated quantification of prediction uncertainty, while maintaining the expressive power of their deterministic counterpart.</li></ul></li><li>#THESIS/MSC
<a href=https://uwspace.uwaterloo.ca/bitstream/handle/10012/15354/Dockhorn_Tim.pdf rel=noopener>Generative Modeling with Neural Ordinary Differential Equations (2019)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/2001.04385 rel=noopener>Universal Differential Equations for Scientific Machine Learning (Rackauckas 2020)</a><ul><li><a href=https://www.stochasticlifestyle.com/how-to-train-interpretable-neural-networks-that-accurately-extrapolate-from-small-data/>https://www.stochasticlifestyle.com/how-to-train-interpretable-neural-networks-that-accurately-extrapolate-from-small-data/</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/pdf/2005.00865 rel=noopener>Neural Differential Equations for Single Image Super-Resolution (Le Scao 2020)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/2006.04439 rel=noopener>Liquid Time-constant Networks (Hasani 2020)</a><ul><li>#TALK <a href="https://www.youtube.com/watch?v=IlliqYiRhMU">https://www.youtube.com/watch?v=IlliqYiRhMU</a></li><li>#CODE <a href=https://github.com/raminmh/liquid_time_constant_networks>https://github.com/raminmh/liquid_time_constant_networks</a></li><li><a href=https://news.mit.edu/2021/machine-learning-adapts-0128>https://news.mit.edu/2021/machine-learning-adapts-0128</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2202.02435 rel=noopener>On Neural Differential Equations (Kidger 2022)</a></li></ul></article><hr><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=https://carlos-gg.github.io/digitalgarden//AI/Deep-learning/DL>Deep Learning (DL)</a></li></ul></div></div><div id=contact_buttons><footer><p>Made by Carlos Alberto Gomez Gonzalez using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2022</p><a href=https://carlos-gg.github.io/digitalgarden/>Root</a><a href=https://carlos-gg.github.io>CarlosGG</a></footer></div><script src=https://carlos-gg.github.io/digitalgarden/js/popover.e57188d2e4c06b0654e020b3a734bb62.min.js></script><script>initPopover("https://carlos-gg.github.io/digitalgarden")</script></div></body></html>