<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Deep learning (DL), also known as deep structured learning, is part of a broader family of [[AI/Machine Learning]] methods based on artificial neural networks with representation learning."><title>Deep Learning (DL)</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=/icon.png><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Source+Sans+Pro:wght@400;600;700&family=Fira+Code:wght@400;700&display=swap" rel=stylesheet><link href=https://carlos-gg.github.io/digitalgarden/styles.cd61336c89ed6e03702366ce4a492b75.min.css rel=stylesheet><script src=https://carlos-gg.github.io/digitalgarden/js/darkmode.46b07878b7f5d9e26ad7a3c40f8a0605.min.js></script>
<script>const BASE_URL="https://carlos-gg.github.io/digitalgarden/",fetchData=Promise.all([fetch("https://carlos-gg.github.io/digitalgarden/indices/linkIndex.245c4ea85f5863f9859f3efe24c1fc8b.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://carlos-gg.github.io/digitalgarden/indices/contentIndex.daaf58e158d27dba662748c331e23d56.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n}))</script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-S103D50NQ0"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S103D50NQ0",{anonymize_ip:!1})}</script><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://carlos-gg.github.io/digitalgarden/js/graph.27e8521c25c27c79dea35f434c486167.js></script>
<script>drawGraph("https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/DL","https://carlos-gg.github.io/digitalgarden",[{"/moc":"#4388cc"}],-1,!0,!1,!0)</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://carlos-gg.github.io/digitalgarden/js/search.bc849b857f2c1b822264d40635bb67b6.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://carlos-gg.github.io/digitalgarden/>CarlosGG's Knowledge Garden 🪴</a></h1><svg tabindex="0" id="search-icon" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg><div class=spacer></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Deep Learning (DL)</h1><p class=meta>Last updated July 28, 2022</p><ul class=tags></ul><aside class=mainTOC><details open><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#resources>Resources</a><ol><li><a href=#dl-news-aggregators>DL news aggregators</a></li><li><a href=#cheatsheets>Cheatsheets</a></li><li><a href=#when-to-use-and-not-to-use-deep-learning>When to use and not to use deep learning</a></li></ol></li><li><a href=#books>Books</a></li><li><a href=#talks>Talks</a></li><li><a href=#courses>Courses</a></li><li><a href=#code>Code</a></li><li><a href=#references>References</a><ol><li><a href=#generalization>Generalization</a></li><li><a href=#regularization>Regularization</a><ol><li><a href=#data-augmentation>Data augmentation</a></li><li><a href=#dropout>Dropout</a></li></ol></li><li><a href=#stochastic-depth>Stochastic depth</a><ol><li><a href=#normalization>Normalization</a></li></ol></li><li><a href=#activations>Activations</a></li><li><a href=#losscost-functions>Loss/Cost functions</a></li><li><a href=#optimizers-and-backpropagation>Optimizers and backpropagation</a></li><li><a href=#efficiency-and-performance>Efficiency and performance</a></li><li><a href=#attention>Attention</a></li><li><a href=#explainability-methods-for-neural-networks>Explainability methods for Neural Networks</a></li></ol></li><li><a href=#applications>Applications</a><ol><li><a href=#deep-learning-for-multi-dimensional-data>Deep learning for multi-dimensional data</a></li><li><a href=#deep-learning-for-tabular-data>Deep learning for tabular data</a></li><li><a href=#deep-learning-for-scientific-discovery>Deep learning for scientific discovery</a></li><li><a href=#multimodal-learning>Multimodal learning</a></li><li><a href=#dl-for-nlp-time-series-and-sequence-modelling>DL for NLP, time series and sequence modelling</a></li></ol></li><li><a href=#architectures-and-model-families>Architectures and model families</a><ol><li><a href=#geometric-dl>Geometric DL</a></li><li><a href=#mlps>MLPs</a></li><li><a href=#deep-belief-network>Deep belief network</a></li><li><a href=#autoencoders>Autoencoders</a></li><li><a href=#cnns>CNNs</a></li><li><a href=#rnns>RNNs</a></li><li><a href=#capsnets>CapsNets</a></li><li><a href=#gans>GANs</a></li><li><a href=#diffusion-models>Diffusion models</a></li><li><a href=#gnns>GNNs</a></li><li><a href=#residual-and-dense-neural-networks>Residual and dense neural networks</a></li><li><a href=#neural-odes>Neural ODEs</a></li><li><a href=#fourier-neural-operators>Fourier Neural Operators</a></li><li><a href=#transformers>Transformers</a></li><li><a href=#gflownets>GFlowNets</a></li><li><a href=#neural-cellular-automata>Neural Cellular Automata</a></li><li><a href=#neural-processes>Neural processes</a></li><li><a href=#bayesianprobabilistic-dl>Bayesian/probabilistic DL</a></li></ol></li></ol></nav></details></aside><blockquote><p>Deep learning (DL), also known as deep structured learning, is part of a broader family of <a href=/digitalgarden/AI/Machine-Learning rel=noopener class=internal-link data-src=/digitalgarden/AI/Machine-Learning>AI/Machine Learning</a> methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised. DL uses huge neural networks with many layers of processing units, taking advantage of advances in computing power and improved training techniques to learn complex patterns in large amounts of data</p></blockquote><h2 id=resources>Resources</h2><ul><li><a href=https://github.com/ChristosChristofidis/awesome-deep-learning rel=noopener>https://github.com/ChristosChristofidis/awesome-deep-learning</a></li><li><a href=https://github.com/endymecy/awesome-deeplearning-resources rel=noopener>https://github.com/endymecy/awesome-deeplearning-resources</a></li><li><a href=https://en.wikipedia.org/wiki/Deep_learning rel=noopener>https://en.wikipedia.org/wiki/Deep_learning</a></li><li><a href=https://jalammar.github.io/visual-interactive-guide-basics-neural-networks/ rel=noopener>https://jalammar.github.io/visual-interactive-guide-basics-neural-networks/</a></li><li><a href=https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/ rel=noopener>A Quick Introduction to Neural Networks</a></li><li><a href=http://karpathy.github.io/2022/03/14/lecun1989/ rel=noopener>Deep Neural Nets: 33 years ago and 33 years from now (Andrej Karpathy)</a></li><li><a href=https://spectrum.ieee.org/deep-learning-computational-cost rel=noopener>Deep learning&rsquo;s diminish returns (Thompson)</a><ul><li><a href=https://towardsdatascience.com/the-future-of-deep-learning-7e8574ad6ae3 rel=noopener>https://towardsdatascience.com/the-future-of-deep-learning-7e8574ad6ae3</a></li></ul></li><li><a href=https://nautil.us/deep-learning-is-hitting-a-wall-14467/ rel=noopener>Deep Learning Is Hitting a Wall</a></li><li><a href=http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning/ rel=noopener>A Brief History of Neural Nets and Deep Learning (2020)</a></li><li><a href=https://dawn.cs.stanford.edu/benchmark/ rel=noopener>Time Benchmark of models</a></li><li><a href=http://karpathy.github.io/2019/04/25/recipe/ rel=noopener>A Recipe for Training Neural Networks</a></li><li><a href=https://www.quantamagazine.org/computer-scientists-prove-why-bigger-neural-networks-do-better-20220210/ rel=noopener>Computer Scientists Prove Why Bigger Neural Networks Do Better</a></li><li><a href=https://techxplore.com/news/2021-05-survnet-procedure-variable-deep-neural.html rel=noopener>SurvNet: A backward elimination procedure to enhance variable selection for deep neural networks</a></li></ul><h3 id=dl-news-aggregators>DL news aggregators</h3><ul><li><a href=https://deepai.org/ rel=noopener>DeepAI</a></li><li><a href=https://paperswithcode.com/ rel=noopener>Papers with code</a></li><li><a href=https://deeplearn.org/ rel=noopener>Deep learning monitor</a></li></ul><h3 id=cheatsheets>Cheatsheets</h3><ul><li><a href=https://github.com/afshinea/stanford-cs-230-deep-learning/blob/master/super-cheatsheet-deep-learning.pdf rel=noopener>https://github.com/afshinea/stanford-cs-230-deep-learning/blob/master/super-cheatsheet-deep-learning.pdf</a></li></ul><h3 id=when-to-use-and-not-to-use-deep-learning>When to use and not to use deep learning</h3><ul><li><a href=https://blog.dataiku.com/when-and-when-not-to-use-deep-learning rel=noopener>When and When Not to Use Deep Learning</a></li><li><a href=http://beamandrew.github.io/deeplearning/2017/06/04/deep_learning_works.html rel=noopener>You can probably use deep learning even if your data isn&rsquo;t that big</a></li><li><a href=http://hyperparameter.space/blog/when-not-to-use-deep-learning/ rel=noopener>When not to use deep learning</a></li><li><a href=http://maxberggren.se/2017/06/18/deep-learning-vs-xgboost/ rel=noopener>Using ANNs on small data – Deep Learning vs. Xgboost</a></li><li><a href=https://blog.keras.io/the-limitations-of-deep-learning.html rel=noopener>The limitations of deep learning</a></li></ul><h2 id=books>Books</h2><ul><li>#BOOK
<a href=https://blogs.rstudio.com/ai/posts/2022-05-31-deep-learning-with-r-2e/ rel=noopener>Deep Learning with R, 2nd Edition (Kalinowski 2022)</a></li><li>#BOOK
<a href=https://arxiv.org/abs/2201.00650 rel=noopener>Deep Learning Interviews: Hundreds of fully solved job interview questions from a wide range of key topics in AI (Kashani 2022)</a></li><li>#BOOK
<a href=https://physicsbaseddeeplearning.org/intro.html rel=noopener>Physics-based Deep Learning Book (Thuerey 2021)</a> ^PBDL</li><li>#BOOK
<a href=https://deeplearningtheory.com/PDLT.pdf rel=noopener>The Principles of DL Theory: An Effective Theory Approach to Understanding Neural Networks (Roberts 2022)</a><ul><li><a href=https://ai.facebook.com/blog/advancing-ai-theory-with-a-first-principles-understanding-of-deep-neural-networks/ rel=noopener>https://ai.facebook.com/blog/advancing-ai-theory-with-a-first-principles-understanding-of-deep-neural-networks/</a></li><li>#PAPER
<a href=https://arxiv.org/abs/2106.10165 rel=noopener>The Principles of Deep Learning Theory (Roberts 2021)</a></li><li><a href="https://www.youtube.com/watch?v=m2bXL5Z5CBM" rel=noopener>Paper explained</a></li></ul></li><li>#BOOK
<a href=https://www.deeplearningbook.org/ rel=noopener>Deep Learning Book (Goodfellow, 2016 MIT)</a><ul><li>The Deep Learning textbook is a resource intended to help students and practitioners enter the field of machine learning in general and deep learning in particular</li></ul></li><li>#BOOK
<a href=http://deeplearning.net/tutorial/ rel=noopener>DL tutorial (LISA Lab, U Montreal)</a></li><li>#BOOK
<a href=https://www.manning.com/books/deep-learning-with-python-second-edition rel=noopener>Deep Learning with Python (Chollet, 2021 MANNING)</a><ul><li><a href=http://faculty.neu.edu.cn/yury/AAI/Textbook/Deep-Learning-with-Python.pdf rel=noopener>1st edition</a></li></ul></li><li>#BOOK
<a href=https://freecomputerbooks.com/Machine-Learning-Yearning.html rel=noopener>Machine learning yearning (Andrew Ng, 2018)</a><ul><li><a href=https://github.com/ajaymache/machine-learning-yearning rel=noopener>https://github.com/ajaymache/machine-learning-yearning</a></li></ul></li><li>#BOOK
<a href=https://d2l.ai/index.html rel=noopener>Dive into Deep Learning (Zhang)</a><ul><li>An interactive deep learning book for students, engineers, and researchers. Uses MXNet/Gluon, Pytorch and Tensorflow</li><li><a href=https://en.d2l.ai/d2l-en.zip rel=noopener>Jupyter notebooks for each section</a></li></ul></li><li>#BOOK
<a href=https://torres.ai/deep-learning-inteligencia-artificial-keras/ rel=noopener>Introduccion practica con Keras (Torres 2018)</a></li><li>#BOOK
<a href=http://neuralnetworksanddeeplearning.com/index.html rel=noopener>Neural Networks and Deep Learning</a></li></ul><h2 id=talks>Talks</h2><ul><li>#TALK
<a href=https://events.rainfocus.com/widget/nvidia/nvidiagtc/sessioncatalog/session/1631029840983001jvzq rel=noopener>The Future of Sparsity in Deep Learning (Trevor Gale, Phd student Stanford, 2021)</a></li><li>#TALK Deep Learning (Yoshua Bengio, MLSS 2020):<ul><li><a href="https://www.youtube.com/watch?v=c_U4THknoHE" rel=noopener>Part I</a></li><li><a href="https://www.youtube.com/watch?v=PDPdIDihPvc" rel=noopener>Part II</a></li></ul></li><li>#TALK
<a href="https://www.youtube.com/watch?v=YzD7Z2yRL7Y" rel=noopener>Deep Learning Hardware: Past, Present, and Future (Yann LeCun, ISSCC 2019)</a></li><li>#TALK
<a href="https://www.youtube.com/watch?v=Bo8MY4JpiXE" rel=noopener>Keras, Deep Learning, and the Progress of AI (François Chollet, Lex Fridman Podcast, 2019)</a></li><li>#TALK
<a href="https://www.youtube.com/watch?v=RM-Jtc2ryfM&t=5s" rel=noopener>Deep Learning and the Future of Artificial Intelligence (Yann LeCun, 2018)</a></li><li>#TALK
<a href="https://www.youtube.com/watch?v=1_KhJv0Em5Y" rel=noopener>AI Breakthroughs & Obstacles to Progress, Mathematical and Otherwise (Yann LeCun, 2018)</a></li><li>#TALK
<a href="https://www.youtube.com/watch?v=MUF32XHqM34" rel=noopener>François Chollet at France is AI 2017: Deep Learning: current limits and future perspectives (Chollet 2017)</a></li><li>#TALK
<a href="https://www.youtube.com/watch?v=0tEhw5t6rhc" rel=noopener>Power & Limits of Deep Learning (Yann Lecun, 2017)</a></li><li>#TALK
<a href="https://www.youtube.com/watch?v=dz_jeuWx3j0" rel=noopener>The Deep End of Deep Learning (Hugo Larochelle, TEDxBoston 2016)</a></li><li>#TALK
<a href="https://www.youtube.com/watch?v=ILsA4nyG7I0" rel=noopener>How deep neural networks work (Brandon Rohrer)</a><ul><li>Simple explanations of DL basics and nice graphics</li></ul></li></ul><h2 id=courses>Courses</h2><ul><li>#COURSE
<a href=https://github.com/YipengHu/COMP0090 rel=noopener>Introduction to Deep Learning (COMP0090, UCL)</a></li><li>#COURSE
<a href=https://fullstackdeeplearning.com/ rel=noopener>Full Stack Deep Learning</a><ul><li><a href=https://fullstackdeeplearning.com/spring2021/ rel=noopener>Full Stack Deep Learning - Spring 2021</a><ul><li><a href=https://fullstackdeeplearning.com/spring2021/lecture-13/ rel=noopener>Lecture 13: ML Teams and Startups</a></li></ul></li><li><a href=https://fall2019.fullstackdeeplearning.com/ rel=noopener>https://fall2019.fullstackdeeplearning.com/</a><ul><li><a href=https://github.com/full-stack-deep-learning/course-gitbook rel=noopener>https://github.com/full-stack-deep-learning/course-gitbook</a></li></ul></li></ul></li><li>#COURSE
<a href=https://atcold.github.io/pytorch-Deep-Learning/ rel=noopener>Deep Learning (NYU)</a><ul><li><a href=https://github.com/Atcold/pytorch-Deep-Learning rel=noopener>https://github.com/Atcold/pytorch-Deep-Learning</a> (pytorch)</li></ul></li><li>#COURSE
<a href=http://cs230.stanford.edu/ rel=noopener>Deep Learning (CS230, Stanford)</a><ul><li><a href=https://github.com/afshinea/stanford-cs-230-deep-learning rel=noopener>Cheatsheets</a></li></ul></li><li>#COURSE
<a href=http://web.stanford.edu/class/cs20si/syllabus.html rel=noopener>Tensorflow for Deep Learning Research (CS20SI, Stanford)</a></li><li>#COURSE
<a href="https://www.youtube.com/playlist?list=PLqYmG7hTraZCDxZ44o4p3N5Anz3lLRVZF" rel=noopener>DeepMind x UCL | Deep Learning Lecture Series 2020</a></li><li>#COURSE
<a href=http://introtodeeplearning.com/ rel=noopener>Introduction to Deep Learning (6.S191, MIT)</a><ul><li><a href="https://www.youtube.com/watch?v=7sB052Pz0sQ" rel=noopener>MIT Introduction to Deep Learning | 6.S191 | 2022</a></li></ul></li><li>#COURSE
<a href=https://deeplearning.mit.edu/ rel=noopener>MIT Deep Learning and Artificial Intelligence Lectures</a><ul><li><a href="https://www.youtube.com/watch?v=0VH1Lim8gL8&list=PLrAXtmErZgOeiKm4sgNOknGvNjby9efdf" rel=noopener>Youtube playlist</a></li><li><a href="https://www.youtube.com/watch?v=0VH1Lim8gL8" rel=noopener>Deep Learning State of the Art (2020)</a></li></ul></li><li>#COURSE
<a href=http://introtodeeplearning.com/ rel=noopener>Introduction to Deep Learning (MIT 6.S191)</a></li><li>#COURSE
<a href=http://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/ rel=noopener>Intro to Neural Networks and Machine Learning (CSC 321, UToronto)</a></li><li>#COURSE
<a href=https://www.udacity.com/course/deep-learning-nanodegree--nd101 rel=noopener>Deep Learning nanodegree (Udacity)</a><ul><li><a href=https://github.com/udacity/deep-learning-v2-pytorch rel=noopener>https://github.com/udacity/deep-learning-v2-pytorch</a></li><li><a href=https://www.udacity.com/course/deep-learning-pytorch--ud188 rel=noopener>https://www.udacity.com/course/deep-learning-pytorch--ud188</a></li></ul></li><li>#COURSE
<a href=https://jovian.ai/learn/deep-learning-with-pytorch-zero-to-gans rel=noopener>Deep Learning with PyTorch: Zero to GANs (Jovian)</a></li><li>#COURSE
<a href=http://course.fast.ai/ rel=noopener>Fast AI - Practical Deep Learning For Coders</a><ul><li>Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD - the book and the course</li><li><a href=https://github.com/fastai/fastbook rel=noopener>https://github.com/fastai/fastbook</a></li></ul></li><li>#COURSE
<a href=https://m2dsupsdlclass.github.io/lectures-labs/ rel=noopener>Deep Learning course (U Paris-Saclay)</a></li><li>#COURSE
<a href=https://albahnsen.com/courses/applied-deep-learning/ rel=noopener>Introduction to Machine Learning and Neural Networks (Uniandes)</a><ul><li><a href=https://github.com/albahnsen/AppliedDeepLearningClass rel=noopener>https://github.com/albahnsen/AppliedDeepLearningClass</a></li></ul></li><li>#COURSE
<a href=https://www.coursera.org/specializations/deep-learning rel=noopener>Deep learning specialization (deeplearning.ai, Coursera, Andrew Ng)</a><ul><li><a href=https://www.deeplearning.ai/deep-learning-specialization/ rel=noopener>https://www.deeplearning.ai/deep-learning-specialization/</a></li></ul></li><li>#COURSE
<a href=http://info.usherbrooke.ca/hlarochelle/neural_networks/description.html rel=noopener>Neural Networks (U Sherbrooke)</a></li><li>#COURSE
<a href=http://ml4a.github.io/classes/itp-F18/ rel=noopener>The Neural Aesthetic (ITP-NYU)</a></li></ul><h2 id=code>Code</h2><p>State of ML frameworks:</p><ul><li><p><a href=https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/ rel=noopener>https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/</a></p></li><li><p><a href=https://towardsdatascience.com/tensorflow-or-pytorch-146f5397278a rel=noopener>https://towardsdatascience.com/tensorflow-or-pytorch-146f5397278a</a></p></li><li><p>#CODE
<a href=/digitalgarden/AI/DS-and-DataEng/Tensorflow-keras rel=noopener class=internal-link data-src=/digitalgarden/AI/DS-and-DataEng/Tensorflow-keras>Tensorflow, keras</a></p></li><li><p>#CODE
<a href=/digitalgarden/AI/DS-and-DataEng/Pytorch rel=noopener class=internal-link data-src=/digitalgarden/AI/DS-and-DataEng/Pytorch>Pytorch</a></p></li><li><p>#CODE
<a href=https://github.com/unifyai/ivy rel=noopener>Ivy</a></p><ul><li>The unified machine learning framework, enabling framework-agnostic functions, layers and libraries</li><li><a href=https://lets-unify.ai/ rel=noopener>lets-unify.ai</a></li><li>#PAPER
<a href=https://arxiv.org/abs/2102.02886 rel=noopener>Ivy: Templated Deep Learning for Inter-Framework Portability (Lenton 2021)</a></li><li><a href=https://medium.com/@unifyai/why-unify-21b502f2015e rel=noopener>https://medium.com/@unifyai/why-unify-21b502f2015e</a></li><li><a href=https://medium.com/@unifyai/standardization-7726c5113e4 rel=noopener>https://medium.com/@unifyai/standardization-7726c5113e4</a></li></ul></li><li><p>#CODE
<a href=https://huggingface.co/ rel=noopener>Huggingface</a></p><ul><li>Build, train and deploy state of the art models powered by the reference open source in ML</li><li><a href=https://github.com/huggingface/datasets rel=noopener>Datasets</a></li><li><a href=https://github.com/huggingface/datasets-viewer rel=noopener>Datasets-viewer</a><ul><li><a href=https://huggingface.co/datasets/viewer/ rel=noopener>https://huggingface.co/datasets/viewer/</a></li></ul></li><li><a href=https://github.com/huggingface/transformers rel=noopener>Transformers</a></li></ul></li><li><p>#CODE
<a href=https://github.com/openai/triton rel=noopener>Triton</a></p><ul><li>language and compiler for writing highly efficient custom Deep-Learning primitives</li><li><a href=https://openai.com/blog/triton/ rel=noopener>https://openai.com/blog/triton/</a></li><li><a href=https://www.infoq.com/news/2021/08/openAI-triton/ rel=noopener>https://www.infoq.com/news/2021/08/openAI-triton/</a></li><li>Triton uses Python as its base. The developer writes code in Python using Triton’s libraries, which are then JIT-compiled to run on the GPU. This allows integration with the rest of the Python ecosystem, currently the biggest destination for developing machine-learning solutions</li></ul></li><li><p>#CODE
<a href=https://github.com/Oneflow-Inc/oneflow rel=noopener>Oneflow</a></p><ul><li>OneFlow is a performance-centered and open-source deep learning framework</li><li><a href=http://www.oneflow.org/ rel=noopener>http://www.oneflow.org/</a></li></ul></li><li><p>#CODE
<a href=https://github.com/mindspore-ai/mindspore rel=noopener>MindSpore (Huawei)</a> ^huaweimindpore</p><ul><li><a href=https://towardsdatascience.com/program-your-first-neural-network-with-huawei-mindspore-1fc50023e90d rel=noopener>https://towardsdatascience.com/program-your-first-neural-network-with-huawei-mindspore-1fc50023e90d</a></li><li><a href=https://towardsdatascience.com/huaweis-mindspore-a-new-competitor-for-tensorflow-and-pytorch-d319deff2aec rel=noopener>https://towardsdatascience.com/huaweis-mindspore-a-new-competitor-for-tensorflow-and-pytorch-d319deff2aec</a></li><li><a href=https://www.mindspore.cn/en rel=noopener>https://www.mindspore.cn/en</a></li></ul></li><li><p>#CODE
<a href=https://github.com/tensorlayer/tensorlayer rel=noopener>Tensorlayer - Deep Learning and Reinforcement Learning Library for Scientists and Engineers</a></p><ul><li><a href=http://tensorlayer.org/ rel=noopener>http://tensorlayer.org/</a></li></ul></li><li><p>#CODE
<a href=https://github.com/poets-ai/elegy rel=noopener>Elegy - Neural Networks framework based on Jax and inspired by Keras</a></p><ul><li><a href=https://poets-ai.github.io/elegy/ rel=noopener>https://poets-ai.github.io/elegy/</a></li><li>See
<a href=/digitalgarden/AI/Math-and-Statistics/Mathematical-Optimization rel=noopener class=internal-link data-src=/digitalgarden/AI/Math-and-Statistics/Mathematical-Optimization>Mathematical Optimization</a> JAX</li></ul></li><li><p>#CODE
<a href=https://github.com/PaddlePaddle/Paddle rel=noopener>Paddle (Baidu)</a></p><ul><li><a href=http://www.paddlepaddle.org/ rel=noopener>http://www.paddlepaddle.org/</a></li><li>PArallel Distributed Deep LEarning: Machine Learning Framework from Industrial Practice</li></ul></li><li><p>#CODE
<a href=https://github.com/apache/incubator-mxnet rel=noopener>Mxnet (Apache)</a></p><ul><li><a href=http://mxnet.io/ rel=noopener>http://mxnet.io/</a></li><li><a href=https://mli.github.io/cvpr17/ rel=noopener>Towards Next Generation Deep Learning Framework</a></li></ul></li><li><p>#CODE
<a href=https://github.com/Microsoft/CNTK rel=noopener>Microsoft Cognitive Toolkit (CNTK)</a></p><ul><li><a href=https://www.microsoft.com/en-us/research/product/cognitive-toolkit/ rel=noopener>https://www.microsoft.com/en-us/research/product/cognitive-toolkit/</a></li><li>Microsoft Cognitive Toolkit: A free, easy-to-use, open-source, commercial-grade toolkit that trains deep learning algorithms to learn like the human brain.</li><li>#TALK
<a href="https://www.youtube.com/watch?v=9gDDO5ldT-4&feature=youtu.be" rel=noopener>https://www.youtube.com/watch?v=9gDDO5ldT-4&feature=youtu.be</a></li></ul></li><li><p>#CODE
<a href=https://github.com/itdxer/neupy rel=noopener>Neupy - NeuPy is a Tensorflow based python library for prototyping and building neural networks</a></p><ul><li><a href=http://neupy.com/pages/home.html rel=noopener>http://neupy.com/pages/home.html</a></li></ul></li><li><p>#CODE Chainer - Chainer is a Python-based deep learning framework aiming at flexibility</p><ul><li><a href=https://github.com/chainer/chainer rel=noopener>https://github.com/chainer/chainer</a></li></ul></li><li><p>#CODE
<a href=https://dl.sony.com/ rel=noopener>Neural Network Console (Sony)</a></p></li><li><p>#CODE
<a href=https://github.com/OpenMined/PySyft rel=noopener>PySyft</a></p><ul><li>PySyft is a Python library for secure and private Deep Learning.</li><li>PySyft decouples private data from model training, using Federated Learning, Differential Privacy, and Encrypted Computation (like Multi-Party Computation (MPC) and Homomorphic Encryption (HE)) within the main Deep Learning frameworks like PyTorch and TensorFlow.</li><li>#PAPER
<a href=https://arxiv.org/abs/1811.04017 rel=noopener>A generic framework for privacy preserving deep learning</a></li></ul></li><li><p>#CODE
<a href=https://deepcognition.ai/ rel=noopener>Deep cognition</a></p></li><li><p>#CODE
<a href=https://github.com/GT4SD/gt4sd-core rel=noopener>Gt4sd-core (IBM)</a></p><ul><li>GT4SD, an open-source library to accelerate hypothesis generation in the scientific discovery process</li><li><a href=https://gt4sd.github.io/gt4sd-core/ rel=noopener>https://gt4sd.github.io/gt4sd-core/</a></li><li><a href=https://research.ibm.com/blog/generative-models-toolkit-for-scientific-discovery rel=noopener>https://research.ibm.com/blog/generative-models-toolkit-for-scientific-discovery</a></li><li><a href=https://thenewstack.io/ibms-open-source-gt4sd-generates-ideas-for-scientists/ rel=noopener>https://thenewstack.io/ibms-open-source-gt4sd-generates-ideas-for-scientists/</a></li></ul></li></ul><h2 id=references>References</h2><ul><li>#PAPER
<a href=https://www.sciencedirect.com/science/article/pii/S0893608014002135 rel=noopener>Deep learning in NNs: An overview (Schmidhuber 2015)</a></li><li>#PAPER
<a href=https://www.nature.com/articles/nature14539 rel=noopener>Deep learning (LeCun 2015)</a> ^dllecun15<ul><li><a href=https://www.researchgate.net/profile/Y_Bengio/publication/277411157_Deep_Learning/links/55e0cdf908ae2fac471ccf0f/Deep-Learning.pdf rel=noopener>https://www.researchgate.net/profile/Y_Bengio/publication/277411157_Deep_Learning/links/55e0cdf908ae2fac471ccf0f/Deep-Learning.pdf</a></li></ul></li><li>#PAPER
<a href=https://www.ijcai.org/Proceedings/16/Papers/628.pdf rel=noopener>Deep Neural Decision Forests (Kontschieder 2016)</a><ul><li>#CODE
<a href=https://keras.io/examples/structured_data/deep_neural_decision_forests/ rel=noopener>https://keras.io/examples/structured_data/deep_neural_decision_forests/</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1702.07800v4 rel=noopener>On the Origin of Deep Learning (Wang 2017)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1707.09873v1 rel=noopener>Representation Learning on Large and Small Data (Chou 2017)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1404.7828 rel=noopener>Deep Learning in Neural Networks: An Overview (Schmidhuber, 2018)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1710.11573 rel=noopener>Deep Learning as a Mixed Convex-Combinatorial Optimization Problem (Friesen 2018)</a></li><li>#PAPER
<a href=https://ieeexplore.ieee.org/document/8253590 rel=noopener>Using Deep Neural Networks for Inverse Problems in Imaging: Beyond Analytical Methods (Lucas, 2018)</a><ul><li><a href=http://decsai.ugr.es/vip/files/journals/08253590.pdf rel=noopener>http://decsai.ugr.es/vip/files/journals/08253590.pdf</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1806.07572# rel=noopener>Neural Tangent Kernel: Convergence and Generalization in Neural Networks (Jacot 2018)</a><ul><li><a href=https://www.quantamagazine.org/a-new-link-to-an-old-model-could-crack-the-mystery-of-deep-learning-20211011/ rel=noopener>https://www.quantamagazine.org/a-new-link-to-an-old-model-could-crack-the-mystery-of-deep-learning-20211011/</a></li></ul></li><li>#PAPER
<a href=https://www.nature.com/articles/s42256-020-00237-3 rel=noopener>Neural circuit policies enabling auditable autonomy (Lechner 2020)</a><ul><li>#CODE
<a href=https://github.com/mlech26l/keras-ncp rel=noopener>https://github.com/mlech26l/keras-ncp</a></li><li><a href=https://www.csail.mit.edu/news/new-deep-learning-models-require-fewer-neurons rel=noopener>https://www.csail.mit.edu/news/new-deep-learning-models-require-fewer-neurons</a></li><li><a href=https://www.marktechpost.com/2021/10/19/mit-csail-tu-wien-and-ist-researchers-introduce-deep-learning-models-that-require-fewer-neurons/ rel=noopener>https://www.marktechpost.com/2021/10/19/mit-csail-tu-wien-and-ist-researchers-introduce-deep-learning-models-that-require-fewer-neurons/</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2003.01822 rel=noopener>Implicitly Defined Layers in Neural Networks (Zhang 2020)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/2102.09235 rel=noopener>A Mathematical Principle of Deep Learning: Learn the Geodesic Curve in the Wasserstein Space (Gai 2021)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/2104.00008 rel=noopener>Why is AI hard and Physics simple? (Roberts 2021)</a></li><li>#PAPER
<a href=https://dl.acm.org/doi/10.1145/3448250 rel=noopener>Deep Learning for AI (By Yoshua Bengio, Yann Lecun, Geoffrey Hinton, Turing lecture, 2021)</a><ul><li><a href=https://cacm.acm.org/magazines/2021/7/253464-deep-learning-for-ai/fulltex rel=noopener>https://cacm.acm.org/magazines/2021/7/253464-deep-learning-for-ai/fulltex</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2102.12903 rel=noopener>Self-Tuning for Data-Efficient Deep Learning (Wang 2021)</a><ul><li>#CODE
<a href=https://github.com/thuml/Self-Tuning rel=noopener>https://github.com/thuml/Self-Tuning</a></li><li>#TALK
<a href="https://recorder-v3.slideslive.com/#/share?share=40334&s=f7988e61-bece-4a7a-a6ba-3e1a2b49b37b" rel=noopener>https://recorder-v3.slideslive.com/#/share?share=40334&s=f7988e61-bece-4a7a-a6ba-3e1a2b49b37b</a></li></ul></li><li>#PAPER
<a href=https://www.nature.com/articles/s42256-020-00237-3 rel=noopener>Neural circuit policies enabling auditable autonomy (Lechner 2021)</a><ul><li>#CODE
<a href=https://github.com/mlech26l/keras-ncp rel=noopener>https://github.com/mlech26l/keras-ncp</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2106.07804 rel=noopener>Controlling Neural Networks with Rule Representations (Seo 2021)</a><ul><li><a href=https://ai.googleblog.com/2022/01/controlling-neural-networks-with-rule.html rel=noopener>https://ai.googleblog.com/2022/01/controlling-neural-networks-with-rule.html</a></li></ul></li><li>#PAPER
<a href=https://www.nature.com/articles/s41586-021-04223-6 rel=noopener>Deep physical neural networks trained with backpropagation (Wrigth 2022)</a></li><li>#PAPER
<a href=https://arxiv.org/pdf/2104.02395 rel=noopener>Ensemble deep learning: A review (Ganaie 2022)</a></li><li>#PAPER
<a href=https://arxiv.org/pdf/2203.05483 rel=noopener>projUNN: efficient method for training deep networks with unitary matrices (Kiani 2022)</a><ul><li><a href=https://www.marktechpost.com/2022/04/09/researchers-including-yann-lecun-propose-projunn-an-efficient-method-for-training-deep-neural-networks-with-unitary-matrices/ rel=noopener>https://www.marktechpost.com/2022/04/09/researchers-including-yann-lecun-propose-projunn-an-efficient-method-for-training-deep-neural-networks-with-unitary-matrices/</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/pdf/2204.02965 rel=noopener>LilNetX: Lightweight Networks with EXtreme Model Compression and Structured Sparsification (Girish 2022)</a><ul><li><a href=https://www.marktechpost.com/2022/04/12/researchers-propose-a-novel-framework-lilnetx-for-training-deep-neural-network-with-extreme-model-compression-and-structured-sparsification/ rel=noopener>https://www.marktechpost.com/2022/04/12/researchers-propose-a-novel-framework-lilnetx-for-training-deep-neural-network-with-extreme-model-compression-and-structured-sparsification/</a></li></ul></li></ul><h3 id=generalization>Generalization</h3><ul><li><p><a href=http://www.inference.vc/everything-that-works-works-because-its-bayesian-2/ rel=noopener>http://www.inference.vc/everything-that-works-works-because-its-bayesian-2/</a></p></li><li><p>#PAPER
<a href=https://arxiv.org/abs/1611.03530 rel=noopener>Understanding deep learning requires re-thinking generalization (Zhang 2016)</a></p><ul><li><a href=https://blog.acolyer.org/2017/05/11/understanding-deep-learning-requires-re-thinking-generalization/ rel=noopener>https://blog.acolyer.org/2017/05/11/understanding-deep-learning-requires-re-thinking-generalization/</a></li><li><a href=https://www.quora.com/Why-is-the-paper-%E2%80%9CUnderstanding-Deep-Learning-Requires-Rethinking-Generalization%E2%80%9D-important rel=noopener>https://www.quora.com/Why-is-the-paper-%E2%80%9CUnderstanding-Deep-Learning-Requires-Rethinking-Generalization%E2%80%9D-important</a></li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/abs/1706.05394 rel=noopener>A Closer Look at Memorization in Deep Networks (Arpit 2017)</a></p></li><li><p>#PAPER
<a href="https://openreview.net/pdf?id=rJv6ZgHYg" rel=noopener>Deep nets don’t learn via memorization (Krueger 2017)</a></p></li><li><p>#PAPER
<a href=https://arxiv.org/abs/1710.09553 rel=noopener>Rethinking generalization requires revisiting old ideas: statistical mechanics approaches and complex learning behavior (Martin 2017)</a></p></li><li><p>#PAPER
<a href=https://arxiv.org/abs/1901.08644 rel=noopener>Ablation Studies in Artificial Neural Networks (Meyes 2019)</a></p></li><li><p>#PAPER
<a href=https://arxiv.org/abs/2012.09816 rel=noopener>Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning (Allen-Zhu 2020)</a></p><ul><li><a href=https://www.microsoft.com/en-us/research/blog/three-mysteries-in-deep-learning-ensemble-knowledge-distillation-and-self-distillation/ rel=noopener>https://www.microsoft.com/en-us/research/blog/three-mysteries-in-deep-learning-ensemble-knowledge-distillation-and-self-distillation/</a></li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/abs/2010.08127 rel=noopener>The Deep Bootstrap Framework: Good Online Learners are Good Offline Generalizers (Nakkiran 2021)</a></p><ul><li><a href=https://ai.googleblog.com/2021/03/a-new-lens-on-understanding.html rel=noopener>https://ai.googleblog.com/2021/03/a-new-lens-on-understanding.html</a></li></ul></li><li><p>#PAPER
<a href=https://www.nature.com/articles/s41467-021-24025-8 rel=noopener>Predicting trends in the quality of state-of-the-art neural networks without access to training or testing data (Martin 2021)</a></p></li><li><p>#PAPER
<a href=https://arxiv.org/abs/2109.14119 rel=noopener>Stochastic Training is Not Necessary for Generalization (Geiping 2021)</a></p></li><li><p>#PAPER
<a href=https://arxiv.org/abs/2011.03395 rel=noopener>Underspecification Presents Challenges for Credibility in Modern Machine Learning (D&rsquo;Amour 2021)</a></p><ul><li><a href=https://ai.googleblog.com/2021/10/how-underspecification-presents.html rel=noopener>https://ai.googleblog.com/2021/10/how-underspecification-presents.html</a></li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/abs/2110.09485 rel=noopener>Learning in High Dimension Always Amounts to Extrapolation (Balestriero 2021)</a></p><ul><li>In order for NNs to succeed at solving a task, they have to operate in the “extrapolation” regime! But not all of them generalise as well as others. So this opens up new questions about the relationship between this specific notion of extrapolation and generalisation more generally.</li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/abs/2002.03061 rel=noopener>Incorporating Symmetry into Deep Dynamics Models for Improved Generalization (Wang 2021)</a></p><ul><li>#CODE
<a href=https://github.com/Rose-STL-Lab/Equivariant-Net rel=noopener>https://github.com/Rose-STL-Lab/Equivariant-Net</a></li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/abs/2201.02177v1 rel=noopener>Grokking - Generatlization beyond overfitting on small algorithmic datasets (Power 2022)</a></p><ul><li><a href="https://www.youtube.com/watch?v=dND-7llwrpw" rel=noopener>Paper explained</a></li></ul></li></ul><h3 id=regularization>Regularization</h3><ul><li>In general, techniques aimed at reducing overfitting and improve generalization</li><li><a href=https://www.tensorflow.org/tutorials/keras/overfit_and_underfit rel=noopener>Overfit and underfit</a></li><li><a href=https://theaisummer.com/regularization/ rel=noopener>Regularization techniques for training deep neural networks</a></li><li><a href=https://towardsdatascience.com/regularization-in-deep-learning-l1-l2-and-dropout-377e75acc036 rel=noopener>https://towardsdatascience.com/regularization-in-deep-learning-l1-l2-and-dropout-377e75acc036</a></li><li><a href=https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-learning-with-weight-regularization/ rel=noopener>https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-learning-with-weight-regularization/</a></li><li><a href=https://medium.com/intelligentmachines/convolutional-neural-network-and-regularization-techniques-with-tensorflow-and-keras-5a09e6e65dc7 rel=noopener>https://medium.com/intelligentmachines/convolutional-neural-network-and-regularization-techniques-with-tensorflow-and-keras-5a09e6e65dc7</a></li></ul><h4 id=data-augmentation>Data augmentation</h4><ul><li><a href=https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html rel=noopener>https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html</a></li><li><a href=https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/ rel=noopener>https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/</a></li><li>#PAPER
<a href=https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0 rel=noopener>A survey on Image Data Augmentation for Deep Learning (Shorten 2019)</a></li><li>#PAPER
<a href=https://arxiv.org/pdf/1805.09501 rel=noopener>AutoAugment: Learning Augmentation Policies from Data (Cubuk 2019)</a></li></ul><h4 id=dropout>Dropout</h4><ul><li><p><a href=http://www.cs.toronto.edu/~hinton/absps/dropout.pdf rel=noopener>http://www.cs.toronto.edu/~hinton/absps/dropout.pdf</a></p></li><li><p><a href=https://pgaleone.eu/deep-learning/regularization/2017/01/10/anaysis-of-dropout/ rel=noopener>https://pgaleone.eu/deep-learning/regularization/2017/01/10/anaysis-of-dropout/</a></p></li><li><p><a href=https://towardsdatascience.com/12-main-dropout-methods-mathematical-and-visual-explanation-58cdc2112293 rel=noopener>12 Main Dropout Methods: Mathematical and Visual Explanation for DNNs, CNNs, and RNNs</a></p></li><li><p>#PAPER
<a href=http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf rel=noopener>Dropout: A Simple Way to Prevent Neural Networks from Overfitting (Srivastava 2014)</a></p></li><li><p>#PAPER
<a href=https://arxiv.org/abs/1411.4280v3 rel=noopener>Efficient Object Localization Using Convolutional Networks (Tompson 2015)</a></p><ul><li>Proposed spatial dropout</li></ul></li><li><p>#PAPER
<a href=https://link.springer.com/chapter/10.1007/978-3-319-54184-6_12 rel=noopener>Analysis on the Dropout Effect in Convolutional Neural Networks (Park 2017)</a></p><ul><li><a href=http://mipal.snu.ac.kr/images/1/16/Dropout_ACCV2016.pdf rel=noopener>http://mipal.snu.ac.kr/images/1/16/Dropout_ACCV2016.pdf</a></li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/abs/1904.03392 rel=noopener>Effective and Efficient Dropout for Deep Convolutional Neural Networks (Cai 2020)</a></p></li></ul><h3 id=stochastic-depth>Stochastic depth</h3><ul><li>#PAPER
<a href=https://arxiv.org/pdf/1603.09382 rel=noopener>Deep Networks with Stochastic Depth (Huang 2016)</a><ul><li>Stochastic depth is a regularization technique that randomly drops a set of layers. During inference, the layers are kept as they are. It is very much similar to Dropout but only that it operates on a block of layers rather than individual nodes present inside a layer</li></ul></li></ul><h4 id=normalization>Normalization</h4><ul><li><p>Normalization techniques also improve generalization error, providing some regularization</p></li><li><p><a href=https://medium.com/techspace-usict/normalization-techniques-in-deep-neural-networks-9121bf100d8 rel=noopener>Normalization Techniques in Deep Neural Networks</a></p></li><li><p><a href=https://towardsdatascience.com/different-types-of-normalization-in-tensorflow-dac60396efb0 rel=noopener>Different Types of Normalization in Tensorflow</a></p></li><li><p><a href=https://arthurdouillard.com/post/normalization/ rel=noopener>Normalization in Deep Learning</a></p></li><li><p><a href=https://sebastianraschka.com/faq/docs/scale-training-test.html rel=noopener>https://sebastianraschka.com/faq/docs/scale-training-test.html</a></p></li><li><p>Data normalization/standardization can be used as an alternative (before training) to synch batchnorm (multi-gpu training)</p></li><li><p><a href=https://sthalles.github.io/advanced_gans/ rel=noopener>Spectral normalization</a></p></li><li><p>#PAPER
<a href=https://arxiv.org/abs/2009.12836 rel=noopener>Normalization Techniques in Training DNNs: Methodology, Analysis and Application (Huang 2020)</a></p></li></ul><h5 id=batchnorm>BatchNorm</h5><ul><li>#PAPER
<a href=https://arxiv.org/abs/1502.03167 rel=noopener>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (Ioffe 2015)</a><ul><li>#TALK
<a href="https://www.youtube.com/watch?v=ZOabsYbmBRM&feature=youtu.be" rel=noopener>https://www.youtube.com/watch?v=ZOabsYbmBRM&feature=youtu.be</a></li><li><a href=http://stackoverflow.com/questions/34716454/where-do-i-call-the-batchnormalization-function-in-keras rel=noopener>http://stackoverflow.com/questions/34716454/where-do-i-call-the-batchnormalization-function-in-keras</a></li><li>Slower convergence w/o BN, BN can be applied on top of standardization</li><li>Synch BatchNorm appears in TF 2.2, for multi-gpu training<ul><li><a href=https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/SyncBatchNormalization rel=noopener>https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/SyncBatchNormalization</a></li></ul></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1905.05928 rel=noopener>Rethinking the Usage of Batch Normalization and Dropout (Chen 2019)</a></li></ul><h3 id=activations>Activations</h3><ul><li><a href=https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html rel=noopener>https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html</a></li><li><a href=https://mlfromscratch.com/activation-functions-explained/ rel=noopener>https://mlfromscratch.com/activation-functions-explained/</a></li><li><a href=https://en.wikipedia.org/wiki/Rectifier_%28neural_networks%29 rel=noopener>RELU</a><ul><li><a href=https://www.quora.com/What-is-special-about-rectifier-neural-units-used-in-NN-learning rel=noopener>https://www.quora.com/What-is-special-about-rectifier-neural-units-used-in-NN-learning</a></li></ul></li><li><a href=http://stats.stackexchange.com/questions/126238/what-are-the-advantages-of-relu-over-sigmoid-function-in-deep-neural-network rel=noopener>http://stats.stackexchange.com/questions/126238/what-are-the-advantages-of-relu-over-sigmoid-function-in-deep-neural-network</a></li><li><a href=https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions rel=noopener>https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions</a></li></ul><h3 id=losscost-functions>Loss/Cost functions</h3><ul><li>Cross entropy<ul><li><a href=http://neuralnetworksanddeeplearning.com/chap3.html rel=noopener>http://neuralnetworksanddeeplearning.com/chap3.html</a></li><li><a href=https://en.wikipedia.org/wiki/Cross_entropy rel=noopener>https://en.wikipedia.org/wiki/Cross_entropy</a></li><li><a href=http://www.kdnuggets.com/2017/02/gentlest-introduction-tensorflow-part-4.html rel=noopener>http://www.kdnuggets.com/2017/02/gentlest-introduction-tensorflow-part-4.html</a></li></ul></li><li>Perceptual loss, image reconstruction<ul><li><a href=https://arxiv.org/pdf/1511.06409.pdf rel=noopener>https://arxiv.org/pdf/1511.06409.pdf</a> (Learning to Generate Images With Perceptual Similarity Metrics)</li><li>#PAPER
<a href=https://arxiv.org/abs/1511.08861 rel=noopener>Loss Functions for Image Restoration with Neural Networks (Zhao 2018)</a></li><li><a href=https://medium.com/@sanari85/rediscovery-of-ssim-index-in-image-reconstruction-ssim-as-a-loss-function-a1ffef7d2be rel=noopener>https://medium.com/@sanari85/rediscovery-of-ssim-index-in-image-reconstruction-ssim-as-a-loss-function-a1ffef7d2be</a><ul><li>We use three different metric for comparing each different methods such as DSSIM, MSE, and MAE. Structural dissimilarity(DSSIM) is an image distance metric, that corresponds better to the human perception than MAE or RMSE. Mean Squared Error (MSE) measures the average of the squares of the errors that is, the average squared difference between the estimated values and the actual value. Mean Absolute Error (MAE) is the average distance between each pixel point.
<a href=https://arxiv.org/abs/2001.05372 rel=noopener>https://arxiv.org/abs/2001.05372</a></li></ul></li></ul></li><li><a href=https://towardsdatascience.com/deep-learning-image-enhancement-insights-on-loss-function-engineering-f57ccbb585d7 rel=noopener>Deep learning image enhancement insights on loss function engineering</a></li><li>Mean squared logarithmic error<ul><li><a href=https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/mean-squared-logarithmic-error-%28msle%29 rel=noopener>https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/mean-squared-logarithmic-error-(msle)</a></li><li><a href=https://medium.com/@olegrybkin_20684/the-reasonable-ineffectiveness-of-mse-pixel-loss-for-future-prediction-and-what-to-do-about-it-4dca8152355d rel=noopener>https://medium.com/@olegrybkin_20684/the-reasonable-ineffectiveness-of-mse-pixel-loss-for-future-prediction-and-what-to-do-about-it-4dca8152355d</a></li></ul></li></ul><h3 id=optimizers-and-backpropagation>Optimizers and backpropagation</h3><ul><li><p><a href=https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/ rel=noopener>How to use Learning Curves to Diagnose Machine Learning Model Performance</a></p></li><li><p><a href=https://www.quora.com/Intuitively-how-does-mini-batch-size-affect-the-performance-of-stochastic-gradient-descent rel=noopener>https://www.quora.com/Intuitively-how-does-mini-batch-size-affect-the-performance-of-stochastic-gradient-descent</a></p></li><li><p><a href=https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/ rel=noopener>Keras optimizers</a></p></li><li><p><a href=http://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/ rel=noopener>Adam</a></p></li><li><p><a href=https://ruder.io/optimizing-gradient-descent/index.html#otherrecentoptimizers rel=noopener>An overview of gradient descent optimization algorithms (2016)</a></p></li><li><p><a href=https://hackernoon.com/some-state-of-the-art-optimizers-in-neural-networks-a3c2ba5a5643 rel=noopener>https://hackernoon.com/some-state-of-the-art-optimizers-in-neural-networks-a3c2ba5a5643</a></p></li><li><p><a href=https://www.jeremyjordan.me/neural-networks-training/ rel=noopener>https://www.jeremyjordan.me/neural-networks-training/</a></p></li><li><p><a href=http://colah.github.io/posts/2015-08-Backprop/ rel=noopener>http://colah.github.io/posts/2015-08-Backprop/</a></p></li><li><p><a href=https://github.com/DebPanigrahi/Machine-Learning/blob/master/back_prop.ipynb rel=noopener>Back-propagation - Math Simplified</a></p></li><li><p><a href=https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/ rel=noopener>https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/</a></p></li><li><p><a href=https://venturebeat.com/2020/12/16/at-neurips-2020-researchers-proposed-faster-more-efficient-alternatives-to-backpropagation/amp/ rel=noopener>https://venturebeat.com/2020/12/16/at-neurips-2020-researchers-proposed-faster-more-efficient-alternatives-to-backpropagation/amp/</a></p></li><li><p>#PAPER
<a href=https://arxiv.org/abs/1609.04836 rel=noopener>On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima (Shirish Keshkar 2017)</a></p></li><li><p>#PAPER
<a href=https://arxiv.org/abs/1706.02677 rel=noopener>Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour (Goyal 2018)</a></p></li><li><p>#PAPER
<a href=https://arxiv.org/abs/1711.05101 rel=noopener>Decoupled Weight Decay Regularization (Loshchilov 2018)</a></p><ul><li>AdamW optimizer</li><li>#CODE
<a href=https://www.tensorflow.org/addons/api_docs/python/tfa/optimizers/AdamW rel=noopener>https://www.tensorflow.org/addons/api_docs/python/tfa/optimizers/AdamW</a></li><li><a href=https://www.fast.ai/2018/07/02/adam-weight-decay/ rel=noopener>https://www.fast.ai/2018/07/02/adam-weight-decay/</a></li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/abs/1912.02292 rel=noopener>Deep Double Descent: Where Bigger Models and More Data Hurt (Nakkiran 2019)</a></p><ul><li><a href=https://openai.com/blog/deep-double-descent/ rel=noopener>https://openai.com/blog/deep-double-descent/</a></li><li><a href=https://medium.com/mlearning-ai/double-descent-8f92dfdc442f rel=noopener>https://medium.com/mlearning-ai/double-descent-8f92dfdc442f</a></li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/abs/1812.11118 rel=noopener>Reconciling modern machine learning practice and the bias-variance trade-off (Belkin 2019)</a></p><ul><li><a href="https://www.youtube.com/watch?v=ZAW9EyNo2fw" rel=noopener>Paper explained</a></li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/abs/1912.02292 rel=noopener>Deep Double Descent: Where Bigger Models and More Data Hurt (Nakkiran 2020)</a></p><ul><li><a href=https://openai.com/blog/deep-double-descent/ rel=noopener>https://openai.com/blog/deep-double-descent/</a></li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/abs/2007.01547 rel=noopener>Descending through a Crowded Valley &ndash; Benchmarking Deep Learning Optimizers (Schmidt 2020)</a></p><ul><li><a href="https://www.youtube.com/watch?v=DiNzQP7kK-s" rel=noopener>Paper explained</a></li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/abs/2007.10099 rel=noopener>Early Stopping in Deep Networks: Double Descent and How to Eliminate it (Heckel 2020)</a></p><ul><li>contrary to model-wise double descent, epoch-wise double descent is not a phenomena tied o over-parameterization</li><li>both under- and overparameterized models can have epoch-wise double descent</li><li>#CODE
<a href=https://github.com/MLI-lab/early_stopping_double_descent rel=noopener>https://github.com/MLI-lab/early_stopping_double_descent</a></li></ul></li></ul><h3 id=efficiency-and-performance>Efficiency and performance</h3><ul><li>#PAPER
<a href=https://arxiv.org/abs/2106.08962 rel=noopener>Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better (Menghani 2021)</a><ul><li><a href=https://analyticsindiamag.com/how-to-build-smaller-faster-better-deep-learning-models/ rel=noopener>https://analyticsindiamag.com/how-to-build-smaller-faster-better-deep-learning-models/</a></li></ul></li></ul><h3 id=attention>Attention</h3><p>See &ldquo;For NLP&rdquo; section in <a href=/digitalgarden/AI/Deep-learning/Transformers rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Transformers>AI/Deep learning/Transformers</a> and &ldquo;Channel/Visual attention&rdquo; section in <a href=/digitalgarden/AI/Deep-learning/CNNs rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/CNNs>/AI/Deep learning/CNNs</a></p><ul><li>#COURSE
<a href="https://www.youtube.com/watch?v=AIiwuClvH6k" rel=noopener>Attention and Memory in Deep Learning (DeepMind x UCL | Deep Learning Lectures | 8/12)</a></li></ul><h3 id=explainability-methods-for-neural-networks>Explainability methods for Neural Networks</h3><p>See <a href=/digitalgarden/AI/Deep-learning/Explainability-methods-for-NNs rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Explainability-methods-for-NNs>AI/Deep learning/Explainability methods for NNs</a></p><h2 id=applications>Applications</h2><h3 id=deep-learning-for-multi-dimensional-data>Deep learning for multi-dimensional data</h3><p>See <a href=/digitalgarden/AI/Computer-Vision/Video-segmentation-and-prediction rel=noopener class=internal-link data-src=/digitalgarden/AI/Computer-Vision/Video-segmentation-and-prediction>AI/Computer Vision/Video segmentation and prediction</a>, <a href=/digitalgarden/AI/Deep-learning/Encoder-decoder-networks rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Encoder-decoder-networks>AI/Deep learning/Encoder-decoder networks</a>, <a href=/digitalgarden/AI/Deep-learning/Transformers rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Transformers>AI/Deep learning/Transformers</a> and <a href=/digitalgarden/AI/Deep-learning/Generative-modelling rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Generative-modelling>AI/Deep learning/Generative modelling</a></p><ul><li>#PAPER
<a href=https://arxiv.org/abs/2009.06304 rel=noopener>Demystifying Deep Learning in Predictive Spatio-Temporal Analytics: An Information-Theoretic Framework (Tan 2020)</a></li></ul><h3 id=deep-learning-for-tabular-data>Deep learning for tabular data</h3><ul><li><a href=https://www.fast.ai/2018/04/29/categorical-embeddings/ rel=noopener>An Introduction to Deep Learning for Tabular Data</a></li><li><a href=https://pdf.co/blog/deep-learning-on-tabular-data-using-tensorflow-20 rel=noopener>Applying Deep Learning on Tabular Data Using TensorFlow 2.0</a></li><li><a href=https://sebastianraschka.com/blog/2022/deep-learning-for-tabular-data.html rel=noopener>A short chronology of deep learning for tabular data (Sebastian Rschka)</a></li><li>#CODE See Pytorch tabular in <a href=/digitalgarden/AI/DS-and-DataEng/Pytorch rel=noopener class=internal-link data-src=/digitalgarden/AI/DS-and-DataEng/Pytorch>AI/DS and DataEng/Pytorch</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1909.06312 rel=noopener>Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data (Popov 2019)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1908.07442 rel=noopener>TabNet: Attentive Interpretable Tabular Learning (Arik 2020)</a></li><li>#PAPER
<a href=https://www.nature.com/articles/s41598-021-90923-y rel=noopener>Converting tabular data into images for deep learning with convolutional neural networks (Zhu 2021)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/2106.03253 rel=noopener>Tabular Data: Deep Learning is Not All You Need (Shwartz-Ziv 2021)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/2106.05239 rel=noopener>XBNet: An Extremely Boosted Neural Network (Sarkar 2021)</a><ul><li>#CODE
<a href=https://github.com/tusharsarkar3/XBNet rel=noopener>XBNet</a></li><li>Boosted neural network for tabular data</li><li><a href=https://analyticsindiamag.com/guide-to-xbnet-an-extremely-boosted-neural-network/ rel=noopener>https://analyticsindiamag.com/guide-to-xbnet-an-extremely-boosted-neural-network/</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2106.11959 rel=noopener>Revisiting Deep Learning Models for Tabular Data (Gorishniy 2021)</a><ul><li>#CODE
<a href=https://github.com/yandex-research/rtdl rel=noopener>RDTL (Yandex)</a></li><li><a href=https://yandex-research.github.io/rtdl/ rel=noopener>https://yandex-research.github.io/rtdl/</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2105.02584v1 rel=noopener>TABBIE: Pretrained Representations of Tabular Data (Lida 2021)</a></li></ul><h3 id=deep-learning-for-scientific-discovery>Deep learning for scientific discovery</h3><p>See <a href=/digitalgarden/AI/Deep-learning/Neural-ODEs rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Neural-ODEs>AI/Deep learning/Neural ODEs</a></p><ul><li>#CODE See GT4SD library in Code section</li><li>#PAPER
<a href=https://arxiv.org/abs/2003.11755 rel=noopener>A Survey of Deep Learning for Scientific Discovery (Raghu & Schmidt, 2020)</a> ^dlscience20</li><li>#PAPER
<a href=https://arxiv.org/abs/1907.04502 rel=noopener>DeepXDE: A deep learning library for solving differential equations (Lu 2020)</a><ul><li>#CODE
<a href=https://github.com/lululxvi/deepxde rel=noopener>https://github.com/lululxvi/deepxde</a></li><li><a href=https://deepxde.readthedocs.io/en/latest/ rel=noopener>https://deepxde.readthedocs.io/en/latest/</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2202.07575 rel=noopener>SciANN: A Keras/Tensorflow wrapper for scientific computations and physics-informed deep learning using artificial neural networks (Haghighat 2020)</a><ul><li>#CODE
<a href=https://github.com/sciann/sciann rel=noopener>https://github.com/sciann/sciann</a></li></ul></li><li>#PAPER
<a href=http://ai.googleblog.com/2021/06/learning-accurate-physics-simulator-via.html rel=noopener>Learning an Accurate Physics Simulator via Adversarial Reinforcement Learning (Jiang 2021)</a></li><li>#PAPER
<a href=https://www.nature.com/articles/s41598-022-08745-5 rel=noopener>Data-driven discovery of Green’s functions with human-understandable deep learning (Boulle 2022)</a><ul><li><a href=https://phys.org/news/2022-04-rational-neural-network-advances-partial.html rel=noopener>https://phys.org/news/2022-04-rational-neural-network-advances-partial.html</a></li></ul></li></ul><h3 id=multimodal-learning>Multimodal learning</h3><p>See <a href=/digitalgarden/AI/Deep-learning/Multimodal-learning rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Multimodal-learning>AI/Deep learning/Multimodal learning</a></p><h3 id=dl-for-nlp-time-series-and-sequence-modelling>DL for NLP, time series and sequence modelling</h3><p>See <a href=/digitalgarden/AI/Time-Series-analysis rel=noopener class=internal-link data-src=/digitalgarden/AI/Time-Series-analysis>AI/Time Series analysis</a>, <a href=/digitalgarden/AI/Forecasting rel=noopener class=internal-link data-src=/digitalgarden/AI/Forecasting>AI/Forecasting</a> and &ldquo;Deep learning approaches&rdquo; in <a href=/digitalgarden/AI/NLP rel=noopener class=internal-link data-src=/digitalgarden/AI/NLP>AI/NLP</a></p><h2 id=architectures-and-model-families>Architectures and model families</h2><ul><li><a href=http://www.asimovinstitute.org/neural-network-zoo/ rel=noopener>The neural network zoo</a></li><li><a href=https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-deep-learning-tips-and-tricks rel=noopener>Deep Learning Tips and Tricks cheatsheet</a></li><li><a href=https://jalammar.github.io/visual-interactive-guide-basics-neural-networks/ rel=noopener>A Visual and Interactive Guide to the Basics of NNs</a></li><li><a href=https://jalammar.github.io/feedforward-neural-networks-visual-interactive/ rel=noopener>A Visual And Interactive Look at Basic Neural Network Math</a></li><li>#CODE
<a href=https://modelzoo.co/ rel=noopener>Model Zoo</a></li><li>#CODE
<a href=https://github.com/rasbt/deeplearning-models rel=noopener>Deep Learning Models (Raschka)</a></li></ul><h3 id=geometric-dl>Geometric DL</h3><p>See <a href=/digitalgarden/AI/Deep-learning/Geometric-deep-learning rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Geometric-deep-learning>AI/Deep learning/Geometric deep learning</a></p><h3 id=mlps>MLPs</h3><p>See <a href=/digitalgarden/AI/Deep-learning/MLPs rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/MLPs>AI/Deep learning/MLPs</a></p><h3 id=deep-belief-network>Deep belief network</h3><p>See <a href=/digitalgarden/AI/Deep-learning/Deep-belief-network rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Deep-belief-network>AI/Deep learning/Deep belief network</a></p><h3 id=autoencoders>Autoencoders</h3><p>See <a href=/digitalgarden/AI/Deep-learning/Autoencoders rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Autoencoders>AI/Deep learning/Autoencoders</a></p><h3 id=cnns>CNNs</h3><p>See <a href=/digitalgarden/AI/Deep-learning/CNNs rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/CNNs>AI/Deep learning/CNNs</a></p><h3 id=rnns>RNNs</h3><p>See <a href=/digitalgarden/AI/Deep-learning/RNNs rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/RNNs>AI/Deep learning/RNNs</a></p><h3 id=capsnets>CapsNets</h3><p>See <a href=/digitalgarden/AI/Deep-learning/CapsNets rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/CapsNets>AI/Deep learning/CapsNets</a></p><h3 id=gans>GANs</h3><p>See <a href=/digitalgarden/AI/Deep-learning/GANs rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/GANs>AI/Deep learning/GANs</a></p><h3 id=diffusion-models>Diffusion models</h3><p>See <a href=/digitalgarden/AI/Deep-learning/Diffusion-models rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Diffusion-models>AI/Deep learning/Diffusion models</a></p><h3 id=gnns>GNNs</h3><p>See <a href=/digitalgarden/AI/Deep-learning/GNNs rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/GNNs>AI/Deep learning/GNNs</a></p><h3 id=residual-and-dense-neural-networks>Residual and dense neural networks</h3><p>See <a href=/digitalgarden/AI/Deep-learning/Residual-and-dense-neural-networks rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Residual-and-dense-neural-networks>AI/Deep learning/Residual and dense neural networks</a></p><h3 id=neural-odes>Neural ODEs</h3><p>See <a href=/digitalgarden/AI/Deep-learning/Neural-ODEs rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Neural-ODEs>AI/Deep learning/Neural ODEs</a></p><h3 id=fourier-neural-operators>Fourier Neural Operators</h3><p>See <a href=/digitalgarden/AI/Deep-learning/Fourier-Neural-Operators rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Fourier-Neural-Operators>AI/Deep learning/Fourier Neural Operators</a></p><h3 id=transformers>Transformers</h3><p>See <a href=/digitalgarden/AI/Deep-learning/Transformers rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Transformers>AI/Deep learning/Transformers</a></p><h3 id=gflownets>GFlowNets</h3><p>See <a href=/digitalgarden/AI/Deep-learning/GFlowNets rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/GFlowNets>AI/Deep learning/GFlowNets</a></p><h3 id=neural-cellular-automata>Neural Cellular Automata</h3><p>See <a href=/digitalgarden/AI/Deep-learning/Neural-Cellular-Automata rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Neural-Cellular-Automata>AI/Deep learning/Neural Cellular Automata</a></p><h3 id=neural-processes>Neural processes</h3><p>See <a href=/digitalgarden/AI/Deep-learning/Neural-processes rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Neural-processes>AI/Deep learning/Neural processes</a></p><h3 id=bayesianprobabilistic-dl>Bayesian/probabilistic DL</h3><p>See <a href=/digitalgarden/AI/Deep-learning/Probabilistic-deep-learning rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Probabilistic-deep-learning>AI/Deep learning/Probabilistic deep learning</a></p></article><hr><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/digitalgarden/AI/AI>Artificial Intelligence</a></li><li><a href=/digitalgarden/AI/Computer-Vision/Video-segmentation-and-prediction>Video segmentation and prediction</a></li><li><a href=/digitalgarden/AI/Deep-learning/Encoder-decoder-networks>Encoder-decoder networks</a></li><li><a href=/digitalgarden/AI/Machine-Learning>Machine Learning (ML)</a></li><li><a href=/digitalgarden/AI/Time-Series-analysis>Time series analysis</a></li><li><a href=/digitalgarden/AI4ES/Bias-correction-adjustment>Bias correction, adjustment</a></li><li><a href=/digitalgarden/AI4ES/Extremes-events>Extremes events</a></li><li><a href=/digitalgarden/>CarlosGG's Knowledge Garden 🪴</a></li></ul></div></div><div id=contact_buttons><footer><p>Made by Carlos Alberto Gomez Gonzalez using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2022</p><ul><li><a href=https://carlos-gg.github.io/digitalgarden/>Home</a></li><a href=https://carlos-gg.github.io>Carlos'Homepage</a></ul></footer></div><script src=https://carlos-gg.github.io/digitalgarden/js/popover.e57188d2e4c06b0654e020b3a734bb62.min.js></script>
<script>initPopover("https://carlos-gg.github.io/digitalgarden")</script></div></body></html>