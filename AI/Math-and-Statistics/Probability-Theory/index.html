<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Resources   https://en.wikipedia.org/wiki/Probability_theory  https://stanford.edu/~shervine/teaching/cme-106/key-concepts  A visual introduction to probability and statistics  https://leanpub.com/LittleInferenceBook/read#leanpub-auto-probability Probability forms the foundation for almost all treatments of statistical inference."><title>Probability Theory</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://carlos-gg.github.io/digitalgarden//icon.png><link href=https://carlos-gg.github.io/digitalgarden/styles.b3e1e36b0403ac565c9392b3e23ef3b6.min.css rel=stylesheet><link href=https://carlos-gg.github.io/digitalgarden/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://carlos-gg.github.io/digitalgarden/js/darkmode.18b7c6dfe67ae3bf2317338cf4189144.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/util.9825137f5e7825e8553c68ce39ac9e44.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/popover.abe6a51cc7138c5dff00f151dd627ad1.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/code-title.b35124ad8db0ba37162b886afb711cbc.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/clipboard.c20857734e53a3fb733b7443879efa61.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const BASE_URL="https://carlos-gg.github.io/digitalgarden/",fetchData=Promise.all([fetch("https://carlos-gg.github.io/digitalgarden/indices/linkIndex.36332fb20fc41f515c0a269de715e752.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://carlos-gg.github.io/digitalgarden/indices/contentIndex.30961cc1ced34e6c9bc3d701b0e16891.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://carlos-gg.github.io/digitalgarden",!0,!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!0;drawGraph("https://carlos-gg.github.io/digitalgarden",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/carlos-gg.github.io\/digitalgarden\/js\/router.9d4974281069e9ebb189f642ae1e3ca2.min.js"
    attachSPARouting(init, render)
  </script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-S103D50NQ0"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S103D50NQ0",{anonymize_ip:!1})}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://carlos-gg.github.io/digitalgarden/js/full-text-search.24827f874defbbc6d529926cbfcfb493.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://carlos-gg.github.io/digitalgarden/>CarlosGG's Knowledge Garden ðŸª´</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Probability Theory</h1><p class=meta>Last updated
Apr 10, 2022
<a href=https://github.com/jackyzha0/quartz/tree/hugo/content/AI/Math%20and%20Statistics/Probability%20Theory.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details open><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#resources>Resources</a><ol><li><ol><li><a href=#random-variables>Random variables</a></li><li><a href=#conditional-probability>Conditional probability</a></li><li><a href=#independance>Independance</a></li><li><a href=#common-distributions>Common distributions</a></li><li><a href=#expected-value>Expected value</a></li><li><a href=#central-limit-theorem>Central limit theorem</a></li><li><a href=#kullback-leibler-divergence>Kullback-Leibler Divergence</a></li></ol></li></ol></li><li><a href=#books>Books</a></li></ol></nav></details></aside><a href=#resources><h2 id=resources><span class=hanchor arialabel=Anchor># </span>Resources</h2></a><ul><li><a href=https://en.wikipedia.org/wiki/Probability_theory rel=noopener>https://en.wikipedia.org/wiki/Probability_theory</a></li><li><a href=https://stanford.edu/~shervine/teaching/cme-106/key-concepts rel=noopener>https://stanford.edu/~shervine/teaching/cme-106/key-concepts</a></li><li><a href=http://students.brown.edu/seeing-theory/ rel=noopener>A visual introduction to probability and statistics</a></li><li><a href=https://leanpub.com/LittleInferenceBook/read#leanpub-auto-probability rel=noopener>https://leanpub.com/LittleInferenceBook/read#leanpub-auto-probability</a></li><li>Probability forms the foundation for almost all treatments of statistical inference. Probability is a law that assigns numbers to the long run occurrence of random phenomena after repeated unrelated realizations</li><li><a href=http://nbviewer.jupyter.org/url/norvig.com/ipython/Probability.ipynb rel=noopener>http://nbviewer.jupyter.org/url/norvig.com/ipython/Probability.ipynb</a></li><li><a href=https://en.wikipedia.org/wiki/Probability_interpretations rel=noopener>https://en.wikipedia.org/wiki/Probability_interpretations</a></li><li><a href=https://en.wikipedia.org/wiki/Bayesian_probability rel=noopener>https://en.wikipedia.org/wiki/Bayesian_probability</a></li><li><a href="http://youtu.be/oTERv_vrmJM?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ" rel=noopener>The Coursera Statistical Inference class</a></li><li><a href=https://en.wikipedia.org/wiki/Monty_Hall_problem rel=noopener>Monty Hall problem</a></li><li><a href=https://github.com/cs109/2015lab1/blob/master/hw0.ipynb rel=noopener>Coded in Python</a></li><li>Cheatsheets:<ul><li><a href=https://stanford.edu/~shervine/teaching/cme-106/cheatsheet-probability rel=noopener>https://stanford.edu/~shervine/teaching/cme-106/cheatsheet-probability</a></li><li>Probability Cheatsheet (Chen):<ul><li><a href=http://www.wzchen.com/probability-cheatsheet/ rel=noopener>http://www.wzchen.com/probability-cheatsheet/</a></li><li>This is an 10-page probability cheatsheet compiled from Harvard&rsquo;s Introduction to Probability course, taught by Joe Blitzstein (@stat110). The probability formula sheet summarizes important probability probability concepts, formulas, and distributions, with figures, examples, and stories.</li></ul></li><li>Review of Probability Theory (CS229 Stanford)<ul><li><a href=http://cs229.stanford.edu/section/cs229-prob.pdf rel=noopener>http://cs229.stanford.edu/section/cs229-prob.pdf</a></li><li><a href=http://cs229.stanford.edu/section/cs229-prob-slide.pdf rel=noopener>http://cs229.stanford.edu/section/cs229-prob-slide.pdf</a></li></ul></li></ul></li></ul><a href=#random-variables><h4 id=random-variables><span class=hanchor arialabel=Anchor># </span>Random variables</h4></a><ul><li><a href="http://youtu.be/Shzt9uZ8BII?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ" rel=noopener>http://youtu.be/Shzt9uZ8BII?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ</a></li><li><a href=https://leanpub.com/LittleInferenceBook/read#leanpub-auto-random-variables rel=noopener>https://leanpub.com/LittleInferenceBook/read#leanpub-auto-random-variables</a></li><li>Random variable is a numerical outcome of an experiment. The random variables that we study will come in two varieties, discrete or continuous. Discrete random variables are random variables that take on only a countable number of possibilities. Mass functions will assign probabilities that they take specific values. Continuous random variable can conceptually take any value on the real line or some subset of the real line and we talk about the probability that they lie within some range. Densities will characterize these probabilities.</li><li>For all of these kinds of random variables, we need convenient mathematical functions to model the probabilities of collections of realizations. These functions, called mass functions and densities, take possible values of the random variables, and assign the associated probabilities. These entities describe the population of interest.</li><li>PDF - probability density function<ul><li><a href=https://en.wikipedia.org/wiki/Probability_density_function rel=noopener>https://en.wikipedia.org/wiki/Probability_density_function</a></li><li><a href="http://youtu.be/mPe0Us4VYDM?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ" rel=noopener>http://youtu.be/mPe0Us4VYDM?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ</a></li><li><a href=https://leanpub.com/LittleInferenceBook/read#leanpub-auto-probability-density-functions rel=noopener>https://leanpub.com/LittleInferenceBook/read#leanpub-auto-probability-density-functions</a></li><li>A probability density function (pdf), is a function associated with a continuous random variable. Because of the peculiarities of treating measurements as having been recorded to infinite decimal expansions, we need a different set of rules. This leads us to the central dogma of probability density functions: Areas under PDFs correspond to probabilities for that random variable.</li><li>A PDF, or density of a continuous random variable, is a function, whose value at any given sample (or point) in the sample space(the set of possible values taken by the random variable) can be interpreted as providing a relative likelihood that the value of the random variable would equal that sample.</li><li>The PDF is used to specify the probability of the random variable falling within a particular range of values, as opposed to taking on any one value.</li></ul></li></ul><a href=#conditional-probability><h4 id=conditional-probability><span class=hanchor arialabel=Anchor># </span>Conditional probability</h4></a><ul><li><a href="http://youtu.be/u6AH6qsSVA4?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ" rel=noopener>http://youtu.be/u6AH6qsSVA4?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ</a></li><li><a href=https://leanpub.com/LittleInferenceBook/read#leanpub-auto-conditional-probability rel=noopener>https://leanpub.com/LittleInferenceBook/read#leanpub-auto-conditional-probability</a></li><li>Conditioning is a central subject in statistics. If we are given information about a random variable, it changes the probabilities associated with it. For example, the probability of getting a one when rolling a (standard) die is usually assumed to be one sixth. If you were given the extra information that the die roll was an odd number (hence 1, 3 or 5) then conditional on this new information, the probability of a one is now one third.</li><li><a href=http://setosa.io/ev/conditional-probability/ rel=noopener>http://setosa.io/ev/conditional-probability/</a></li></ul><a href=#independance><h4 id=independance><span class=hanchor arialabel=Anchor># </span>Independance</h4></a><ul><li><a href=https://en.wikipedia.org/wiki/Independence_%28probability_theory%29 rel=noopener>https://en.wikipedia.org/wiki/Independence_(probability_theory)</a></li><li><a href="http://youtu.be/MY1EfrR1ZUs?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ" rel=noopener>http://youtu.be/MY1EfrR1ZUs?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ</a></li><li><a href=https://leanpub.com/LittleInferenceBook/read#leanpub-auto-independence rel=noopener>https://leanpub.com/LittleInferenceBook/read#leanpub-auto-independence</a></li><li>Statistical independence of events is the idea that the events are unrelated. Consider successive coin flips. Knowledge of the result of the first coin flip tells us nothing about the second.</li><li>The important principle is that probabilities of independent things multiply! This has numerous consequences, including the idea that we shouldnâ€™t multiply non-independent probabilities.</li><li><a href=https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables rel=noopener>IID samples</a><ul><li>IID - Independent and identically distributed</li><li>In probability theory and statistics, a sequence or other collection of random variables is independent and identically distributed(i.i.d.) if each random variable has the same probability distribution as the others and all are mutually independent.</li><li><a href=https://leanpub.com/LittleInferenceBook/read#leanpub-auto-iid-random-variables rel=noopener>https://leanpub.com/LittleInferenceBook/read#leanpub-auto-iid-random-variables</a></li><li>Random variables are said to be independent and identically distributed (iid) if they are independent and all are drawn from the same population. The reason iid samples are so important is that they are a model for random samples. This is a default starting point for most statistical inferences.</li></ul></li></ul><a href=#common-distributions><h4 id=common-distributions><span class=hanchor arialabel=Anchor># </span>Common distributions</h4></a><ul><li><a href=https://stanford.edu/~shervine/teaching/cme-106/distribution-tables rel=noopener>https://stanford.edu/~shervine/teaching/cme-106/distribution-tables</a></li><li><a href=https://blog.cloudera.com/blog/2015/12/common-probability-distributions-the-data-scientists-crib-sheet/ rel=noopener>https://blog.cloudera.com/blog/2015/12/common-probability-distributions-the-data-scientists-crib-sheet/</a></li><li><a href=https://leanpub.com/LittleInferenceBook/read#leanpub-auto-some-common-distributions rel=noopener>https://leanpub.com/LittleInferenceBook/read#leanpub-auto-some-common-distributions</a></li><li><a href=http://efavdb.com/normal-distributions/ rel=noopener>Normal distribution</a></li><li>Bernoulli<ul><li><a href=https://leanpub.com/LittleInferenceBook/read#leanpub-auto-the-bernoulli-distribution rel=noopener>https://leanpub.com/LittleInferenceBook/read#leanpub-auto-the-bernoulli-distribution</a></li><li>TheBernoulli distribution arises as the result of a binary outcome, such as a coin flip.</li></ul></li><li>Normal<ul><li><a href="http://youtu.be/dUTWvKa0Leo?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ" rel=noopener>http://youtu.be/dUTWvKa0Leo?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ</a></li><li><a href=https://leanpub.com/LittleInferenceBook/read#leanpub-auto-the-normal-distribution rel=noopener>https://leanpub.com/LittleInferenceBook/read#leanpub-auto-the-normal-distribution</a></li><li>The normal distribution is easily the handiest distribution in all of statistics. It can be used in an endless variety of settings. Moreover, as weâ€™ll see later on in the course, sample means follow normal distributions for large sample sizes.</li><li>The normal distribution only requires two numbers to characterize it, mean and variance.</li></ul></li><li>Poisson<ul><li><a href="http://youtu.be/ZPLZg7qz4xE?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ" rel=noopener>http://youtu.be/ZPLZg7qz4xE?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ</a></li><li><a href=https://leanpub.com/LittleInferenceBook/read#leanpub-auto-the-poisson-distribution rel=noopener>https://leanpub.com/LittleInferenceBook/read#leanpub-auto-the-poisson-distribution</a></li><li>The Poisson distribution is used to model counts. It is perhaps only second to the normal distribution usefulness. In fact, the Bernoulli, binomial and multinomial distributions can all be modeled by clever uses of the Poisson.</li><li>The Poisson distribution is especially useful for modeling unbounded counts or counts per unit of time (rates). Like the number of clicks on advertisements, or the number of people who show up at a bus stop. There is also a deep connection between the Poisson distribution and popular models for so-called event-time data.</li></ul></li></ul><a href=#expected-value><h4 id=expected-value><span class=hanchor arialabel=Anchor># </span>Expected value</h4></a><ul><li><a href=https://en.wikipedia.org/wiki/Expected_value rel=noopener>https://en.wikipedia.org/wiki/Expected_value</a></li><li>In probability theory, the expected value of a random variable, intuitively, is the long-run average value of repetitions of the experiment it represents.</li><li>Less roughly, the law of large numbers states that the arithmetic mean of the values almost surely converges to the expected value as the number of repetitions approaches infinity. The expected value is also known as the expectation, mathematical expectation, EV, average, mean value, mean, or first moment.</li><li><a href="http://youtu.be/zljxRbu6jyc?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ" rel=noopener>Expected values</a></li><li><a href=https://leanpub.com/LittleInferenceBook/read#leanpub-auto-expected-values rel=noopener>https://leanpub.com/LittleInferenceBook/read#leanpub-auto-expected-values</a></li><li>Expected values characterize a distribution. The most useful expected value, the mean, characterizes the center of a density or mass function. Another expected value summary, the variance, characterizes how spread out a density is. Yet another expected value calculation is the skewness, which considers how much a density is pulled toward high or low values.</li></ul><a href=#central-limit-theorem><h4 id=central-limit-theorem><span class=hanchor arialabel=Anchor># </span>Central limit theorem</h4></a><ul><li><a href=https://en.wikipedia.org/wiki/Central_limit_theorem rel=noopener>https://en.wikipedia.org/wiki/Central_limit_theorem</a></li><li><a href="http://youtu.be/FAIyVHmniK0?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ" rel=noopener>http://youtu.be/FAIyVHmniK0?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ</a></li><li><a href=https://leanpub.com/LittleInferenceBook/read#leanpub-auto-the-central-limit-theorem rel=noopener>https://leanpub.com/LittleInferenceBook/read#leanpub-auto-the-central-limit-theorem</a></li><li>CLT is one of the most important theorems in statistics. For our purposes, the CLT states that the distribution of averages of iid variables becomes that of a standard normal as the sample size increases.</li></ul><a href=#kullback-leibler-divergence><h4 id=kullback-leibler-divergence><span class=hanchor arialabel=Anchor># </span>Kullback-Leibler Divergence</h4></a><ul><li><a href=https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence rel=noopener>https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence</a></li><li><a href=https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained rel=noopener>https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained</a></li></ul><a href=#books><h2 id=books><span class=hanchor arialabel=Anchor># </span>Books</h2></a><ul><li>#COURSE
<a href=https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/ rel=noopener>Introduction to Probability and Statistics (MIT)</a></li></ul></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/digitalgarden/AI/Math-and-Statistics/Math-and-Statistics/ data-ctx="AI/Math and Statistics/Probability Theory" data-src=/AI/Math-and-Statistics/Math-and-Statistics class=internal-link>Math and Statistics</a></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://carlos-gg.github.io/digitalgarden/js/graph.abd4bc2af3869a96524d7d23b76152c7.js></script></div></div><div id=contact_buttons><footer><p>Made by Carlos Alberto Gomez Gonzalez using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2022</p><ul><li><a href=https://carlos-gg.github.io/digitalgarden/>Home</a></li><li><a href=https://carlos-gg.github.io>Carlos'Homepage</a></li></ul></footer></div></div></body></html>