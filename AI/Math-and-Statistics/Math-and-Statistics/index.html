<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Resources   https://en.wikipedia.org/wiki/Portal:Mathematics  https://en.wikipedia.org/wiki/Mathematics  Statistics cheatsheet  Mathematics for Machine Learning  https://github.com/rouseguy/intro2stats  Stanford-cs-229 ML, probability and stats refresher  https://www."><title>Math and Statistics</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://carlos-gg.github.io/digitalgarden//icon.png><link href=https://carlos-gg.github.io/digitalgarden/styles.b3e1e36b0403ac565c9392b3e23ef3b6.min.css rel=stylesheet><link href=https://carlos-gg.github.io/digitalgarden/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://carlos-gg.github.io/digitalgarden/js/darkmode.18b7c6dfe67ae3bf2317338cf4189144.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/util.9825137f5e7825e8553c68ce39ac9e44.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/popover.abe6a51cc7138c5dff00f151dd627ad1.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/code-title.b35124ad8db0ba37162b886afb711cbc.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/clipboard.c20857734e53a3fb733b7443879efa61.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const BASE_URL="https://carlos-gg.github.io/digitalgarden/",fetchData=Promise.all([fetch("https://carlos-gg.github.io/digitalgarden/indices/linkIndex.b55b28901ea8fe73a122bb94b07c303a.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://carlos-gg.github.io/digitalgarden/indices/contentIndex.5a84c5b53f0723785a0ec39a0c059f82.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://carlos-gg.github.io/digitalgarden",!0,!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!0;drawGraph("https://carlos-gg.github.io/digitalgarden",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/carlos-gg.github.io\/digitalgarden\/js\/router.9d4974281069e9ebb189f642ae1e3ca2.min.js"
    attachSPARouting(init, render)
  </script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-S103D50NQ0"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S103D50NQ0",{anonymize_ip:!1})}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://carlos-gg.github.io/digitalgarden/js/full-text-search.24827f874defbbc6d529926cbfcfb493.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://carlos-gg.github.io/digitalgarden/>CarlosGG's Knowledge Garden 🪴</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Math and Statistics</h1><p class=meta>Last updated
Sep 9, 2022
<a href=https://github.com/jackyzha0/quartz/tree/hugo/content/AI/Math%20and%20Statistics/Math%20and%20Statistics.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details open><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#resources>Resources</a></li><li><a href=#books>Books</a></li><li><a href=#courses>Courses</a></li><li><a href=#code>Code</a><ol><li><a href=#ab-testing>A/B testing</a></li></ol></li><li><a href=#subtopics>Subtopics</a><ol><li><a href=#calculus>Calculus</a></li><li><a href=#linear-algebra>Linear Algebra</a></li><li><a href=#distances>Distances</a></li><li><a href=#descriptive-stats>Descriptive stats</a><ol><li><a href=#correlation-and-dependance>Correlation and dependance</a></li></ol></li><li><a href=#experimental-design>Experimental design</a><ol><li><a href=#ab-testing-1>A/B testing</a></li><li><a href=#multi-armed-bandit>Multi-armed bandit</a></li></ol></li><li><a href=#statistical-inference>Statistical Inference</a><ol><li><a href=#frequentist-inference>Frequentist inference</a></li></ol></li><li><a href=#bootstrap-and-permutation-tests>Bootstrap and permutation tests</a><ol><li><a href=#bayesian-bootstrap>Bayesian bootstrap</a></li></ol></li><li><a href=#probability-theory>Probability theory</a></li><li><a href=#bayesian-modelling>Bayesian modelling</a></li><li><a href=#monte-carlo-methods>Monte Carlo methods</a></li><li><a href=#mathematical-optimization>Mathematical Optimization</a></li><li><a href=#time-series-analysis>Time series analysis</a></li><li><a href=#regression-analysis>Regression analysis</a></li><li><a href=#compressed-sensing>Compressed sensing</a><ol><li><a href=#nyquist-theorem>Nyquist theorem</a></li><li><a href=#matching-pursuit>Matching pursuit</a></li></ol></li></ol></li></ol></nav></details></aside><a href=#resources><h2 id=resources><span class=hanchor arialabel=Anchor># </span>Resources</h2></a><ul><li><a href=https://en.wikipedia.org/wiki/Portal:Mathematics rel=noopener>https://en.wikipedia.org/wiki/Portal:Mathematics</a></li><li><a href=https://en.wikipedia.org/wiki/Mathematics rel=noopener>https://en.wikipedia.org/wiki/Mathematics</a></li><li><a href=https://stanford.edu/~shervine/teaching/cme-106/cheatsheet-statistics rel=noopener>Statistics cheatsheet</a></li><li><a href=https://github.com/dair-ai/Mathematics-for-ML rel=noopener>Mathematics for Machine Learning</a></li><li><a href=https://github.com/rouseguy/intro2stats rel=noopener>https://github.com/rouseguy/intro2stats</a></li><li><a href=https://github.com/afshinea/stanford-cs-229-machine-learning/blob/master/en/refresher-probabilities-statistics.pdf rel=noopener>Stanford-cs-229 ML, probability and stats refresher</a></li><li><a href=https://www.khanacademy.org/math/statistics-probability rel=noopener>https://www.khanacademy.org/math/statistics-probability</a></li><li><a href=http://christopherroach.com/articles/statistics-for-hackers/ rel=noopener>http://christopherroach.com/articles/statistics-for-hackers/</a></li><li><a href=https://stanford.edu/~shervine/teaching/cme-102/trigonometry rel=noopener>Trigonometry refresher</a></li></ul><a href=#books><h2 id=books><span class=hanchor arialabel=Anchor># </span>Books</h2></a><ul><li>#BOOK
<a href=https://www.wiley.com/en-us/Essential+Mathematics+and+Statistics+for+Science%2C+2nd+Edition-p-9780470694480 rel=noopener>Essential Mathematics and Statistics for Science (Currell 2009, WILEY)</a><ul><li><a href=http://www.stewartschultz.com/statistics/books/Essential-Mathematics.pdf rel=noopener>http://www.stewartschultz.com/statistics/books/Essential%20Mathematics.pdf</a></li></ul></li><li>#BOOK
<a href=https://greenteapress.com/wp/think-stats-2e/ rel=noopener>Think Stats - Exploratory Data Analysis in Python (Downey 2014)</a><ul><li>Think Stats is an introduction to Probability and Statistics for Python programmers</li></ul></li><li>#BOOK
<a href=https://www.springer.com/fr/book/9783319283159 rel=noopener>An Introduction to Statistics with Python (Haslwanter, 2015 6 SPRINGER)</a><ul><li>Applications in the life sciences</li><li><a href=https://es.scribd.com/document/338198132/An-Introduction-to-Statistics-With-Python-With-Applications-in-the-Life-Sciences rel=noopener>https://es.scribd.com/document/338198132/An-Introduction-to-Statistics-With-Python-With-Applications-in-the-Life-Sciences</a></li></ul></li><li>#BOOK
<a href=http://statsthinking21.org/index.html rel=noopener>Statistical Thinking for the 21st Century (Poldrack 2018)</a><ul><li>R language</li></ul></li><li>#BOOK
<a href=https://bvanderlei.github.io/jupyter-guide-to-linear-algebra/intro.html rel=noopener>Jupyter Guide to Linear Algebra</a></li></ul><a href=#courses><h2 id=courses><span class=hanchor arialabel=Anchor># </span>Courses</h2></a><ul><li>#COURSE
<a href=https://www.coursera.org/learn/statistical-inference rel=noopener>Statistical inference for data science</a><ul><li><a href=https://leanpub.com/LittleInferenceBook rel=noopener>https://leanpub.com/LittleInferenceBook</a></li><li><a href="https://www.youtube.com/playlist?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ" rel=noopener>Coursera Inference Version 3</a></li></ul></li><li>#COURSE
<a href=https://lagunita.stanford.edu/courses/course-v1:OLI+ProbStat+Open_Jan2017/about rel=noopener>Probability and Statistics (Stanford online)</a></li><li>#COURSE
<a href=http://statweb.stanford.edu/~tibs/stat315a/ rel=noopener>Modern Applied Statistics: Elements of Statistical Learning (Statistics 315a, Stanford)</a></li><li>#COURSE
<a href=https://ocw.mit.edu/high-school/mathematics/ rel=noopener>Calculus introductory courses (MIT)</a><ul><li><a href=https://ocw.mit.edu/courses/mathematics/18-01sc-single-variable-calculus-fall-2010 rel=noopener>Single Variable Calculus (18.01SC)</a></li><li><a href=https://ocw.mit.edu/courses/mathematics/18-02sc-multivariable-calculus-fall-2010 rel=noopener>Multivariable Calculus (18.02SC)</a></li><li><a href=https://ocw.mit.edu/resources/res-18-005-highlights-of-calculus-spring-2010 rel=noopener>Highlights of calculus (Strang)</a></li></ul></li><li>#TALK
<a href="https://www.youtube.com/watch?v=yaSgoGLXKOg" rel=noopener>Statistics in Python (Varoquaux 2015 Euroscipy)</a></li></ul><a href=#code><h2 id=code><span class=hanchor arialabel=Anchor># </span>Code</h2></a><ul><li>#CODE
<a href=https://numpy.org/ rel=noopener>Numpy</a><ul><li>#PAPER
<a href=https://www.nature.com/articles/s41586-020-2649-2 rel=noopener>Array programming with NumPy (Harris 2020)</a></li></ul></li><li>#CODE
<a href=https://www.scipy.org/ rel=noopener>Scipy</a></li><li>#CODE
<a href=https://github.com/nums-project/nums rel=noopener>Nums</a><ul><li>A library that translates Python and NumPy to optimized distributed systems code</li></ul></li><li>#CODE
<a href=http://www.statsmodels.org/ rel=noopener>Statsmodels</a></li><li>#CODE
<a href=https://github.com/PyLops/pylops rel=noopener>PyLops</a><ul><li><a href=https://pylops.readthedocs.io/en/latest/index.html rel=noopener>https://pylops.readthedocs.io/en/latest/index.html</a></li><li>Python library is inspired by the MATLAB Spot – A Linear-Operator Toolbox project</li></ul></li><li>#CODE
<a href=https://github.com/lmc2179/bayesian_bootstrap rel=noopener>Bayesian bootstrap</a></li></ul><a href=#ab-testing><h3 id=ab-testing><span class=hanchor arialabel=Anchor># </span>A/B testing</h3></a><ul><li>#CODE
<a href=https://github.com/sixpack/sixpack rel=noopener>Sixpack</a></li><li>#CODE
<a href=https://github.com/zalando/expan rel=noopener>Expan (Zalando)</a><ul><li>A Python library for statistical analysis of randomised control trials (A/B tests)</li><li>#TALK
<a href="https://www.youtube.com/watch?v=furJxiZlo6w" rel=noopener>https://www.youtube.com/watch?v=furJxiZlo6w</a></li></ul></li><li>#CODE Proctor (Indeed):<ul><li><a href=https://github.com/indeedeng/proctor rel=noopener>https://github.com/indeedeng/proctor</a></li><li><a href=http://opensource.indeedeng.io/proctor/ rel=noopener>http://opensource.indeedeng.io/proctor/</a></li></ul></li></ul><a href=#subtopics><h2 id=subtopics><span class=hanchor arialabel=Anchor># </span>Subtopics</h2></a><a href=#calculus><h3 id=calculus><span class=hanchor arialabel=Anchor># </span>Calculus</h3></a><ul><li><a href=https://stanford.edu/~shervine/teaching/cme-102/calculus rel=noopener>Calculus refresher</a></li><li>Ordinary Differential Equations<ul><li><a href=https://stanford.edu/~shervine/teaching/cme-102/cheatsheet-first-ode rel=noopener>https://stanford.edu/~shervine/teaching/cme-102/cheatsheet-first-ode</a></li><li><a href=https://stanford.edu/~shervine/teaching/cme-102/cheatsheet-second-ode rel=noopener>https://stanford.edu/~shervine/teaching/cme-102/cheatsheet-second-ode</a></li><li><a href=https://stanford.edu/~shervine/teaching/cme-102/cheatsheet-applications rel=noopener>https://stanford.edu/~shervine/teaching/cme-102/cheatsheet-applications</a></li></ul></li><li><a href=https://github.com/afshinea/stanford-cs-229-machine-learning/blob/master/en/refresher-algebra-calculus.pdf rel=noopener>Stanford-cs-229 ML, algebra and calculus refresher</a></li><li><a href=https://scipy-latinamerica.github.io/revista.io/blog/2018/10/20/introduccion-al-calculo-con-python/ rel=noopener>https://scipy-latinamerica.github.io/revista.io/blog/2018/10/20/introduccion-al-calculo-con-python/</a></li><li><a href=https://www.khanacademy.org/math/multivariable-calculus rel=noopener>https://www.khanacademy.org/math/multivariable-calculus</a></li></ul><a href=#linear-algebra><h3 id=linear-algebra><span class=hanchor arialabel=Anchor># </span>Linear Algebra</h3></a><p>See <a href=/digitalgarden/AI/Math-and-Statistics/Linear-Algebra rel=noopener class=internal-link data-src=/digitalgarden/AI/Math-and-Statistics/Linear-Algebra>AI/Math and Statistics/Linear Algebra</a></p><a href=#distances><h3 id=distances><span class=hanchor arialabel=Anchor># </span>Distances</h3></a><p>See <a href=/digitalgarden/AI/Math-and-Statistics/Distances rel=noopener class=internal-link data-src=/digitalgarden/AI/Math-and-Statistics/Distances>AI/Math and Statistics/Distances</a></p><a href=#descriptive-stats><h3 id=descriptive-stats><span class=hanchor arialabel=Anchor># </span>Descriptive stats</h3></a><ul><li><a href=https://en.wikipedia.org/wiki/Descriptive_statistics rel=noopener>https://en.wikipedia.org/wiki/Descriptive_statistics</a></li><li><a href=http://debrouwere.org/2017/02/01/unlearning-descriptive-statistics/ rel=noopener>http://debrouwere.org/2017/02/01/unlearning-descriptive-statistics/</a></li></ul><a href=#correlation-and-dependance><h4 id=correlation-and-dependance><span class=hanchor arialabel=Anchor># </span>Correlation and dependance</h4></a><ul><li><a href=https://www.datascience.com/blog/introduction-to-correlation-learn-data-science-tutorials rel=noopener>https://www.datascience.com/blog/introduction-to-correlation-learn-data-science-tutorials</a></li><li><a href=https://en.wikipedia.org/wiki/Correlation_and_dependence rel=noopener>https://en.wikipedia.org/wiki/Correlation_and_dependence</a></li><li>Correlation is a statistical measure that describes the association between random variables. Why is correlation a useful metric?<ul><li>Correlation can help in predicting one quantity from another</li><li>Correlation can (but often does not, as we will see in some examples below) indicate the presence of a causal relationship</li><li>Correlation is used as a basic quantity and foundation for many other modeling techniques</li></ul></li><li>Types:<ul><li>Pearson’s Correlation:<ul><li>Pearson is the most widely used correlation coefficient. Pearson correlation measures the linear association between continuous variables. In other words, this coefficient quantifies the degree to which a relationship between two variables can be described by a line. Raw observations are centered by subtracting their means and re-scaled by a measure of standard deviations.</li><li><code>Ro_X,Y = E[(X - mu_X)(Y - mu_Y)] / simga_X sigma_Y</code></li><li>numerator -> covariance</li><li>Dividing the covariance between two variables by the product of standard deviations ensures that correlation will always fall between -1 and 1 (much easier to interpret)</li></ul></li><li>Spearman&rsquo;s Correlation:<ul><li>Spearman&rsquo;s rank correlation coefficient can be defined as a special case of Pearson ρapplied to ranked (sorted) variables. Rather than comparing means and variances, Spearman&rsquo;s coefficient looks at the relative order of values for each variable. The formula for Spearman&rsquo;s coefficient looks very similar to that of Pearson, with the distinction of being computed on ranks instead of raw scores.</li></ul></li><li>Kendall&rsquo;s Tau:<ul><li>Also based on variable ranks, however, unlike Spearman&rsquo;s coefficient, Kendall’s tau does not take into account the difference between ranks— only directional agreement.</li></ul></li></ul></li><li>Covariance (matrix)<ul><li><a href=https://en.wikipedia.org/wiki/Covariance rel=noopener>https://en.wikipedia.org/wiki/Covariance</a></li><li>In probability theory and statistics, covariance is a measure of the joint variability of two random variables. The sign of the covariance therefore shows the tendency in the linear relationship between the variables. The magnitude of the covariance is not easy to interpret. The normalized version of the covariance, the correlation coefficient, however, shows by its magnitude the strength of the linear relation.</li><li><a href=https://en.wikipedia.org/wiki/Covariance_matrix rel=noopener>In multidimensional case: covariance matrix</a></li></ul></li><li>Correlation & Causation<ul><li><a href=https://en.wikipedia.org/wiki/Correlation_does_not_imply_causation rel=noopener>https://en.wikipedia.org/wiki/Correlation_does_not_imply_causation</a></li><li>&ldquo;Correlation does not imply causation&rdquo; is a phrase used in statistics to emphasize that a correlation between two variables does not imply that one causes the other. Spurious statistical associations can be found in a multitude of quantities, simply due to chance.</li><li>Often, a relationship may appear to be causal through high correlation due to some unobserved variables. For example, the number of grocery stores in a city can be strongly correlated with the number of ice cream creameries. However, there is an obvious hidden variable here— the population size of the city.</li><li><a href=https://medium.com/@akelleh/a-technical-primer-on-causality-181db2575e41#.4csn3na8j rel=noopener>https://medium.com/@akelleh/a-technical-primer-on-causality-181db2575e41#.4csn3na8j</a></li><li><a href=https://www.khanacademy.org/math/probability/scatterplots-a1/creating-interpreting-scatterplots/v/correlation-and-causality rel=noopener>https://www.khanacademy.org/math/probability/scatterplots-a1/creating-interpreting-scatterplots/v/correlation-and-causality</a></li><li>Also, weak or no correlation does not imply lack of association. Correlation is only one data summary statistic that by no means tells the complete story of relationships in the data.</li></ul></li></ul><a href=#experimental-design><h3 id=experimental-design><span class=hanchor arialabel=Anchor># </span>Experimental design</h3></a><p>See <a href=/digitalgarden/AI/Active-learning rel=noopener class=internal-link data-src=/digitalgarden/AI/Active-learning>AI/Active learning</a></p><ul><li><a href="https://www.youtube.com/watch?v=vSXOJnGNtM4" rel=noopener>Coursera - data scientist&rsquo;s toolbox</a></li><li>Good experiment: replication, measure variability, generalise to the problem, transparent</li><li>Confounding variable - strategies: randomization, stratifying</li><li>Prediction is not and inference. Both are important and depend on the problem. Prediction is more challenging that inference. For prediction there are key quantities (metrics): sensitivity, specificity, positive predictive value, negative predictive value, accuracy</li></ul><a href=#ab-testing-1><h4 id=ab-testing-1><span class=hanchor arialabel=Anchor># </span>A/B testing</h4></a><ul><li><a href=https://en.wikipedia.org/wiki/A/B_testing rel=noopener>https://en.wikipedia.org/wiki/A/B_testing</a></li><li>In marketing and business intelligence, A/B testing is a term for a randomized experiment with two variants, A and B, which are the control and variation in the controlled experiment. A/B testing is a form of statistical hypothesis testing with two variants leading to the technical term, two-sample hypothesis testing, used in the field of statistics. Other terms used for this method include bucket tests and split-run testing.</li><li><a href=https://www.optimizely.com/ab-testing/ rel=noopener>https://www.optimizely.com/ab-testing/</a></li><li><a href=https://www.wired.com/2012/04/ff_abtesting/ rel=noopener>https://www.wired.com/2012/04/ff_abtesting/</a></li><li><a href=http://data36.com/ab-testing-5-rules/ rel=noopener>http://data36.com/ab-testing-5-rules/</a></li><li><a href=https://www.udacity.com/course/ab-testing--ud257 rel=noopener>https://www.udacity.com/course/ab-testing--ud257</a></li><li><a href=https://tech.okcupid.com/the-pitfalls-of-a-b-testing-in-social-networks/ rel=noopener>https://tech.okcupid.com/the-pitfalls-of-a-b-testing-in-social-networks/</a></li><li><a href=https://www.quora.com/What-kind-of-A-B-testing-questions-should-I-expect-in-a-data-scientist-interview-and-how-should-I-prepare-for-such-questions rel=noopener>https://www.quora.com/What-kind-of-A-B-testing-questions-should-I-expect-in-a-data-scientist-interview-and-how-should-I-prepare-for-such-questions</a></li><li><a href=http://www.kdnuggets.com/2017/05/must-know-key-issues-problems-ab-testing.html rel=noopener>http://www.kdnuggets.com/2017/05/must-know-key-issues-problems-ab-testing.html</a></li></ul><a href=#multi-armed-bandit><h4 id=multi-armed-bandit><span class=hanchor arialabel=Anchor># </span>Multi-armed bandit</h4></a><ul><li><a href=http://blog.actblue.com/2015/04/29/the-multi-armed-bandit-new-and-much-improved-ab-testing-tools-2/ rel=noopener>http://blog.actblue.com/2015/04/29/the-multi-armed-bandit-new-and-much-improved-ab-testing-tools-2/</a></li><li><a href=https://conversionxl.com/bandit-tests/ rel=noopener>https://conversionxl.com/bandit-tests/</a></li><li><a href="https://support.google.com/analytics/answer/2844870?hl=en" rel=noopener>https://support.google.com/analytics/answer/2844870?hl=en</a></li><li><a href=https://vwo.com/blog/multi-armed-bandit-algorithm/ rel=noopener>https://vwo.com/blog/multi-armed-bandit-algorithm/</a></li></ul><a href=#statistical-inference><h3 id=statistical-inference><span class=hanchor arialabel=Anchor># </span>Statistical Inference</h3></a><ul><li><a href=https://en.wikipedia.org/wiki/Statistical_inference rel=noopener>https://en.wikipedia.org/wiki/Statistical_inference</a></li><li><a href="https://www.youtube.com/watch?v=WkOinijQmPU&feature=youtu.be&list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ" rel=noopener>https://www.youtube.com/watch?v=WkOinijQmPU&feature=youtu.be&list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ</a></li><li><a href=https://leanpub.com/LittleInferenceBook/ rel=noopener>https://leanpub.com/LittleInferenceBook/</a></li><li>Statistical inference : the process of generating conclusions about a population from a noisy sample. Without statistical inference we’re simply living within our data. With statistical inference, we’re trying to generate new knowledge. The use of probability models as the connection between our data and a populations represents the most effective way to obtain inference.</li><li>Question to answer: Are the statistics calculated on a small sample representative of the ones of the whole population?</li><li><a href=http://www.datasciencecentral.com/profiles/blogs/the-death-of-the-statistical-test-of-hypothesis rel=noopener>http://www.datasciencecentral.com/profiles/blogs/the-death-of-the-statistical-test-of-hypothesis</a></li></ul><a href=#frequentist-inference><h4 id=frequentist-inference><span class=hanchor arialabel=Anchor># </span>Frequentist inference</h4></a><ul><li><a href=https://en.wikipedia.org/wiki/Frequentist_inference rel=noopener>https://en.wikipedia.org/wiki/Frequentist_inference</a></li><li><a href=https://en.wikipedia.org/wiki/Nonparametric_statistics rel=noopener>https://en.wikipedia.org/wiki/Nonparametric_statistics</a></li><li>Statistical Hypothesis testing<ul><li><a href="http://youtu.be/Wqvx6_12ZMs?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ" rel=noopener>http://youtu.be/Wqvx6_12ZMs?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ</a></li><li><a href=https://leanpub.com/LittleInferenceBook/read#leanpub-auto-hypothesis-testing rel=noopener>https://leanpub.com/LittleInferenceBook/read#leanpub-auto-hypothesis-testing</a></li><li>Hypothesis testing is concerned with making decisions using data.</li><li>To make decisions using data, we need to characterize the kinds of conclusions we can make. Classical hypothesis testing is concerned with deciding between two decisions (things get much harder if there’s more than two). The first, a null hypothesis is specified that represents the status quo. This hypothesis is usually labeled, H_0. This is what we assume by default. The alternative or research hypothesis is what we require evidence to conclude. This hypothesis is usually labeled H_a, or sometimes H_1 (or some other number other than 0). So to reiterate, the null hypothesis is assumed true and statistical evidence is required to reject it in favor of a research or alternative hypothesis</li><li><a href=https://en.wikipedia.org/wiki/Student%27s_t-test rel=noopener>t-test</a><ul><li><a href=http://www.cs.cornell.edu/~asampson/blog/statsmistakes.html rel=noopener>http://www.cs.cornell.edu/~asampson/blog/statsmistakes.html</a></li><li><a href=https://www.quora.com/What-is-an-intuitive-explanation-of-the-t-test-in-hypothesis-testing rel=noopener>https://www.quora.com/What-is-an-intuitive-explanation-of-the-t-test-in-hypothesis-testing</a></li><li><a href=https://medium.freecodecamp.org/the-t-distribution-a-key-statistical-concept-discovered-by-a-beer-brewery-dbfdc693184 rel=noopener>https://medium.freecodecamp.org/the-t-distribution-a-key-statistical-concept-discovered-by-a-beer-brewery-dbfdc693184</a></li><li>Good for small samples?<ul><li>Historically, the very first demonstration of the t-test (in &ldquo;Student&rdquo;&rsquo;s 1908 paper) was in an application to sample sizes of size four. Indeed, obtaining improved results for smallsamples is the test&rsquo;s claim to fame: once the sample size reaches 40 or so, the t-test is not substantially different from the z-tests researchers had been applying throughout the 19th century. </li><li>There is no minimum sample size for the t test to be valid. Validity requires that the assumptions for the test statistic hold approximately. Those assumptions are in the one sample case that the data are iid normal (or approximately normal) with mean 0 under the null hypothesis and a variance that is unknown but estimated from the sample. In the two sample case it is that both samples are independent of each other and each sample consists of iid normal variables with the two samples having the same mean and a common unknown variance under the null hypothesis. A pooled estimate of variance is used for the statistic.</li><li>The problem with low sample size is with regard to the power of the test.</li><li>Using the Student’s t-test with extremely small sample sizes (Winter 2013). It is concluded that there are no principal objections to using a t-test with Ns as small as 2. This study showed that there are no objections to using a t-test with extremely small samples, as long as the effect size is large. </li></ul></li></ul></li><li><a href=https://en.wikipedia.org/wiki/Z-test rel=noopener>z-test</a></li><li><a href=https://en.wikipedia.org/wiki/F-test rel=noopener>f-test</a></li><li><a href=https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test rel=noopener>Mann–Whitney U test</a></li><li><a href=https://en.wikipedia.org/wiki/Welch%27s_t-test rel=noopener>Welch&rsquo;s t-test</a></li><li><a href=https://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test rel=noopener>Anderson–Darling test</a></li><li><a href=https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test rel=noopener>Kolmogorov–Smirnov test</a><ul><li>In essence, the test answers the question &ldquo;What is the probability that this collection of samples could have been drawn from that probability distribution?&rdquo; or, in the second case, &ldquo;What is the probability that these two sets of samples were drawn from the same (but unknown) probability distribution?&rdquo;</li><li><a href=https://asaip.psu.edu/Articles/beware-the-kolmogorov-smirnov-test/ rel=noopener>https://asaip.psu.edu/Articles/beware-the-kolmogorov-smirnov-test/</a></li></ul></li></ul></li><li>Confidence intervals<ul><li><a href="http://youtu.be/u85aQ0mtiZ8?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ" rel=noopener>http://youtu.be/u85aQ0mtiZ8?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ</a></li><li><a href=https://leanpub.com/LittleInferenceBook/read#leanpub-auto-confidence-intervals rel=noopener>https://leanpub.com/LittleInferenceBook/read#leanpub-auto-confidence-intervals</a></li><li>Confidence intervals are methods for quantifying uncertainty in our estimates. The fact that the interval has width characterizes that there is randomness that prevents us from getting a perfect estimate.</li><li>t-confidence intervals<ul><li><a href="http://youtu.be/pHXrDMjzyYg?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ" rel=noopener>http://youtu.be/pHXrDMjzyYg?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ</a></li><li><a href=https://leanpub.com/LittleInferenceBook/read#leanpub-auto-t-confidence-intervals rel=noopener>https://leanpub.com/LittleInferenceBook/read#leanpub-auto-t-confidence-intervals</a></li></ul></li></ul></li><li>Null hypothesis<ul><li><a href=https://en.wikipedia.org/wiki/Null_hypothesis rel=noopener>https://en.wikipedia.org/wiki/Null_hypothesis</a></li><li>The term &ldquo;null hypothesis&rdquo; is a general statement or default position that there is no relationship between two measured phenomena, or no association among groups.</li><li>H_0 is generally assumed to be true until evidence indicates otherwise.</li></ul></li><li>P-value<ul><li><a href=https://en.wikipedia.org/wiki/P-value rel=noopener>https://en.wikipedia.org/wiki/P-value</a></li><li>A p-value is the probability that, using a given statistical model, the statistical summary (such as the sample mean difference between two compared groups) would be the same as or more extreme than the actual observed results, when the null hypothesis is true.</li><li>After choosing the models H_0, H_1 and a threshold value alpha for p (the significance level of the test, traditionally 5% or 1%), if the p-value is less than or equal to alpha, the test suggests that the observed data is inconsistent with the null hypothesis, so the null hypothesis must be rejected. However, that does not prove that the tested hypothesis is true. When the p-value is calculated correctly, this test guarantees that the Type I error rate is at most alpha. For typical analysis, using the standard alpha= 0.05 cutoff, the null hypothesis is rejected when p&lt; .05 and not rejected when p> .05.</li><li>The p-value does not, in itself, support reasoning about the probabilities of hypotheses but is only a tool for deciding whether to reject the null hypothesis (it can only provide evidence against a hypothesis).</li><li><a href="http://youtu.be/Ky68x_7iK6c?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ" rel=noopener>http://youtu.be/Ky68x_7iK6c?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ</a></li><li><a href=https://leanpub.com/LittleInferenceBook/read#leanpub-auto-p-values rel=noopener>https://leanpub.com/LittleInferenceBook/read#leanpub-auto-p-values</a></li><li>P-values are the most common measure of statistical significance. Their ubiquity, along with concern over their interpretation and use makes them controversial among statisticians.</li><li><a href=http://blog.minitab.com/blog/adventures-in-statistics-2/how-to-correctly-interpret-p-values rel=noopener>http://blog.minitab.com/blog/adventures-in-statistics-2/how-to-correctly-interpret-p-values</a></li><li>A P value is the probability of obtaining an effect at least as extreme as the one in your sample data, assuming the truth of the null hypothesis.</li><li>For example, suppose that a vaccine study produced a P value of 0.04. This P value indicates that if the vaccine had no effect, you’d obtain the observed difference or more in 4% of studies due to random sampling error.</li><li>P values address only one question: how likely are your data, assuming a true null hypothesis? It does not measure support for the alternative hypothesis.</li><li><a href=http://machinelearningmastery.com/use-statistical-significance-tests-interpret-machine-learning-results/ rel=noopener>http://machinelearningmastery.com/use-statistical-significance-tests-interpret-machine-learning-results/</a></li><li><a href=https://www.nature.com/news/big-names-in-statistics-want-to-shake-up-much-maligned-p-value-1.22375 rel=noopener>https://www.nature.com/news/big-names-in-statistics-want-to-shake-up-much-maligned-p-value-1.22375</a></li></ul></li><li>Statistical Power<ul><li><a href=https://en.wikipedia.org/wiki/Statistical_power rel=noopener>https://en.wikipedia.org/wiki/Statistical_power</a></li><li>#TALK Statistical power:<ul><li><a href="http://youtu.be/-TsBOLiW4rQ?list=PLpl-gQkQivXiB1mGyzLrUjzsblmQsLtkzJ" rel=noopener>http://youtu.be/-TsBOLiW4rQ?list=PLpl-gQkQivXiB1mGyzLrUjzsblmQsLtkzJ</a></li><li><a href="http://youtu.be/GRS2b1aedmk?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ" rel=noopener>http://youtu.be/GRS2b1aedmk?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ</a></li></ul></li><li><a href=https://leanpub.com/LittleInferenceBook/read#leanpub-auto-power rel=noopener>https://leanpub.com/LittleInferenceBook/read#leanpub-auto-power</a></li><li>Power is the probability of rejecting the null hypothesis when it is false. Ergo, power (as its name would suggest) is a good thing; you want more power. A type II error (a bad thing, as its name would suggest) is failing to reject the null hypothesis when it’s false; the probability of a type II error is usually called Beta. Note Power = 1 - Beta.</li></ul></li><li><a href=https://en.wikipedia.org/wiki/Effect_size rel=noopener>Effect size</a></li><li><a href=https://en.wikipedia.org/wiki/Goodness_of_fit rel=noopener>Goodness of fit</a><ul><li><a href=https://en.wikipedia.org/wiki/Chi-squared_test rel=noopener>Chi squared</a></li></ul></li></ul><a href=#bootstrap-and-permutation-tests><h3 id=bootstrap-and-permutation-tests><span class=hanchor arialabel=Anchor># </span>Bootstrap and permutation tests</h3></a><ul><li><a href="http://youtu.be/0hNQx9nagq4?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ" rel=noopener>http://youtu.be/0hNQx9nagq4?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ</a></li><li><a href=https://leanpub.com/LittleInferenceBook/read#leanpub-auto-the-bootstrap-and-resampling rel=noopener>https://leanpub.com/LittleInferenceBook/read#leanpub-auto-the-bootstrap-and-resampling</a></li><li>The bootstrap is a tremendously useful tool for constructing confidence intervals and calculating standard errors for difficult statistics. That’s the bootstrap principle: investigate the sampling distribution of a statistic by simulating repeated realizations from the observed distribution.</li><li><a href=https://leanpub.com/LittleInferenceBook/read#leanpub-auto-permutation-tests rel=noopener>https://leanpub.com/LittleInferenceBook/read#leanpub-auto-permutation-tests</a></li><li>Consider comparing means between the group. However, let’s use the calculate the distribution of our statistic under a null hypothesis that the labels are irrelevant (exchangeable). This is a handy way to create a null distribution for our test statistic by simply permuting the labels over and over and seeing how extreme our data are with respect to this permuted distribution.</li><li>The procedure would be as follows:<ul><li>consider a data from with count and spray,</li><li>permute the spray (group) labels,</li><li>recalculate the statistic (such as the difference in means),</li><li>calculate the percentage of simulations where the simulated statistic was more extreme (toward the alternative) than the observed.</li></ul></li></ul><a href=#bayesian-bootstrap><h4 id=bayesian-bootstrap><span class=hanchor arialabel=Anchor># </span>Bayesian bootstrap</h4></a><ul><li><a href=http://www.sumsar.net/blog/2015/04/the-non-parametric-bootstrap-as-a-bayesian-model/ rel=noopener>http://www.sumsar.net/blog/2015/04/the-non-parametric-bootstrap-as-a-bayesian-model/</a></li></ul><a href=#probability-theory><h3 id=probability-theory><span class=hanchor arialabel=Anchor># </span>Probability theory</h3></a><p>See <a href=/digitalgarden/AI/Math-and-Statistics/Probability-Theory rel=noopener class=internal-link data-src=/digitalgarden/AI/Math-and-Statistics/Probability-Theory>AI/Math and Statistics/Probability Theory</a></p><a href=#bayesian-modelling><h3 id=bayesian-modelling><span class=hanchor arialabel=Anchor># </span>Bayesian modelling</h3></a><p>See <a href=/digitalgarden/AI/Bayesian-modelling rel=noopener class=internal-link data-src=/digitalgarden/AI/Bayesian-modelling>AI/Bayesian modelling</a></p><a href=#monte-carlo-methods><h3 id=monte-carlo-methods><span class=hanchor arialabel=Anchor># </span>Monte Carlo methods</h3></a><p>See <a href=/digitalgarden/AI/Math-and-Statistics/Monte-Carlo-methods rel=noopener class=internal-link data-src=/digitalgarden/AI/Math-and-Statistics/Monte-Carlo-methods>AI/Math and Statistics/Monte Carlo methods</a></p><a href=#mathematical-optimization><h3 id=mathematical-optimization><span class=hanchor arialabel=Anchor># </span>Mathematical Optimization</h3></a><p>See <a href=/digitalgarden/AI/Math-and-Statistics/Mathematical-Optimization rel=noopener class=internal-link data-src=/digitalgarden/AI/Math-and-Statistics/Mathematical-Optimization>AI/Math and Statistics/Mathematical Optimization</a></p><a href=#time-series-analysis><h3 id=time-series-analysis><span class=hanchor arialabel=Anchor># </span>Time series analysis</h3></a><p>See <a href=/digitalgarden/AI/Time-Series-analysis rel=noopener class=internal-link data-src=/digitalgarden/AI/Time-Series-analysis>AI/Time Series analysis</a></p><a href=#regression-analysis><h3 id=regression-analysis><span class=hanchor arialabel=Anchor># </span>Regression analysis</h3></a><p>See <a href=/digitalgarden/AI/Supervised-Learning/Regression rel=noopener class=internal-link data-src=/digitalgarden/AI/Supervised-Learning/Regression>AI/Supervised Learning/Regression</a></p><ul><li><a href=https://en.wikipedia.org/wiki/Regression_analysis rel=noopener>https://en.wikipedia.org/wiki/Regression_analysis</a></li><li>Regression analysis is widely used for prediction and forecasting, where its use has substantial overlap with the field of <a href=/digitalgarden/AI/Machine-Learning rel=noopener class=internal-link data-src=/digitalgarden/AI/Machine-Learning>AI/Machine Learning</a></li></ul><a href=#compressed-sensing><h3 id=compressed-sensing><span class=hanchor arialabel=Anchor># </span>Compressed sensing</h3></a><p>See <a href=/digitalgarden/AI/Unsupervised-learning/Sparse-dictionary-learning rel=noopener class=internal-link data-src=/digitalgarden/AI/Unsupervised-learning/Sparse-dictionary-learning>AI/Unsupervised learning/Sparse dictionary learning</a></p><ul><li><a href=https://en.wikipedia.org/wiki/Compressed_sensing rel=noopener>https://en.wikipedia.org/wiki/Compressed_sensing</a></li><li>Compressed sensing(also known as compressive sensing, compressive sampling, or sparse sampling) is a signal processing technique for efficiently acquiring and reconstructing a signal, by finding solutions to underdetermined linear systems. This is based on the principle that, through optimization, the sparsity of a signal can be exploited to recover it from far fewer samples than required by the Shannon-Nyquist sampling theorem.</li><li>There are two conditions under which recovery is possible:<ul><li>Sparsity, which requires the signal to be sparse in some domain.</li><li>Incoherence, which is applied through the isometric property which is sufficient for sparse signals</li></ul></li><li><a href=https://calculatedcontent.com/2012/12/28/foundations-theory-of-compressed-sensing/amp/ rel=noopener>https://calculatedcontent.com/2012/12/28/foundations-theory-of-compressed-sensing/amp/</a></li></ul><a href=#nyquist-theorem><h4 id=nyquist-theorem><span class=hanchor arialabel=Anchor># </span>Nyquist theorem</h4></a><ul><li><a href=https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem rel=noopener>https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem</a></li><li>In the field of digital signal processing, the sampling theorem is a fundamental bridge between continuous-time signals(often called &ldquo;analog signals&rdquo;) and discrete-time signals(often called &ldquo;digital signals&rdquo;). It establishes a sufficient condition for a sample rate that permits a discrete sequence of samples to capture all the information from a continuous-time signal of finite bandwidth.</li></ul><a href=#matching-pursuit><h4 id=matching-pursuit><span class=hanchor arialabel=Anchor># </span>Matching pursuit</h4></a><ul><li><a href=https://en.wikipedia.org/wiki/Matching_pursuit rel=noopener>https://en.wikipedia.org/wiki/Matching_pursuit</a></li><li>Matching pursuit (MP) is asparse approximation algorithm which involves finding the &ldquo;best matching&rdquo; projections of multidimensional data onto the span of an over-complete (i.e., redundant) dictionary D.</li><li>Orthogonal Matching Pursuit<ul><li>Extension of MP: after every step, all the coefficients extracted so far are updated, by computing the orthogonal projection of the signal onto the set of atoms selected so far. This can lead to better results than standard MP, but requires more computation.</li><li><a href=http://scikit-learn.org/stable/auto_examples/linear_model/plot_omp.html rel=noopener>http://scikit-learn.org/stable/auto_examples/linear_model/plot_omp.html</a></li></ul></li></ul></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/digitalgarden/AI/AI/ data-ctx="AI/Math and Statistics/Math and Statistics" data-src=/AI/AI class=internal-link>Artificial Intelligence</a></li><li><a href=/digitalgarden/AI/DS-and-DataEng/Data-Science/ data-ctx="AI/Math and Statistics/Math and Statistics" data-src=/AI/DS-and-DataEng/Data-Science class=internal-link>Data Science</a></li><li><a href=/digitalgarden/AI4ES/ESM-tuning/ data-ctx="AI/Math and Statistics/Math and Statistics#^1ef748" data-src=/AI4ES/ESM-tuning class=internal-link>ESM tuning</a></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://carlos-gg.github.io/digitalgarden/js/graph.abd4bc2af3869a96524d7d23b76152c7.js></script></div></div><div id=contact_buttons><footer><p>Made by Carlos Alberto Gomez Gonzalez using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2022</p><ul><li><a href=https://carlos-gg.github.io/digitalgarden/>Home</a></li><li><a href=https://carlos-gg.github.io>Carlos'Homepage</a></li></ul></footer></div></div></body></html>