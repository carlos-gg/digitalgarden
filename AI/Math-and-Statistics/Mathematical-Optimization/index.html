<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Resources   https://en.wikipedia.org/wiki/Mathematical_optimization  A birds-eye view of optimization algorithms (Pedregosa)  http://people.duke.edu/~ccc14/sta-663/BlackBoxOptimization.html  http://www.benfrederickson.com/numerical-optimization/ (notebook kind of post with python, d3)  http://www."><title>Mathematical Optimization</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=/icon.png><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Source+Sans+Pro:wght@400;600;700&family=Fira+Code:wght@400;700&display=swap" rel=stylesheet><link href=https://carlos-gg.github.io/digitalgarden/styles.cd61336c89ed6e03702366ce4a492b75.min.css rel=stylesheet><script src=https://carlos-gg.github.io/digitalgarden/js/darkmode.46b07878b7f5d9e26ad7a3c40f8a0605.min.js></script>
<script>const BASE_URL="https://carlos-gg.github.io/digitalgarden/",fetchData=Promise.all([fetch("https://carlos-gg.github.io/digitalgarden/indices/linkIndex.1ff99dcf7c422151409778e8568a4b35.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://carlos-gg.github.io/digitalgarden/indices/contentIndex.5c27f14f3db38f51a89e646fe043d1a2.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n}))</script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-S103D50NQ0"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S103D50NQ0",{anonymize_ip:!1})}</script><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://carlos-gg.github.io/digitalgarden/js/graph.27e8521c25c27c79dea35f434c486167.js></script>
<script>drawGraph("https://carlos-gg.github.io/digitalgarden/AI/Math-and-Statistics/Mathematical-Optimization","https://carlos-gg.github.io/digitalgarden",[{"/moc":"#4388cc"}],-1,!0,!1,!0)</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://carlos-gg.github.io/digitalgarden/js/search.bc849b857f2c1b822264d40635bb67b6.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://carlos-gg.github.io/digitalgarden/>CarlosGG's Knowledge Garden ðŸª´</a></h1><svg tabindex="0" id="search-icon" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg><div class=spacer></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Mathematical Optimization</h1><p class=meta>Last updated June 17, 2022</p><ul class=tags></ul><aside class=mainTOC><details open><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#resources>Resources</a><ol><li><a href=#heuristics>Heuristics</a></li><li><a href=#iterative-methods>Iterative methods</a></li></ol></li><li><a href=#code>Code</a></li></ol></nav></details></aside><h2 id=resources>Resources</h2><ul><li><a href=https://en.wikipedia.org/wiki/Mathematical_optimization rel=noopener>https://en.wikipedia.org/wiki/Mathematical_optimization</a></li><li><a href=http://fa.bianp.net/teaching/2018/eecs227at/ rel=noopener>A birds-eye view of optimization algorithms (Pedregosa)</a></li><li><a href=http://people.duke.edu/~ccc14/sta-663/BlackBoxOptimization.html rel=noopener>http://people.duke.edu/~ccc14/sta-663/BlackBoxOptimization.html</a></li><li><a href=http://www.benfrederickson.com/numerical-optimization/ rel=noopener>http://www.benfrederickson.com/numerical-optimization/</a> (notebook kind of post with python, d3)</li><li><a href=http://www.kdnuggets.com/2016/12/hard-thing-about-deep-learning.html rel=noopener>http://www.kdnuggets.com/2016/12/hard-thing-about-deep-learning.html</a></li><li><a href=https://www.neuraldesigner.com/blog/5_algorithms_to_train_a_neural_network rel=noopener>https://www.neuraldesigner.com/blog/5_algorithms_to_train_a_neural_network</a></li><li><a href=http://distill.pub/2017/momentum/ rel=noopener>Why Momentum works</a></li></ul><h3 id=heuristics>Heuristics</h3><ul><li>A heuristic is any algorithm which is not guaranteed (mathematically) to find the solution, but which is nevertheless useful in certain practical situations.</li><li><a href=/digitalgarden/AI/Math-and-Statistics/Genetic-algorithms rel=noopener class=internal-link data-src=/digitalgarden/AI/Math-and-Statistics/Genetic-algorithms>Genetic algorithms</a></li><li>Nelderâ€“Mead method<ul><li><a href=https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method rel=noopener>https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method</a></li><li>The Nelderâ€“Mead method or downhill simplex method or amoeba method is a commonly applied numerical method used to find the minimum or maximum of an objective function in a multidimensional space. It is applied to nonlinear optimization problems for which derivatives may not be known. However, the Nelderâ€“Mead technique is a heuristic search method that can converge to non-stationary points on problems that can be solved by alternative methods.</li><li>The method uses the concept of a simplex, which is a special polytope of n+ 1 vertices in n dimensions. Examples of simplices include a line segment on a line, a triangle on a plane, a tetrahedron in three-dimensional space and so forth. The method approximates a local optimum of a problem with n variables when the objective function varies smoothly and is unimodal.</li><li>Nelderâ€“Mead in n dimensions maintains a set of n+1test points arranged as a simplex. It then extrapolates the behavior of the objective function measured at each test point, in order to find a new test point and to replace one of the old test points with the new one, and so the technique progresses. The simplest approach is to replace the worst point with a point reflected through the centroid of the remaining n points. If this point is better than the best current point, then we can try stretching exponentially out along this line. On the other hand, if this new point isn&rsquo;t much better than the previous value, then we are stepping across a valley, so we shrink the simplex towards a better point.</li><li><a href=https://pyfssa.readthedocs.io/en/stable/nelder-mead.html rel=noopener>The Nelderâ€“Mead algorithm</a></li><li><a href=https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin.html rel=noopener>https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin.html</a></li></ul></li></ul><h3 id=iterative-methods>Iterative methods</h3><p>The iterative methods used to solve problems of nonlinear programming differ according to whether they evaluate Hessians, gradients, or only function values.</p><ul><li>Gradient descent<ul><li><a href=https://en.wikipedia.org/wiki/Gradient_descent rel=noopener>https://en.wikipedia.org/wiki/Gradient_descent</a></li><li>Gradient descent is a first-order iterative optimization algorithm. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or of the approximate gradient) of the function at the current point. Gradient descent is also known as steepest descent, or the method of steepest descent.</li><li><a href=http://ruder.io/optimizing-gradient-descent/index.html rel=noopener>An overview of gradient descent optimization algorithms</a></li><li><a href=https://towardsdatascience.com/gradient-descent-demystified-bc30b26e432a rel=noopener>https://towardsdatascience.com/gradient-descent-demystified-bc30b26e432a</a></li><li><a href=https://www.jeremyjordan.me/gradient-descent/ rel=noopener>https://www.jeremyjordan.me/gradient-descent/</a></li></ul></li><li><a href=https://en.wikipedia.org/wiki/Conjugate_gradient_method rel=noopener>Conjugate gradient method</a></li><li><a href=https://en.wikipedia.org/wiki/Interior_point_method rel=noopener>Interior point method</a></li></ul><h2 id=code>Code</h2><ul><li>#CODE
<a href=https://code.fb.com/ai-research/nevergrad/ rel=noopener>Nevergrad (Facebook) - A Python toolbox for performing gradient-free optimization</a></li><li>#CODE
<a href=https://scikit-optimize.github.io/ rel=noopener>scikit-optimize</a></li><li>#CODE
<a href=https://gpflowopt.readthedocs.io/en/latest/index.html rel=noopener>GPflowOpt - library for Bayesian Optimization with GPflow</a></li><li>#CODE
<a href=http://pybrain.org/docs/api/optimization/optimization.html rel=noopener>PyBrain - Black-box Optimization Algorithms</a></li><li>#CODE
<a href=https://github.com/google/jax rel=noopener>JAX</a> ^jax<ul><li>JAX is Autograd and XLA, brought together for high-performance machine learning research. It can automatically differentiate native Python and NumPy functions</li><li>#TALK
<a href="https://www.youtube.com/watch?v=z-WSrQDXkuM" rel=noopener>JAX: Accelerated Machine Learning Research | SciPy 2020 | VanderPlas</a></li><li><a href=https://towardsdatascience.com/deep-learning-with-jax-and-elegy-c0765e3ec31a rel=noopener>https://towardsdatascience.com/deep-learning-with-jax-and-elegy-c0765e3ec31a</a></li><li>#TALK
<a href="https://www.youtube.com/watch?v=SstuvS-tVc0" rel=noopener>Machine Learning with JAX - From Zero to Hero</a></li></ul></li></ul></article><hr><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/digitalgarden/AI/Deep-learning/DL>Deep Learning (DL)</a></li><li><a href=/digitalgarden/AI/Math-and-Statistics/Math-and-Statistics>Math and Statistics</a></li></ul></div></div><div id=contact_buttons><footer><p>Made by Carlos Alberto Gomez Gonzalez using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2022</p><ul><li><a href=https://carlos-gg.github.io/digitalgarden/>Home</a></li><a href=https://carlos-gg.github.io>Carlos'Homepage</a></ul></footer></div><script src=https://carlos-gg.github.io/digitalgarden/js/popover.e57188d2e4c06b0654e020b3a734bb62.min.js></script>
<script>initPopover("https://carlos-gg.github.io/digitalgarden")</script></div></body></html>