<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="See:
 &ldquo;Regularized regression&rdquo; section in Regression  Feature learning  Resources  https://en.wikipedia.org/wiki/Feature_selection http://machinelearningmastery.com/an-introduction-to-feature-selection/ http://scikit-learn.org/stable/modules/feature_selection.html  Removing features with low variance  Univariate feature selection  Recursive feature elimination Regularization:  http://scikit-learn."><title>Feature selection</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=/icon.png><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Source+Sans+Pro:wght@400;600;700&family=Fira+Code:wght@400;700&display=swap" rel=stylesheet><link href=https://carlos-gg.github.io/digitalgarden/styles.fc271f84b56ab4040866b55bab91ea59.min.css rel=stylesheet><script src=https://carlos-gg.github.io/digitalgarden/js/darkmode.46b07878b7f5d9e26ad7a3c40f8a0605.min.js></script><script>const BASE_URL="https://carlos-gg.github.io/digitalgarden/",fetchData=Promise.all([fetch("https://carlos-gg.github.io/digitalgarden/indices/linkIndex.ec68a1664e9f59f7d667605a294052cc.min.json").then(a=>a.json()).then(a=>({index:a.index,links:a.links})),fetch("https://carlos-gg.github.io/digitalgarden/indices/contentIndex.68a21d6a81614ed8628d084fa3a1d09b.min.json").then(a=>a.json())]).then(([{index:a,links:b},c])=>({index:a,links:b,content:c}))</script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-S103D50NQ0"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-S103D50NQ0',{anonymize_ip:!1})}</script><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://carlos-gg.github.io/digitalgarden/js/graph.9e0d7f85fd8bcbb9da33fd6f8acdfeb9.js></script><script>drawGraph("https://carlos-gg.github.io/digitalgarden/AI/Supervised-Learning/Feature-selection","https://carlos-gg.github.io/digitalgarden",[{"/moc":"#4388cc"}],-1,!0,!1,!0)</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script><script defer src=https://carlos-gg.github.io/digitalgarden/js/search.3c0dba36397221e42c2e229db4303dc2.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://carlos-gg.github.io/digitalgarden/>CarlosGG's Knowledge Garden ðŸª´</a></h1><svg tabindex="0" id="search-icon" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg><div class=spacer></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Feature selection</h1><p class=meta>Last updated April 4, 2022</p><ul class=tags></ul><aside class=mainTOC><h3>Table of Contents</h3><nav id=TableOfContents><ol><li><a href=#resources>Resources</a></li><li><a href=#books>Books</a></li><li><a href=#code>Code</a></li></ol></nav></aside><p>See:</p><ul><li>&ldquo;Regularized regression&rdquo; section in
<a href=/digitalgarden/AI/Supervised-Learning/Regression rel=noopener class=internal-link data-src=/digitalgarden/AI/Supervised-Learning/Regression>Regression</a></li><li><a href=/digitalgarden/AI/Feature-learning rel=noopener class=internal-link data-src=/digitalgarden/AI/Feature-learning>Feature learning</a></li></ul><h2 id=resources>Resources</h2><ul><li><a href=https://en.wikipedia.org/wiki/Feature_selection>https://en.wikipedia.org/wiki/Feature_selection</a></li><li><a href=http://machinelearningmastery.com/an-introduction-to-feature-selection/>http://machinelearningmastery.com/an-introduction-to-feature-selection/</a></li><li><a href=http://scikit-learn.org/stable/modules/feature_selection.html>http://scikit-learn.org/stable/modules/feature_selection.html</a></li><li><a href=http://scikit-learn.org/stable/modules/feature_selection.html#removing-features-with-low-variance rel=noopener>Removing features with low variance</a></li><li><a href=http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection rel=noopener>Univariate feature selection</a></li><li><a href=http://scikit-learn.org/stable/modules/feature_selection.html#recursive-feature-elimination rel=noopener>Recursive feature elimination</a></li><li>Regularization:<ul><li><a href=http://scikit-learn.org/stable/modules/feature_selection.html#l1-based-feature-selection>http://scikit-learn.org/stable/modules/feature_selection.html#l1-based-feature-selection</a></li><li><a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>https://en.wikipedia.org/wiki/Regularization_(mathematics)</a></li><li>Penalized regression has been applied widely across many research disciplines, but it is a great fit for business data with many columns, even data sets with more columns than rows, and for data sets with a lot of correlated variables. L1/LASSO penalties drive unnecessary regression parameters to zero, selecting a small, representative subset of regression parameters for the regression model while avoiding potential multiple comparison problems that arise in forward, backward, and stepwise variable selection. Tikhonov/L2/ridge penalties help preserve parameter estimate stability, even when many correlated variables exist in a wide data set or important predictor variables are correlated. Itâ€™s also important to know penalized regression techniques donâ€™t always create confidence intervals, t-statistics, or p-values for regression parameters. These types of measures are typically only available through iterative methods or bootstrapping that can require extra computing time.</li><li><a href=https://www.quora.com/What-is-regularization-in-machine-learning>https://www.quora.com/What-is-regularization-in-machine-learning</a></li><li>The loss function is penalized by adding an L1 or L2 norm of the weights vector W (the vector of the learned parameters in the linear regression):<ul><li>L(X,Y) + lambda N(W), where N is either the L1, L2 or any other norm.</li><li>This helps avoiding overfitting and performs fetuses selection for the case of the L1 regularization. Lambda can be chosen by cross-validation.</li></ul></li></ul></li><li>Tree-based methods<ul><li><a href=http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html rel=noopener>Random forest, extra trees. Feature importances with forests of trees</a></li><li>XGBoost, Feature importance and why itâ€™s important:<ul><li><a href=http://datawhatnow.com/feature-importance/>http://datawhatnow.com/feature-importance/</a></li><li><a href=http://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/>http://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/</a></li><li>Importance is calculated for a single decision tree by the amount that each attribute split point improves the performance measure, weighted by the number of observations the node is responsible for. The performance measure may be the purity (Gini index) used to select the split points or another more specific error function. The feature importances are then averaged across all of the the decision trees within the model.</li></ul></li></ul></li></ul><h2 id=books>Books</h2><ul><li>#BOOK
<a href=http://www.feat.engineering/index.html rel=noopener>Feature Engineering and Selection: A Practical Approach for Predictive Models (Kuhn 2018)</a></li></ul><h2 id=code>Code</h2><ul><li>#CODE
<a href=https://github.com/jundongl/scikit-feature rel=noopener>Scikit-feature</a><ul><li><a href=http://featureselection.asu.edu/>http://featureselection.asu.edu/</a></li></ul></li><li>#CODE Feature-selector - Feature selector is a tool for dimensionality reduction of machine learning datasets.<ul><li>Methods: Missing Values, Single Unique Values, Collinear Features, Zero Importance Features, Low Importance Features</li><li><a href=https://github.com/WillKoehrsen/feature-selector/blob/master/Feature%20Selector%20Usage.ipynb>https://github.com/WillKoehrsen/feature-selector/blob/master/Feature%20Selector%20Usage.ipynb</a></li><li><a href=https://towardsdatascience.com/a-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0>https://towardsdatascience.com/a-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0</a></li></ul></li><li>#CODE
<a href=https://github.com/ctlab/ITMO_FS rel=noopener>ITMO_FS</a><ul><li>Feature selection library in python</li><li><a href=https://itmo-fs.readthedocs.io/en/latest/>https://itmo-fs.readthedocs.io/en/latest/</a></li></ul></li></ul></article><hr><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=https://carlos-gg.github.io/digitalgarden//AI/Machine-Learning>Machine Learning</a></li></ul></div></div><div id=contact_buttons><footer><p>Made by Carlos Alberto Gomez Gonzalez using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2022</p><a href=https://carlos-gg.github.io/digitalgarden/>Root</a><a href=https://carlos-gg.github.io>CarlosGG</a></footer></div><script src=https://carlos-gg.github.io/digitalgarden/js/popover.718bae159805a590cda4e9bcb14b9dcc.min.js></script><script>initPopover("https://carlos-gg.github.io/digitalgarden")</script></div></body></html>