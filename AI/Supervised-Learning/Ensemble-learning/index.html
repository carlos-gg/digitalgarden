<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone."><title>Ensemble learning</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=/icon.png><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Source+Sans+Pro:wght@400;600;700&family=Fira+Code:wght@400;700&display=swap" rel=stylesheet><link href=https://carlos-gg.github.io/digitalgarden/styles.cd61336c89ed6e03702366ce4a492b75.min.css rel=stylesheet><script src=https://carlos-gg.github.io/digitalgarden/js/darkmode.46b07878b7f5d9e26ad7a3c40f8a0605.min.js></script>
<script>const BASE_URL="https://carlos-gg.github.io/digitalgarden/",fetchData=Promise.all([fetch("https://carlos-gg.github.io/digitalgarden/indices/linkIndex.99f1676cd5e9e1513d7f4c7ed882d133.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://carlos-gg.github.io/digitalgarden/indices/contentIndex.9d373978392d5fba00bff13848d2c39c.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n}))</script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-S103D50NQ0"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S103D50NQ0",{anonymize_ip:!1})}</script><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://carlos-gg.github.io/digitalgarden/js/graph.27e8521c25c27c79dea35f434c486167.js></script>
<script>drawGraph("https://carlos-gg.github.io/digitalgarden/AI/Supervised-Learning/Ensemble-learning","https://carlos-gg.github.io/digitalgarden",[{"/moc":"#4388cc"}],-1,!0,!1,!0)</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://carlos-gg.github.io/digitalgarden/js/search.bc849b857f2c1b822264d40635bb67b6.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://carlos-gg.github.io/digitalgarden/>CarlosGG's Knowledge Garden ðŸª´</a></h1><svg tabindex="0" id="search-icon" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg><div class=spacer></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Ensemble learning</h1><p class=meta>Last updated May 20, 2022</p><ul class=tags></ul><aside class=mainTOC><details open><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#resources>Resources</a><ol><li><a href=#bagging>Bagging</a></li><li><a href=#boosting>Boosting</a></li><li><a href=#stacking>Stacking</a></li></ol></li></ol></nav></details></aside><blockquote><p>In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone. In general, ensembling is a technique of combining two or more algorithms of similar or dissimilar types called base learners. This is done to make a more robust system (improving generalizability / robustness over a single estimator) which incorporates the predictions from all the base learners</p></blockquote><h2 id=resources>Resources</h2><ul><li><a href=https://en.wikipedia.org/wiki/Ensemble_learning rel=noopener>https://en.wikipedia.org/wiki/Ensemble_learning</a></li><li>By analogy, ensemble techniques have been used also in <a href=/digitalgarden/AI/Unsupervised-learning/Unsupervised-learning rel=noopener class=internal-link data-src=/digitalgarden/AI/Unsupervised-learning/Unsupervised-learning>AI/Unsupervised learning/Unsupervised learning</a> scenarios, for example in consensus clustering or in anomaly or <a href=/digitalgarden/AI/Anomaly-and-Outlier-Detection rel=noopener class=internal-link data-src=/digitalgarden/AI/Anomaly-and-Outlier-Detection>AI/Anomaly and Outlier Detection</a></li><li><a href=http://scikit-learn.org/stable/modules/ensemble.html rel=noopener>http://scikit-learn.org/stable/modules/ensemble.html</a></li><li><a href=http://mlwave.com/kaggle-ensembling-guide/ rel=noopener>http://mlwave.com/kaggle-ensembling-guide/</a></li><li><a href=https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/ rel=noopener>https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/</a></li><li><a href=http://www.datasciencecentral.com/profiles/blogs/improving-predictions-with-ensemble-model rel=noopener>http://www.datasciencecentral.com/profiles/blogs/improving-predictions-with-ensemble-model</a></li><li><a href=http://www.kdnuggets.com/2016/11/data-science-basics-intro-ensemble-learners.html rel=noopener>http://www.kdnuggets.com/2016/11/data-science-basics-intro-ensemble-learners.html</a></li><li><a href=https://medium.com/diogo-menezes-borges/ensemble-learning-when-everybody-takes-a-guess-i-guess-ec35f6cb4600 rel=noopener>https://medium.com/diogo-menezes-borges/ensemble-learning-when-everybody-takes-a-guess-i-guess-ec35f6cb4600</a></li><li><a href=https://blog.statsbot.co/ensemble-learning-d1dcd548e936 rel=noopener>https://blog.statsbot.co/ensemble-learning-d1dcd548e936</a></li><li><a href=https://machinelearningmastery.com/horizontal-voting-ensemble/ rel=noopener>How to Reduce Variance in the Final DL Model With a Horizontal Voting Ensemble</a></li></ul><h3 id=bagging>Bagging</h3><ul><li>With bootstrap aggregating (Bagging) we build models of smaller datasets by sampling with replacement. The results of these bootstrap samples are then aggregated, using majority voting (equal weighting of models)</li><li>See <a href=/digitalgarden/AI/Supervised-Learning/Random-forest rel=noopener class=internal-link data-src=/digitalgarden/AI/Supervised-Learning/Random-forest>AI/Supervised Learning/Random forest</a></li></ul><h3 id=boosting>Boosting</h3><p>See <a href=/digitalgarden/AI/Supervised-Learning/Gradient-boosting rel=noopener class=internal-link data-src=/digitalgarden/AI/Supervised-Learning/Gradient-boosting>AI/Supervised Learning/Gradient boosting</a></p><ul><li>Same as bagging but operates via weighted voting. Algorithm proceeds iteratively (one tries to reduce the bias of the combined estimator); new models are influenced by previous ones. E.g. AdaBoost (Adaptive Boosting) and LogitBoost</li><li><a href=https://en.wikipedia.org/wiki/AdaBoost rel=noopener>https://en.wikipedia.org/wiki/AdaBoost</a></li></ul><h3 id=stacking>Stacking</h3><ul><li>uses a meta learner (as opposed to bagging/boosting which use voting schemes)</li><li>It consists in training multiple learners/algorithms (as opposed to bagging/boosting which train a single learner). Each learner uses a subset of data.</li><li>A &ldquo;combiner&rdquo; is trained on a validation set. This combiner can be any ensemble technique, but logistic regression is often found to be an adequate and simple algorithm to perform this combining.</li><li><a href=http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/ rel=noopener>http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/</a></li><li><a href=https://machinelearningmastery.com/ensemble-methods-for-deep-learning-neural-networks/ rel=noopener>https://machinelearningmastery.com/ensemble-methods-for-deep-learning-neural-networks/</a></li></ul></article><hr><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/digitalgarden/AI/Supervised-Learning/Classification>Classification</a></li><li><a href=/digitalgarden/AI/Supervised-Learning/Gradient-boosting>Gradient boosting</a></li><li><a href=/digitalgarden/AI/Supervised-Learning/Random-forest>Random forest</a></li><li><a href=/digitalgarden/AI/Supervised-Learning/Regression>Regression</a></li><li><a href=/digitalgarden/AI/Supervised-Learning/Supervised-learning>Supervised Learning</a></li></ul></div></div><div id=contact_buttons><footer><p>Made by Carlos Alberto Gomez Gonzalez using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2022</p><ul><li><a href=https://carlos-gg.github.io/digitalgarden/>Home</a></li><a href=https://carlos-gg.github.io>Carlos'Homepage</a></ul></footer></div><script src=https://carlos-gg.github.io/digitalgarden/js/popover.e57188d2e4c06b0654e020b3a734bb62.min.js></script>
<script>initPopover("https://carlos-gg.github.io/digitalgarden")</script></div></body></html>