<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Resources   https://github.com/jmportilla/Udemy---Machine-Learning/blob/master/Multi-Class%20Classification.ipynb Comparison of classifiers   https://www.quora.com/What-are-the-advantages-of-different-classification-algorithms  http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html    https://medium.com/@maheshkkumar/implementing-a-binary-classifier-in-python-b69d08d8da21#.goynlh7ah Metrics   https://www.neuraldesigner.com/blog/methods-binary-classification  http://machinelearningmastery."><title>Classification</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=/icon.png><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Source+Sans+Pro:wght@400;600;700&family=Fira+Code:wght@400;700&display=swap" rel=stylesheet><link href=https://carlos-gg.github.io/digitalgarden/styles.cd61336c89ed6e03702366ce4a492b75.min.css rel=stylesheet><script src=https://carlos-gg.github.io/digitalgarden/js/darkmode.46b07878b7f5d9e26ad7a3c40f8a0605.min.js></script>
<script>const BASE_URL="https://carlos-gg.github.io/digitalgarden/",fetchData=Promise.all([fetch("https://carlos-gg.github.io/digitalgarden/indices/linkIndex.4128b711e7ba4b8ed0ddb796152aec9b.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://carlos-gg.github.io/digitalgarden/indices/contentIndex.e998027d22d5f292076f756856644575.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n}))</script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-S103D50NQ0"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S103D50NQ0",{anonymize_ip:!1})}</script><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://carlos-gg.github.io/digitalgarden/js/graph.27e8521c25c27c79dea35f434c486167.js></script>
<script>drawGraph("https://carlos-gg.github.io/digitalgarden/AI/Supervised-Learning/Classification","https://carlos-gg.github.io/digitalgarden",[{"/moc":"#4388cc"}],-1,!0,!1,!0)</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://carlos-gg.github.io/digitalgarden/js/search.bc849b857f2c1b822264d40635bb67b6.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://carlos-gg.github.io/digitalgarden/>CarlosGG's Knowledge Garden 🪴</a></h1><svg tabindex="0" id="search-icon" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg><div class=spacer></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Classification</h1><p class=meta>Last updated August 8, 2022</p><ul class=tags></ul><aside class=mainTOC><details open><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#resources>Resources</a><ol><li><a href=#naive-bayes>Naive Bayes</a></li><li><a href=#support-vector-machine-svm>Support Vector Machine (SVM)</a></li><li><a href=#k-nearest-neighbors>K-nearest neighbors</a></li><li><a href=#logistic-regression>Logistic regression</a></li><li><a href=#gaussian-processes>Gaussian Processes</a></li><li><a href=#tree-based-approaches>Tree-based approaches</a></li></ol></li></ol></nav></details></aside><h2 id=resources>Resources</h2><ul><li><a href=https://github.com/jmportilla/Udemy---Machine-Learning/blob/master/Multi-Class-Classification.ipynb rel=noopener>https://github.com/jmportilla/Udemy---Machine-Learning/blob/master/Multi-Class%20Classification.ipynb</a></li><li>Comparison of classifiers<ul><li><a href=https://www.quora.com/What-are-the-advantages-of-different-classification-algorithms rel=noopener>https://www.quora.com/What-are-the-advantages-of-different-classification-algorithms</a></li><li><a href=http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html rel=noopener>http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html</a></li></ul></li><li><a href=https://medium.com/@maheshkkumar/implementing-a-binary-classifier-in-python-b69d08d8da21#.goynlh7ah rel=noopener>https://medium.com/@maheshkkumar/implementing-a-binary-classifier-in-python-b69d08d8da21#.goynlh7ah</a></li><li>Metrics<ul><li><a href=https://www.neuraldesigner.com/blog/methods-binary-classification rel=noopener>https://www.neuraldesigner.com/blog/methods-binary-classification</a></li><li><a href=http://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/ rel=noopener>http://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/</a></li><li><a href=http://docs.aws.amazon.com/machine-learning/latest/dg/binary-model-insights.html rel=noopener>http://docs.aws.amazon.com/machine-learning/latest/dg/binary-model-insights.html</a></li></ul></li></ul><h3 id=naive-bayes>Naive Bayes</h3><ul><li><a href=https://blancosilva.wordpress.com/2012/06/21/naive-bayes/ rel=noopener>https://blancosilva.wordpress.com/2012/06/21/naive-bayes/</a></li><li><a href=https://github.com/jmportilla/Udemy---Machine-Learning/blob/master/Supervised-Learning---Naive-Bayes.ipynb rel=noopener>https://github.com/jmportilla/Udemy---Machine-Learning/blob/master/Supervised%20Learning%20-%20Naive%20Bayes.ipynb</a></li><li><a href="http://stackoverflow.com/questions/10059594/a-simple-explanation-of-naive-bayes-classification?rq=1" rel=noopener>http://stackoverflow.com/questions/10059594/a-simple-explanation-of-naive-bayes-classification?rq=1</a></li><li><a href=https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/ rel=noopener>https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/</a></li><li>#TALK
<a href="https://www.youtube.com/watch?v=IlVINQDk4o8" rel=noopener>5 Minutes With Ingo - Naïve Bayes</a></li></ul><h3 id=support-vector-machine-svm>Support Vector Machine (SVM)</h3><ul><li>Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier. An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. SVM figures out how to separate the data based on the labels or outputs we’ve defined.</li><li>SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces. It&rsquo;s much more computationally intensive. Complexity: O(n^3m), n number of points, m number of dimensions</li><li><a href=https://github.com/jmportilla/Udemy---Machine-Learning/blob/master/Support-Vector-Machines.ipynb rel=noopener>https://github.com/jmportilla/Udemy---Machine-Learning/blob/master/Support%20Vector%20Machines.ipynb</a></li><li><a href=https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/ rel=noopener>https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/</a></li><li><a href=http://blog.yhat.com/posts/why-support-vector-machine.html rel=noopener>http://blog.yhat.com/posts/why-support-vector-machine.html</a></li><li>The kernel trick takes the data you give it and transforms it. In expanding the dataset there are now more obvious boundaries between your classes and the SVM algorithm is able to compute a much more optimal hyperplane.</li><li>#TALK
<a href="https://www.youtube.com/watch?v=-Z4aojJ-pdg&t=331s" rel=noopener>How Support Vector Machines work / How to open a black box (Brandon Rohrer)</a></li><li>#TALK 5 Minutes With Ingo - Understanding Support Vector Machines:<ul><li><a href="https://www.youtube.com/watch?v=YsiWisFFruY" rel=noopener>https://www.youtube.com/watch?v=YsiWisFFruY</a></li><li><a href="https://www.youtube.com/watch?v=woEwY0Zi6X4" rel=noopener>https://www.youtube.com/watch?v=woEwY0Zi6X4</a></li></ul></li><li>#TALK
<a href="https://www.youtube.com/watch?v=1NxnPkZM9bc" rel=noopener>How SVM (Support Vector Machine) algorithm works</a></li></ul><h3 id=k-nearest-neighbors>K-nearest neighbors</h3><p>See <a href=/digitalgarden/AI/Supervised-Learning/Regression rel=noopener class=internal-link data-src=/digitalgarden/AI/Supervised-Learning/Regression>AI/Supervised Learning/Regression</a></p><h3 id=logistic-regression>Logistic regression</h3><ul><li><a href=https://en.wikipedia.org/wiki/Logistic_regression rel=noopener>https://en.wikipedia.org/wiki/Logistic_regression</a></li><li>In statistics, logistic regression, or logit regression, or logit model is a regression model where the dependent variable (DV) is categorical. This is the case of a binary dependent variable, which can take only two values, &ldquo;0&rdquo; and “1”.</li><li>Cases where the dependent variable has more than two outcome categories may be analysed in multinomial logistic regression, or, if the multiple categories are ordered, in ordinal logistic regression.</li><li><a href=http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html rel=noopener>http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html</a></li><li><a href=https://github.com/jmportilla/Udemy---Machine-Learning/blob/master/Supervised-Learning---Logistic-Regression.ipynb rel=noopener>https://github.com/jmportilla/Udemy---Machine-Learning/blob/master/Supervised%20Learning%20-%20Logistic%20Regression.ipynb</a></li><li><a href=http://www.kdnuggets.com/2017/02/gentlest-introduction-tensorflow-part-4.html rel=noopener>http://www.kdnuggets.com/2017/02/gentlest-introduction-tensorflow-part-4.html</a></li><li><a href=https://stats.stackexchange.com/questions/228763/regularization-methods-for-logistic-regression rel=noopener>https://stats.stackexchange.com/questions/228763/regularization-methods-for-logistic-regression</a></li><li>Regularization can be used in all linear methods, including both regression and classification. There is not too much difference between regression and classification: the only difference is the loss function. Specifically, there are three major components of a linear method: Loss Function, Regularization, Algorithms. Where loss function plus regularization is the objective function in the problem in optimization form and the algorithm is the way to solve it.</li><li>Logistic loss can be used for classification: L(y, y’) = log(1 + exp(−y’y))</li><li>Logistic regression with L2 regularization: minimizew ∑_x,y log(1 + exp(−w^T x⋅y)) + λw^Tw</li></ul><h3 id=gaussian-processes>Gaussian Processes</h3><p>See <a href=/digitalgarden/AI/Supervised-Learning/Regression rel=noopener class=internal-link data-src=/digitalgarden/AI/Supervised-Learning/Regression>AI/Supervised Learning/Regression</a></p><h3 id=tree-based-approaches>Tree-based approaches</h3><p>See <a href=/digitalgarden/AI/Supervised-Learning/Ensemble-learning rel=noopener class=internal-link data-src=/digitalgarden/AI/Supervised-Learning/Ensemble-learning>AI/Supervised Learning/Ensemble learning</a>, <a href=/digitalgarden/AI/Supervised-Learning/Random-forest rel=noopener class=internal-link data-src=/digitalgarden/AI/Supervised-Learning/Random-forest>AI/Supervised Learning/Random forest</a> and <a href=/digitalgarden/AI/Supervised-Learning/Gradient-boosting rel=noopener class=internal-link data-src=/digitalgarden/AI/Supervised-Learning/Gradient-boosting>AI/Supervised Learning/Gradient boosting</a></p><ul><li><a href=https://github.com/jmportilla/Udemy---Machine-Learning/blob/master/Decision-Trees-and-Random-Forests.ipynb rel=noopener>https://github.com/jmportilla/Udemy---Machine-Learning/blob/master/Decision%20Trees%20and%20Random%20Forests.ipynb</a></li><li><a href=https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/ rel=noopener>https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/</a></li><li>Decision trees (DT): DT-learning uses a decision tree as a predictive model which maps observations about an item (represented in the branches) to conclusions about the item&rsquo;s target value (represented in the leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees.</li></ul></article><hr><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/digitalgarden/AI/Supervised-Learning/Regression>Regression</a></li><li><a href=/digitalgarden/AI/Supervised-Learning/Supervised-learning>Supervised Learning</a></li></ul></div></div><div id=contact_buttons><footer><p>Made by Carlos Alberto Gomez Gonzalez using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2022</p><ul><li><a href=https://carlos-gg.github.io/digitalgarden/>Home</a></li><a href=https://carlos-gg.github.io>Carlos'Homepage</a></ul></footer></div><script src=https://carlos-gg.github.io/digitalgarden/js/popover.e57188d2e4c06b0654e020b3a734bb62.min.js></script>
<script>initPopover("https://carlos-gg.github.io/digitalgarden")</script></div></body></html>