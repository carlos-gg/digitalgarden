<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Resources   https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis  http://www.marcoaltini.com/blog/dealing-with-imbalanced-data-undersampling-oversampling-and-proper-cross-validation  http://www.alfredo.motta.name/cross-validation-done-wrong/  http://www.win-vector.com/blog/2015/02/does-balancing-classes-improve-classifier-performance/  http://www.chioka.in/class-imbalance-problem/  http://www.kdnuggets.com/2017/06/7-techniques-handle-imbalanced-data.html  https://svds.com/learning-imbalanced-classes/  Conventional algorithms are often biased towards the majority class because their loss functions attempt to optimize quantities such as error rate, not taking the data distribution into consideration."><title>Class imbalance</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=/icon.png><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Source+Sans+Pro:wght@400;600;700&family=Fira+Code:wght@400;700&display=swap" rel=stylesheet><link href=https://carlos-gg.github.io/digitalgarden/styles.cd61336c89ed6e03702366ce4a492b75.min.css rel=stylesheet><script src=https://carlos-gg.github.io/digitalgarden/js/darkmode.46b07878b7f5d9e26ad7a3c40f8a0605.min.js></script>
<script>const BASE_URL="https://carlos-gg.github.io/digitalgarden/",fetchData=Promise.all([fetch("https://carlos-gg.github.io/digitalgarden/indices/linkIndex.7f19b91fe04ef4490deebd0b2349a6c5.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://carlos-gg.github.io/digitalgarden/indices/contentIndex.fa824c793b1d9c44a2ba64d2fda1776a.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n}))</script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-S103D50NQ0"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S103D50NQ0",{anonymize_ip:!1})}</script><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://carlos-gg.github.io/digitalgarden/js/graph.27e8521c25c27c79dea35f434c486167.js></script>
<script>drawGraph("https://carlos-gg.github.io/digitalgarden/AI/Supervised-Learning/Class-imbalance","https://carlos-gg.github.io/digitalgarden",[{"/moc":"#4388cc"}],-1,!0,!1,!0)</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://carlos-gg.github.io/digitalgarden/js/search.bc849b857f2c1b822264d40635bb67b6.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://carlos-gg.github.io/digitalgarden/>CarlosGG's Knowledge Garden 🪴</a></h1><svg tabindex="0" id="search-icon" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg><div class=spacer></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Class imbalance</h1><p class=meta>Last updated June 30, 2022</p><ul class=tags></ul><aside class=mainTOC><details open><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#resources>Resources</a></li><li><a href=#code>Code</a></li><li><a href=#approaches>Approaches</a><ol><li><a href=#resampling>Resampling</a><ol><li><a href=#oversampling>Oversampling</a></li><li><a href=#undersampling>Undersampling</a></li></ol></li><li><a href=#adjust-the-class-importance-or-the-metric>Adjust the class importance or the metric</a></li><li><a href=#cost-sensitive-training>Cost-sensitive training</a></li><li><a href=#select-or-create-a-suitable-algorithm>Select or create a suitable algorithm</a></li></ol></li></ol></nav></details></aside><h2 id=resources>Resources</h2><ul><li><a href=https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis rel=noopener>https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis</a></li><li><a href=http://www.marcoaltini.com/blog/dealing-with-imbalanced-data-undersampling-oversampling-and-proper-cross-validation rel=noopener>http://www.marcoaltini.com/blog/dealing-with-imbalanced-data-undersampling-oversampling-and-proper-cross-validation</a></li><li><a href=http://www.alfredo.motta.name/cross-validation-done-wrong/ rel=noopener>http://www.alfredo.motta.name/cross-validation-done-wrong/</a></li><li><a href=http://www.win-vector.com/blog/2015/02/does-balancing-classes-improve-classifier-performance/ rel=noopener>http://www.win-vector.com/blog/2015/02/does-balancing-classes-improve-classifier-performance/</a></li><li><a href=http://www.chioka.in/class-imbalance-problem/ rel=noopener>http://www.chioka.in/class-imbalance-problem/</a></li><li><a href=http://www.kdnuggets.com/2017/06/7-techniques-handle-imbalanced-data.html rel=noopener>http://www.kdnuggets.com/2017/06/7-techniques-handle-imbalanced-data.html</a></li><li><a href=https://svds.com/learning-imbalanced-classes/ rel=noopener>https://svds.com/learning-imbalanced-classes/</a><ul><li>Conventional algorithms are often biased towards the majority class because their loss functions attempt to optimize quantities such as error rate, not taking the data distribution into consideration. Result: a trivial classifier that classifies every example as the majority class.</li></ul></li></ul><h2 id=code>Code</h2><ul><li>#CODE
<a href=https://github.com/scikit-learn-contrib/imbalanced-learn rel=noopener>Imbalanced-learn</a><ul><li><a href=https://imbalanced-learn.readthedocs.io/en/stable/ rel=noopener>https://imbalanced-learn.readthedocs.io/en/stable/</a></li><li><a href=https://imbalanced-learn.readthedocs.io/en/stable/api.html rel=noopener>https://imbalanced-learn.readthedocs.io/en/stable/api.html</a></li></ul></li><li>#CODE
<a href=https://github.com/analyticalmindsltd/smote_variants rel=noopener>Smote_variants</a><ul><li><a href=http://smote-variants.readthedocs.io/ rel=noopener>http://smote-variants.readthedocs.io/</a></li><li>The package implements 85 variants of the Synthetic Minority Oversampling Technique (SMOTE). Besides the implementations, an easy to use model selection framework is supplied to enable the rapid evaluation of oversampling techniques on unseen datasets.</li></ul></li></ul><h2 id=approaches>Approaches</h2><h3 id=resampling>Resampling</h3><ul><li>Balance the training dataset</li><li>#PAPER
<a href=https://arxiv.org/abs/1608.06048 rel=noopener>Survey of resampling techniques for improving classification performance in unbalanced datasets (More 2016)</a></li></ul><h4 id=oversampling>Oversampling</h4><ul><li>#PAPER
<a href=https://jair.org/index.php/jair/article/view/10302 rel=noopener>SMOTE: Synthetic Minority Over-sampling Technique (Chaula 2002)</a><ul><li>There are a number of methods available to oversample a dataset used in a typical classification problem (using a classification algorithm to classify a set of images, given a labelled training set of images). The most common technique is known as SMOTE.</li><li><a href=https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/ rel=noopener>https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/</a></li></ul></li><li>#PAPER
<a href=https://ieeexplore.ieee.org/document/4633969 rel=noopener>ADASYN: Adaptive synthetic sampling approach for imbalanced learning (He 2008)</a><ul><li><a href=https://sci2s.ugr.es/keel/pdf/algorithm/congreso/2008-He-ieee.pdf rel=noopener>https://sci2s.ugr.es/keel/pdf/algorithm/congreso/2008-He-ieee.pdf</a></li><li>ADASYN builds on the methodology of SMOTE, by shifting the importance of the classification boundary to those minority classes which are difficult. ADASYN uses a weighted distribution for different minority class examples according to their level of difficulty in learning, where more synthetic data is generated for minority class examples that are harder to learn.</li></ul></li></ul><h4 id=undersampling>Undersampling</h4><ul><li>Down-sampling involves randomly removing observations from the majority class to prevent its signal from dominating the learning algorithm. The most common heuristic for doing so is resampling without replacement.</li><li><a href=https://machinelearningmastery.com/undersampling-algorithms-for-imbalanced-classification/ rel=noopener>https://machinelearningmastery.com/undersampling-algorithms-for-imbalanced-classification/</a></li><li>Cluster. Cluster centroids is a method that replaces cluster of samples by the cluster centroid of a K-means algorithm, where the number of clusters is set by the level of undersampling.</li><li>Tomek links. Tomek links remove unwanted overlap between classes where majority class links are removed until all minimally distanced nearest neighbor pairs are of the same class. Tomek links are pairs of instances of opposite classes who are their own nearest neighbors. Tomek’s algorithm looks for such pairs and removes the majority instance of the pair.<ul><li><a href=https://pdfs.semanticscholar.org/6ec4/18f9071f3a96d5548e87e34be3665703119e.pdf rel=noopener>Classification of Imbalance Data using Tomek Link (T-Link) Combined with Random Under-sampling (RUS) as a Data Reduction Method</a></li></ul></li><li>Throw away minority examples and switch to an anomaly detection framework</li></ul><h3 id=adjust-the-class-importance-or-the-metric>Adjust the class importance or the metric</h3><ul><li>At the algorithm level, or after: Adjust the class weight (misclassification costs), adjust the decision threshold. Many machine learning toolkits have ways to adjust the “importance” of classes (classifiers that take an optional class_weight).</li><li>Change the metric.<ul><li>Evaluating the classifier: Accuracy is not a good metric for imbalanced classes!!</li><li>Use a ROC curve</li><li>Don’t get hard classifications (labels) from your classifier (via score or predict). Instead, get probability estimates via proba or predict_proba</li><li>No matter what you do for training, always test on the natural (stratified) distribution your classifier is going to operate upon. Seesklearn.cross_validation.StratifiedKFold</li><li>For a singe metric (value): AUC, F1 (harmonic mean of precision and recall), Cohen’s Kappa (evaluation statistic that takes into account how much agreement would be expected by chance)</li><li><a href=https://medium.com/towards-data-science/what-metrics-should-we-use-on-imbalanced-data-set-precision-recall-roc-e2e79252aeba rel=noopener>https://medium.com/towards-data-science/what-metrics-should-we-use-on-imbalanced-data-set-precision-recall-roc-e2e79252aeba</a></li><li><a href=http://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/ rel=noopener>http://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/</a></li><li>The following performance measures that can give more insight into the accuracy of the model than traditional classification accuracy:<ul><li>Confusion Matrix: A breakdown of predictions into a table showing correct predictions (the diagonal) and the types of incorrect predictions made (what classes incorrect predictions were assigned).</li><li>Precision: A measure of a classifiers exactness.</li><li>Recall: A measure of a classifiers completeness</li><li>F1 Score (or F-score): A weighted average of precision and recall.</li><li>Kappa (or Cohen’s kappa): Classification accuracy normalized by the imbalance of the classes in the data.</li><li>ROC Curves: Like precision and recall, accuracy is divided into sensitivity and specificity and models can be chosen based on the balance thresholds of these values.</li></ul></li></ul></li></ul><h3 id=cost-sensitive-training>Cost-sensitive training</h3><ul><li>Cost-Sensitive Training. For this tactic we use penalized learning algorithms that increase the cost of classification mistakes on the minority class. A popular algorithm for this technique is Penalized-SVM. During training, we can use the argument class_weight=&lsquo;balanced&rsquo; to penalize mistakes on the minority class by an amount proportional to how under-represented it is.</li></ul><h3 id=select-or-create-a-suitable-algorithm>Select or create a suitable algorithm</h3><ul><li>Create new algorithm for the imbalanced classes situation, or use one which handles the data imbalance</li><li>#PAPER
<a href="https://ieeexplore.ieee.org/document/5645694?arnumber=5645694" rel=noopener>Boosting/bagging. Comparing Boosting and Bagging Techniques With Noisy and Imbalanced Data (Khoshgoftaar 2010)</a><ul><li>The experiments show that the bagging techniques generally outperform boosting, and hence in noisy data environments, bagging is the preferred method for handling class imbalance.</li></ul></li></ul></article><hr><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/digitalgarden/AI/Supervised-Learning/Supervised-learning>Supervised Learning</a></li></ul></div></div><div id=contact_buttons><footer><p>Made by Carlos Alberto Gomez Gonzalez using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2022</p><ul><li><a href=https://carlos-gg.github.io/digitalgarden/>Home</a></li><a href=https://carlos-gg.github.io>Carlos'Homepage</a></ul></footer></div><script src=https://carlos-gg.github.io/digitalgarden/js/popover.e57188d2e4c06b0654e020b3a734bb62.min.js></script>
<script>initPopover("https://carlos-gg.github.io/digitalgarden")</script></div></body></html>