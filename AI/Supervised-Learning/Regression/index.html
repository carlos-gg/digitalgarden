<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="See:
  Time Series analysis  RNNs &ldquo;Sequence time series modelling&rdquo; section in CNNs  Resources  https://www.analytics. idhya.com/blog/2015/08/comprehensive-guide-regression/ https://towardsdatascience."><title>Regression</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=/icon.png><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Source+Sans+Pro:wght@400;600;700&family=Fira+Code:wght@400;700&display=swap" rel=stylesheet><link href=https://carlos-gg.github.io/digitalgarden/styles.fc271f84b56ab4040866b55bab91ea59.min.css rel=stylesheet><script src=https://carlos-gg.github.io/digitalgarden/js/darkmode.46b07878b7f5d9e26ad7a3c40f8a0605.min.js></script><script>const BASE_URL="https://carlos-gg.github.io/digitalgarden/",fetchData=Promise.all([fetch("https://carlos-gg.github.io/digitalgarden/indices/linkIndex.5cf7dac066e1f318d5588b035fc83d1e.min.json").then(a=>a.json()).then(a=>({index:a.index,links:a.links})),fetch("https://carlos-gg.github.io/digitalgarden/indices/contentIndex.957b5c6047c6072aa91e3d66c9061032.min.json").then(a=>a.json())]).then(([{index:a,links:b},c])=>({index:a,links:b,content:c}))</script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-S103D50NQ0"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-S103D50NQ0',{anonymize_ip:!1})}</script><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://carlos-gg.github.io/digitalgarden/js/graph.9e0d7f85fd8bcbb9da33fd6f8acdfeb9.js></script><script>drawGraph("https://carlos-gg.github.io/digitalgarden/AI/Supervised-Learning/Regression","https://carlos-gg.github.io/digitalgarden",[{"/moc":"#4388cc"}],-1,!0,!1,!0)</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script><script defer src=https://carlos-gg.github.io/digitalgarden/js/search.3c0dba36397221e42c2e229db4303dc2.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://carlos-gg.github.io/digitalgarden/>CarlosGG's Knowledge Garden 🪴</a></h1><svg tabindex="0" id="search-icon" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg><div class=spacer></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Regression</h1><p class=meta>Last updated March 17, 2022</p><ul class=tags></ul><aside class=mainTOC><h3>Table of Contents</h3><nav id=TableOfContents><ol><li><a href=#resources>Resources</a><ol><li><a href=#linear-regression>Linear Regression</a></li><li><a href=#least-squares>Least squares</a></li><li><a href=#principal-component-regression>Principal component regression</a></li><li><a href=#generalized-linear-models-glm>Generalized linear models (GLM)</a></li><li><a href=#logistic-regression>Logistic regression</a></li><li><a href=#regularized-regression>Regularized regression</a></li><li><a href=#svm>SVM</a></li><li><a href=#gaussian-process>Gaussian Process</a></li><li><a href=#tree-based-approaches>Tree-based approaches</a></li><li><a href=#k-nearest-neighbors>K-nearest neighbors</a></li><li><a href=#approximate-nearest-neighbors>Approximate nearest neighbors</a></li></ol></li><li><a href=#code>Code</a></li></ol></nav></aside><p>See:</p><ul><li><a href=/digitalgarden/AI/Time-Series-analysis rel=noopener class=internal-link data-src=/digitalgarden/AI/Time-Series-analysis>Time Series analysis</a></li><li><a href=/digitalgarden/AI/Deep-learning/RNNs rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/RNNs>RNNs</a></li><li>&ldquo;Sequence time series modelling&rdquo; section in
<a href=/digitalgarden/AI/Deep-learning/CNNs rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/CNNs>CNNs</a></li></ul><h2 id=resources>Resources</h2><ul><li><a href=https://www.analytics>https://www.analytics</a>. idhya.com/blog/2015/08/comprehensive-guide-regression/</li><li><a href=https://towardsdatascience.com/a-beginners-guide-to-regression-analysis-in-machine-learning-8a828b491bbf>https://towardsdatascience.com/a-beginners-guide-to-regression-analysis-in-machine-learning-8a828b491bbf</a></li><li><a href=http://www.datasciencecentral.com/profiles/blogs/10-types-of-regressions-which-one-to-use>http://www.datasciencecentral.com/profiles/blogs/10-types-of-regressions-which-one-to-use</a></li><li><a href=http://www.datasciencecentral.com/profiles/blogs/23-types-of-regression>http://www.datasciencecentral.com/profiles/blogs/23-types-of-regression</a></li><li><a href=https://blog.datazar.com/curve-fitting-vs-regression-752ce295b0b1 rel=noopener>Curve fitting vs regression</a></li><li>Goodness of fit:<ul><li><a href=http://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit rel=noopener>Coefficient of determination (The R-squared measure of goodness of fit)</a></li><li>Reduced chi-squared</li></ul></li><li><a href=http://scikit-learn.org/stable/modules/linear_model.html rel=noopener>Linear models</a></li></ul><h3 id=linear-regression>Linear Regression</h3><ul><li><a href=https://en.wikipedia.org/wiki/Linear_regression>https://en.wikipedia.org/wiki/Linear_regression</a></li><li>In statistics, linear regression is an approach for modeling the relationship between a scalar dependent variable y and one or more explanatory variables (or independent variables) denoted X. The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression.</li><li><a href=http://www.datasciencecentral.com/profiles/blogs/linear-regression-geometry>http://www.datasciencecentral.com/profiles/blogs/linear-regression-geometry</a></li><li><a href=https://github.com/jmportilla/Udemy---Machine-Learning/blob/master/Supervised%20Learning%20-%20%20Linear%20Regression.ipynb>https://github.com/jmportilla/Udemy---Machine-Learning/blob/master/Supervised%20Learning%20-%20%20Linear%20Regression.ipynb</a></li></ul><h3 id=least-squares>Least squares</h3><ul><li><a href=https://en.wikipedia.org/wiki/Least_squares>https://en.wikipedia.org/wiki/Least_squares</a></li><li>The method of least squares is a standard approach in regression analysis to the approximate solution of overdetermined systems, i.e., sets of equations in which there are more equations than unknowns. &ldquo;Least squares&rdquo; means that the overall solution minimizes the sum of the squares of the errors made in the results of every single equation. The most important application is in data fitting. The best fit in the least-squares sense minimizes the sum of squared residuals (a residual being: the difference between an observed value, and the fitted value provided by a model).</li><li><a href=https://en.wikipedia.org/wiki/Cook%27s_distance rel=noopener>Cook&rsquo;s D</a><ul><li>In statistics, Cook&rsquo;s distance or Cook&rsquo;s D is a commonly used estimate of the influence of a data point when performing a least-squares regression analysis. In a practical ordinary least squares analysis, Cook&rsquo;s distance can be used in several ways: to indicate influential data points that are particularly worth checking for validity; or to indicate regions of the design space where it would be good to be able to obtain more data points.</li><li><a href=https://www.mathworks.com/examples/statistics/mw/stats-ex80505511-determine-outliers-using-cook-s-distance>https://www.mathworks.com/examples/statistics/mw/stats-ex80505511-determine-outliers-using-cook-s-distance</a></li></ul></li><li>Ordinary least squares (OLS):<ul><li><a href=https://en.wikipedia.org/wiki/Ordinary_least_squares>https://en.wikipedia.org/wiki/Ordinary_least_squares</a></li><li><a href=http://setosa.io/ev/ordinary-least-squares-regression/>http://setosa.io/ev/ordinary-least-squares-regression/</a></li></ul></li><li>Non-linear least squares:<ul><li><a href=https://en.wikipedia.org/wiki/Non-linear_least_squares>https://en.wikipedia.org/wiki/Non-linear_least_squares</a></li><li>Non-linear least squares is the form of least squares analysis used to fit a set of m observations with a model that is non-linear in n unknown parameters (m> n).</li></ul></li></ul><h3 id=principal-component-regression>Principal component regression</h3><ul><li><a href=https://en.wikipedia.org/wiki/Principal_component_regression>https://en.wikipedia.org/wiki/Principal_component_regression</a></li></ul><h3 id=generalized-linear-models-glm>Generalized linear models (GLM)</h3><ul><li><a href=https://en.wikipedia.org/wiki/Generalized_linear_model>https://en.wikipedia.org/wiki/Generalized_linear_model</a><ul><li>GLM is a flexible generalization of ordinary linear regression that allows for the response variable to have an error distribution other than the normal distribution</li></ul></li><li><a href=https://scikit-learn.org/0.15/modules/linear_model.html rel=noopener>Generalized Linear Models</a></li><li><a href=https://towardsdatascience.com/understand-generalized-linear-models-and-how-it-relates-to-linear-logistic-and-poisson-regression-53f3aea8a9d rel=noopener>Understand Generalized Linear Models, and How It Relates to Linear, Logistic and Poisson Regression</a></li><li><a href=https://towardsdatascience.com/beyond-linear-regression-an-introduction-to-glms-7ae64a8fad9c>https://towardsdatascience.com/beyond-linear-regression-an-introduction-to-glms-7ae64a8fad9c</a><ul><li>Under the GLM assumptions, the output variable Y can now follow any probability distribution within the “exponential family”, which includes not only the exponential distribution, but also the normal, gamma, chi-squared, Poisson, binomial (for a fixed number of trails), negative binomial (for a fixed number of failures), beta and lognormal distributions, among others</li></ul></li></ul><h3 id=logistic-regression>Logistic regression</h3><p>See
<a href=/digitalgarden/AI/Supervised-Learning/Classification rel=noopener class=internal-link data-src=/digitalgarden/AI/Supervised-Learning/Classification>Classification</a></p><h3 id=regularized-regression>Regularized regression</h3><ul><li><a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>https://en.wikipedia.org/wiki/Regularization_(mathematics)</a></li><li>Regularization, in mathematics and statistics and particularly in the field of machine learning, refers to a process of introducing additional information in order to solve an ill-posed problem or to prevent overfitting.</li><li><a href=https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/>https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/</a></li><li><a href=http://www.astroml.org/book_figures/chapter8/fig_lasso_ridge.html>http://www.astroml.org/book_figures/chapter8/fig_lasso_ridge.html</a></li><li><a href=https://chaoticsenses.wordpress.com/2016/01/20/taming-the-beast-with-regularization-3/>https://chaoticsenses.wordpress.com/2016/01/20/taming-the-beast-with-regularization-3/</a></li><li><a href=https://www.cienciadedatos.net/documentos/py14-ridge-lasso-elastic-net-python.html>https://www.cienciadedatos.net/documentos/py14-ridge-lasso-elastic-net-python.html</a></li><li><a href=https://en.wikipedia.org/wiki/Tikhonov_regularization rel=noopener>Ridge regression</a></li><li><a href=https://en.wikipedia.org/wiki/Lasso_%28statistics%29 rel=noopener>LASSO (Least absolute shrinkage and selection operator)</a></li><li><a href=https://en.wikipedia.org/wiki/Least-angle_regression rel=noopener>LARS (Least angle regression)</a></li><li>Elastic net:<ul><li><a href=https://en.wikipedia.org/wiki/Elastic_net_regularization>https://en.wikipedia.org/wiki/Elastic_net_regularization</a></li><li>In the fitting linear or logistic regression models, the elastic net is a regularized regression method that linearly combines the L1 and L2 penalties of the lasso and ridge methods.</li><li><a href=http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html>http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html</a></li></ul></li></ul><h3 id=svm>SVM</h3><p>See
<a href=/digitalgarden/AI/Supervised-Learning/Classification rel=noopener class=internal-link data-src=/digitalgarden/AI/Supervised-Learning/Classification>Classification</a></p><h3 id=gaussian-process>Gaussian Process</h3><p>See
<a href=/digitalgarden/AI/Supervised-Learning/Gaussian-Process rel=noopener class=internal-link data-src=/digitalgarden/AI/Supervised-Learning/Gaussian-Process>Gaussian Process</a></p><h3 id=tree-based-approaches>Tree-based approaches</h3><p>See
<a href=/digitalgarden/AI/Supervised-Learning/Ensemble-learning rel=noopener class=internal-link data-src=/digitalgarden/AI/Supervised-Learning/Ensemble-learning>Ensemble learning</a> and
<a href=/digitalgarden/AI/Supervised-Learning/Classification rel=noopener class=internal-link data-src=/digitalgarden/AI/Supervised-Learning/Classification>Classification</a></p><h3 id=k-nearest-neighbors>K-nearest neighbors</h3><ul><li><a href=https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm>https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm</a></li><li>k-NN is a non-parametric method used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression:<ul><li>for classification - An object is classified by a majority vote of its k nearest neighbors.</li><li>for regression - The value is the average of the values of its k nearest neighbors.</li></ul></li><li>k-NN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until classification. The k-NN algorithm is among the simplest of all machine learning algorithms.</li></ul><h3 id=approximate-nearest-neighbors>Approximate nearest neighbors</h3><ul><li><a href=https://en.wikipedia.org/wiki/Nearest_neighbor_search>https://en.wikipedia.org/wiki/Nearest_neighbor_search</a></li><li>ANN benchmarks:<ul><li><a href=http://ann-benchmarks.com/>http://ann-benchmarks.com/</a></li><li><a href=https://github.com/erikbern/ann-benchmarks>https://github.com/erikbern/ann-benchmarks</a></li><li><a href=https://erikbern.com/2018/02/15/new-benchmarks-for-approximate-nearest-neighbors.html>https://erikbern.com/2018/02/15/new-benchmarks-for-approximate-nearest-neighbors.html</a></li><li><a href=https://erikbern.com/2018/06/17/new-approximate-nearest-neighbor-benchmarks.html>https://erikbern.com/2018/06/17/new-approximate-nearest-neighbor-benchmarks.html</a></li></ul></li></ul><h2 id=code>Code</h2><ul><li>#CODE
<a href=https://github.com/bbalasub1/glmnet_python rel=noopener>Glmnet_py. Glmnet Vignette (for python)</a> ^glmnetpy</li><li>#CODE Annoy (Spotify).<ul><li>Approximate Nearest Neighbors in C++/Python optimized for memory usage and loading/saving to disk</li><li><a href=https://github.com/spotify/annoy>https://github.com/spotify/annoy</a></li></ul></li><li>#CODE FALCONN - FAst Lookups of Cosine and Other Nearest Neighbors<ul><li><a href=https://github.com/FALCONN-LIB/FALCONN>https://github.com/FALCONN-LIB/FALCONN</a></li><li>Locality-Sensitive Hashing (LSH), nearest neighbor search in high-dimensional spaces</li></ul></li><li>#CODE
<a href=https://github.com/nmslib/nmslib rel=noopener>NMSLIB - Non-Metric Space Library. Efficient cross-platform similarity search library and a toolkit for evaluation of similarity search methods. The core-library does not have any third-party dependencies</a></li><li>#CODE FAISS (Facebook)<ul><li><a href=https://github.com/facebookresearch/faiss>https://github.com/facebookresearch/faiss</a></li><li>Faiss is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning. Faiss is written in C++ with complete wrappers for Python/numpy. Some of the most useful algorithms are implemented on the GPU. It is developed by Facebook AI Research.</li></ul></li></ul></article><hr><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=https://carlos-gg.github.io/digitalgarden//AI/Forecasting>Forecasting</a></li><li><a href=https://carlos-gg.github.io/digitalgarden//AI/Learning-to-rank>Learning to rank</a></li><li><a href=https://carlos-gg.github.io/digitalgarden//AI/Machine-Learning>Machine Learning</a></li><li><a href=https://carlos-gg.github.io/digitalgarden//AI/Math-and-Statistics/Math-and-Statistics>Math and Statistics</a></li><li><a href=https://carlos-gg.github.io/digitalgarden//AI/Supervised-Learning/Classification>Classification</a></li><li><a href=https://carlos-gg.github.io/digitalgarden//AI/Supervised-Learning/Feature-selection>Feature selection</a></li><li><a href=https://carlos-gg.github.io/digitalgarden//AI/Supervised-Learning/Supervised-learning>Supervised Learning</a></li></ul></div></div><div id=contact_buttons><footer><p>Made by Carlos Alberto Gomez Gonzalez using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2022</p><a href=https://carlos-gg.github.io/digitalgarden/>Root</a><a href=https://carlos-gg.github.io>CarlosGG</a></footer></div><script src=https://carlos-gg.github.io/digitalgarden/js/popover.718bae159805a590cda4e9bcb14b9dcc.min.js></script><script>initPopover("https://carlos-gg.github.io/digitalgarden")</script></div></body></html>