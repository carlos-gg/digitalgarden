<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between a dependent variable (often called the &lsquo;outcome&rsquo; or &lsquo;response&rsquo; variable) and one or more independent variables (often called &lsquo;predictors&rsquo;, &lsquo;covariates&rsquo;, &rsquo;explanatory variables&rsquo; or &lsquo;features&rsquo;)"><title>Regression</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://carlos-gg.github.io/digitalgarden//icon.png><link href=https://carlos-gg.github.io/digitalgarden/styles.b3e1e36b0403ac565c9392b3e23ef3b6.min.css rel=stylesheet><link href=https://carlos-gg.github.io/digitalgarden/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://carlos-gg.github.io/digitalgarden/js/darkmode.18b7c6dfe67ae3bf2317338cf4189144.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/util.9825137f5e7825e8553c68ce39ac9e44.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/popover.abe6a51cc7138c5dff00f151dd627ad1.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/code-title.b35124ad8db0ba37162b886afb711cbc.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/clipboard.c20857734e53a3fb733b7443879efa61.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const BASE_URL="https://carlos-gg.github.io/digitalgarden/",fetchData=Promise.all([fetch("https://carlos-gg.github.io/digitalgarden/indices/linkIndex.6e8681d4e784ed3d0cf22907404fef7b.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://carlos-gg.github.io/digitalgarden/indices/contentIndex.96eb64e63352c1d7d440fd6805942d0a.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://carlos-gg.github.io/digitalgarden",!0,!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!0;drawGraph("https://carlos-gg.github.io/digitalgarden",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/carlos-gg.github.io\/digitalgarden\/js\/router.9d4974281069e9ebb189f642ae1e3ca2.min.js"
    attachSPARouting(init, render)
  </script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-S103D50NQ0"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S103D50NQ0",{anonymize_ip:!1})}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://carlos-gg.github.io/digitalgarden/js/full-text-search.24827f874defbbc6d529926cbfcfb493.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://carlos-gg.github.io/digitalgarden/>CarlosGG's Knowledge Garden 🪴</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Regression</h1><p class=meta>Last updated
Sep 6, 2022
<a href=https://github.com/jackyzha0/quartz/tree/hugo/content/AI/Supervised%20Learning/Regression.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details open><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#resources>Resources</a><ol><li><a href=#linear-regression>Linear Regression</a></li><li><a href=#least-squares>Least squares</a></li><li><a href=#principal-component-regression>Principal component regression</a></li><li><a href=#generalized-linear-models-glm>Generalized linear models (GLM)</a></li><li><a href=#logistic-regression>Logistic regression</a></li><li><a href=#regularized-regression>Regularized regression</a></li><li><a href=#svm>SVM</a></li><li><a href=#gaussian-process>Gaussian Process</a></li><li><a href=#tree-based-approaches>Tree-based approaches</a></li><li><a href=#k-nearest-neighbors>K-nearest neighbors</a></li><li><a href=#approximate-nearest-neighbors>Approximate nearest neighbors</a></li></ol></li><li><a href=#code>Code</a></li></ol></nav></details></aside><blockquote><p>In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between a dependent variable (often called the &lsquo;outcome&rsquo; or &lsquo;response&rsquo; variable) and one or more independent variables (often called &lsquo;predictors&rsquo;, &lsquo;covariates&rsquo;, &rsquo;explanatory variables&rsquo; or &lsquo;features&rsquo;)</p></blockquote><blockquote><p>See:</p><ul><li><a href=/digitalgarden/AI/Supervised-Learning/Regularized-regression rel=noopener class=internal-link data-src=/digitalgarden/AI/Supervised-Learning/Regularized-regression>AI/Supervised Learning/Regularized regression</a></li><li><a href=/digitalgarden/AI/Time-Series-analysis rel=noopener class=internal-link data-src=/digitalgarden/AI/Time-Series-analysis>AI/Time Series analysis</a></li><li><a href=/digitalgarden/AI/Forecasting rel=noopener class=internal-link data-src=/digitalgarden/AI/Forecasting>AI/Forecasting</a></li><li><a href=/digitalgarden/AI/Deep-learning/RNNs rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/RNNs>AI/Deep learning/RNNs</a></li><li>&ldquo;Sequence time series modelling&rdquo; section in <a href=/digitalgarden/AI/Deep-learning/CNNs rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/CNNs>AI/Deep learning/CNNs</a></li></ul></blockquote><a href=#resources><h2 id=resources><span class=hanchor arialabel=Anchor># </span>Resources</h2></a><ul><li><a href=https://en.wikipedia.org/wiki/Regression_analysis rel=noopener>https://en.wikipedia.org/wiki/Regression_analysis</a></li><li><a href=https://www.analytics rel=noopener>https://www.analytics</a>. idhya.com/blog/2015/08/comprehensive-guide-regression/</li><li><a href=https://towardsdatascience.com/a-beginners-guide-to-regression-analysis-in-machine-learning-8a828b491bbf rel=noopener>https://towardsdatascience.com/a-beginners-guide-to-regression-analysis-in-machine-learning-8a828b491bbf</a></li><li><a href=http://www.datasciencecentral.com/profiles/blogs/10-types-of-regressions-which-one-to-use rel=noopener>http://www.datasciencecentral.com/profiles/blogs/10-types-of-regressions-which-one-to-use</a></li><li><a href=http://www.datasciencecentral.com/profiles/blogs/23-types-of-regression rel=noopener>http://www.datasciencecentral.com/profiles/blogs/23-types-of-regression</a></li><li><a href=https://blog.datazar.com/curve-fitting-vs-regression-752ce295b0b1 rel=noopener>Curve fitting vs regression</a></li><li>Goodness of fit:<ul><li><a href=http://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit rel=noopener>Coefficient of determination (The R-squared measure of goodness of fit)</a></li><li>Reduced chi-squared</li></ul></li><li><a href=http://scikit-learn.org/stable/modules/linear_model.html rel=noopener>Linear models</a></li><li>Multicollinearity:<ul><li><a href=https://waterprogramming.wordpress.com/2017/02/22/dealing-with-multicollinearity-a-brief-overview-and-introduction-to-tolerant-methods/ rel=noopener>Dealing With Multicollinearity: A Brief Overview and Introduction to Tolerant Methods</a></li><li><a href=https://stackoverflow.com/questions/42904211/lasso-or-ridge-for-correlated-variables rel=noopener>https://stackoverflow.com/questions/42904211/lasso-or-ridge-for-correlated-variables</a></li><li><a href=https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html rel=noopener>Permutation Importance with Multicollinear or Correlated Features (scikit-learn)</a>
1</li></ul></li></ul><a href=#linear-regression><h3 id=linear-regression><span class=hanchor arialabel=Anchor># </span>Linear Regression</h3></a><ul><li><a href=https://en.wikipedia.org/wiki/Linear_regression rel=noopener>https://en.wikipedia.org/wiki/Linear_regression</a></li><li>In statistics, linear regression is an approach for modeling the relationship between a scalar dependent variable y and one or more explanatory variables (or independent variables) denoted X. The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression.</li><li><a href=http://www.datasciencecentral.com/profiles/blogs/linear-regression-geometry rel=noopener>http://www.datasciencecentral.com/profiles/blogs/linear-regression-geometry</a></li><li><a href=https://github.com/jmportilla/Udemy---Machine-Learning/blob/master/Supervised-Learning----Linear-Regression.ipynb rel=noopener>https://github.com/jmportilla/Udemy---Machine-Learning/blob/master/Supervised%20Learning%20-%20%20Linear%20Regression.ipynb</a></li></ul><a href=#least-squares><h3 id=least-squares><span class=hanchor arialabel=Anchor># </span>Least squares</h3></a><ul><li><a href=https://en.wikipedia.org/wiki/Least_squares rel=noopener>https://en.wikipedia.org/wiki/Least_squares</a></li><li>The method of least squares is a standard approach in regression analysis to the approximate solution of overdetermined systems, i.e., sets of equations in which there are more equations than unknowns. &ldquo;Least squares&rdquo; means that the overall solution minimizes the sum of the squares of the errors made in the results of every single equation. The most important application is in data fitting. The best fit in the least-squares sense minimizes the sum of squared residuals (a residual being: the difference between an observed value, and the fitted value provided by a model).</li><li><a href=https://en.wikipedia.org/wiki/Cook%27s_distance rel=noopener>Cook&rsquo;s D</a><ul><li>In statistics, Cook&rsquo;s distance or Cook&rsquo;s D is a commonly used estimate of the influence of a data point when performing a least-squares regression analysis. In a practical ordinary least squares analysis, Cook&rsquo;s distance can be used in several ways: to indicate influential data points that are particularly worth checking for validity; or to indicate regions of the design space where it would be good to be able to obtain more data points.</li><li><a href=https://www.mathworks.com/examples/statistics/mw/stats-ex80505511-determine-outliers-using-cook-s-distance rel=noopener>https://www.mathworks.com/examples/statistics/mw/stats-ex80505511-determine-outliers-using-cook-s-distance</a></li></ul></li><li>Ordinary least squares (OLS):<ul><li><a href=https://en.wikipedia.org/wiki/Ordinary_least_squares rel=noopener>https://en.wikipedia.org/wiki/Ordinary_least_squares</a></li><li><a href=http://setosa.io/ev/ordinary-least-squares-regression/ rel=noopener>http://setosa.io/ev/ordinary-least-squares-regression/</a></li></ul></li><li>Non-linear least squares:<ul><li><a href=https://en.wikipedia.org/wiki/Non-linear_least_squares rel=noopener>https://en.wikipedia.org/wiki/Non-linear_least_squares</a></li><li>Non-linear least squares is the form of least squares analysis used to fit a set of m observations with a model that is non-linear in n unknown parameters (m> n).</li></ul></li></ul><a href=#principal-component-regression><h3 id=principal-component-regression><span class=hanchor arialabel=Anchor># </span>Principal component regression</h3></a><ul><li><a href=https://en.wikipedia.org/wiki/Principal_component_regression rel=noopener>https://en.wikipedia.org/wiki/Principal_component_regression</a></li></ul><a href=#generalized-linear-models-glm><h3 id=generalized-linear-models-glm><span class=hanchor arialabel=Anchor># </span>Generalized linear models (GLM)</h3></a><ul><li><a href=https://en.wikipedia.org/wiki/Generalized_linear_model rel=noopener>https://en.wikipedia.org/wiki/Generalized_linear_model</a><ul><li>GLM is a flexible generalization of ordinary linear regression that allows for the response variable to have an error distribution other than the normal distribution</li></ul></li><li><a href=https://scikit-learn.org/0.15/modules/linear_model.html rel=noopener>Generalized Linear Models</a></li><li><a href=https://towardsdatascience.com/understand-generalized-linear-models-and-how-it-relates-to-linear-logistic-and-poisson-regression-53f3aea8a9d rel=noopener>Understand Generalized Linear Models, and How It Relates to Linear, Logistic and Poisson Regression</a></li><li><a href=https://towardsdatascience.com/beyond-linear-regression-an-introduction-to-glms-7ae64a8fad9c rel=noopener>https://towardsdatascience.com/beyond-linear-regression-an-introduction-to-glms-7ae64a8fad9c</a><ul><li>Under the GLM assumptions, the output variable Y can now follow any probability distribution within the “exponential family”, which includes not only the exponential distribution, but also the normal, gamma, chi-squared, Poisson, binomial (for a fixed number of trails), negative binomial (for a fixed number of failures), beta and lognormal distributions, among others</li></ul></li></ul><a href=#logistic-regression><h3 id=logistic-regression><span class=hanchor arialabel=Anchor># </span>Logistic regression</h3></a><p>See <a href=/digitalgarden/AI/Supervised-Learning/Classification rel=noopener class=internal-link data-src=/digitalgarden/AI/Supervised-Learning/Classification>AI/Supervised Learning/Classification</a></p><a href=#regularized-regression><h3 id=regularized-regression><span class=hanchor arialabel=Anchor># </span>Regularized regression</h3></a><p>See <a href=/digitalgarden/AI/Supervised-Learning/Regularized-regression rel=noopener class=internal-link data-src=/digitalgarden/AI/Supervised-Learning/Regularized-regression>AI/Supervised Learning/Regularized regression</a></p><a href=#svm><h3 id=svm><span class=hanchor arialabel=Anchor># </span>SVM</h3></a><p>See <a href=/digitalgarden/AI/Supervised-Learning/Classification rel=noopener class=internal-link data-src=/digitalgarden/AI/Supervised-Learning/Classification>AI/Supervised Learning/Classification</a></p><a href=#gaussian-process><h3 id=gaussian-process><span class=hanchor arialabel=Anchor># </span>Gaussian Process</h3></a><p>See <a href=/digitalgarden/AI/Supervised-Learning/Gaussian-Process rel=noopener class=internal-link data-src=/digitalgarden/AI/Supervised-Learning/Gaussian-Process>AI/Supervised Learning/Gaussian Process</a></p><a href=#tree-based-approaches><h3 id=tree-based-approaches><span class=hanchor arialabel=Anchor># </span>Tree-based approaches</h3></a><p>See <a href=/digitalgarden/AI/Supervised-Learning/Ensemble-learning rel=noopener class=internal-link data-src=/digitalgarden/AI/Supervised-Learning/Ensemble-learning>AI/Supervised Learning/Ensemble learning</a> and <a href=/digitalgarden/AI/Supervised-Learning/Classification rel=noopener class=internal-link data-src=/digitalgarden/AI/Supervised-Learning/Classification>AI/Supervised Learning/Classification</a></p><a href=#k-nearest-neighbors><h3 id=k-nearest-neighbors><span class=hanchor arialabel=Anchor># </span>K-nearest neighbors</h3></a><ul><li><a href=https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm rel=noopener>https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm</a></li><li>k-NN is a non-parametric method used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression:<ul><li>for classification - An object is classified by a majority vote of its k nearest neighbors.</li><li>for regression - The value is the average of the values of its k nearest neighbors.</li></ul></li><li>k-NN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until classification. The k-NN algorithm is among the simplest of all machine learning algorithms.</li></ul><a href=#approximate-nearest-neighbors><h3 id=approximate-nearest-neighbors><span class=hanchor arialabel=Anchor># </span>Approximate nearest neighbors</h3></a><ul><li><a href=https://en.wikipedia.org/wiki/Nearest_neighbor_search rel=noopener>https://en.wikipedia.org/wiki/Nearest_neighbor_search</a></li><li>ANN benchmarks:<ul><li><a href=http://ann-benchmarks.com/ rel=noopener>http://ann-benchmarks.com/</a></li><li><a href=https://github.com/erikbern/ann-benchmarks rel=noopener>https://github.com/erikbern/ann-benchmarks</a></li><li><a href=https://erikbern.com/2018/02/15/new-benchmarks-for-approximate-nearest-neighbors.html rel=noopener>https://erikbern.com/2018/02/15/new-benchmarks-for-approximate-nearest-neighbors.html</a></li><li><a href=https://erikbern.com/2018/06/17/new-approximate-nearest-neighbor-benchmarks.html rel=noopener>https://erikbern.com/2018/06/17/new-approximate-nearest-neighbor-benchmarks.html</a></li></ul></li></ul><a href=#code><h2 id=code><span class=hanchor arialabel=Anchor># </span>Code</h2></a><ul><li>#CODE
<a href=https://github.com/spotify/annoy rel=noopener>Annoy</a> (Spotify)<ul><li>Approximate Nearest Neighbors in C++/Python optimized for memory usage and loading/saving to disk</li></ul></li><li>#CODE
<a href=https://github.com/FALCONN-LIB/FALCONN rel=noopener>FALCONN</a> - FAst Lookups of Cosine and Other Nearest Neighbors<ul><li>Locality-Sensitive Hashing (LSH), nearest neighbor search in high-dimensional spaces</li></ul></li><li>#CODE
<a href=https://github.com/nmslib/nmslib rel=noopener>NMSLIB</a><ul><li>Non-Metric Space Library. Efficient cross-platform similarity search library and a toolkit for evaluation of similarity search methods. The core-library does not have any third-party dependencies</li></ul></li><li>#CODE
<a href=https://github.com/facebookresearch/faiss rel=noopener>FAISS (Facebook)</a><ul><li>Faiss is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning. Faiss is written in C++ with complete wrappers for Python/numpy. Some of the most useful algorithms are implemented on the GPU. It is developed by Facebook AI Research.</li></ul></li></ul></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/digitalgarden/AI/Forecasting/ data-ctx="AI/Supervised Learning/Regression" data-src=/AI/Forecasting class=internal-link>Forecasting</a></li><li><a href=/digitalgarden/AI/Learning-to-rank/ data-ctx="AI/Supervised Learning/Regression" data-src=/AI/Learning-to-rank class=internal-link>Learning to rank</a></li><li><a href=/digitalgarden/AI/Machine-Learning/ data-ctx="AI/Supervised Learning/Regression" data-src=/AI/Machine-Learning class=internal-link>Machine Learning (ML)</a></li><li><a href=/digitalgarden/AI/Math-and-Statistics/Math-and-Statistics/ data-ctx="AI/Supervised Learning/Regression" data-src=/AI/Math-and-Statistics/Math-and-Statistics class=internal-link>Math and Statistics</a></li><li><a href=/digitalgarden/AI/Supervised-Learning/Classification/ data-ctx="AI/Supervised Learning/Regression" data-src=/AI/Supervised-Learning/Classification class=internal-link>Classification</a></li><li><a href=/digitalgarden/AI/Supervised-Learning/Supervised-learning/ data-ctx="AI/Supervised Learning/Regression" data-src=/AI/Supervised-Learning/Supervised-learning class=internal-link>Supervised Learning</a></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://carlos-gg.github.io/digitalgarden/js/graph.abd4bc2af3869a96524d7d23b76152c7.js></script></div></div><div id=contact_buttons><footer><p>Made by Carlos Alberto Gomez Gonzalez using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2022</p><ul><li><a href=https://carlos-gg.github.io/digitalgarden/>Home</a></li><li><a href=https://carlos-gg.github.io>Carlos'Homepage</a></li></ul></footer></div></div></body></html>