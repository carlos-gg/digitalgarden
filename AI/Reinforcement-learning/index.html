<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Reinforcement learning (RL) is an area of [[AI/Machine Learning]] concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward"><title>Reinforcement learning (RL)</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://carlos-gg.github.io/digitalgarden//icon.png><link href=https://carlos-gg.github.io/digitalgarden/styles.b3e1e36b0403ac565c9392b3e23ef3b6.min.css rel=stylesheet><link href=https://carlos-gg.github.io/digitalgarden/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://carlos-gg.github.io/digitalgarden/js/darkmode.18b7c6dfe67ae3bf2317338cf4189144.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/util.9825137f5e7825e8553c68ce39ac9e44.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/popover.abe6a51cc7138c5dff00f151dd627ad1.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/code-title.b35124ad8db0ba37162b886afb711cbc.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/clipboard.c20857734e53a3fb733b7443879efa61.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const BASE_URL="https://carlos-gg.github.io/digitalgarden/",fetchData=Promise.all([fetch("https://carlos-gg.github.io/digitalgarden/indices/linkIndex.9fca9664cf4043a6eb06c464510db0ca.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://carlos-gg.github.io/digitalgarden/indices/contentIndex.24200ff9e65f73b898452a82159162ba.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://carlos-gg.github.io/digitalgarden",!0,!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!0;drawGraph("https://carlos-gg.github.io/digitalgarden",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/carlos-gg.github.io\/digitalgarden\/js\/router.9d4974281069e9ebb189f642ae1e3ca2.min.js"
    attachSPARouting(init, render)
  </script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-S103D50NQ0"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S103D50NQ0",{anonymize_ip:!1})}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://carlos-gg.github.io/digitalgarden/js/full-text-search.24827f874defbbc6d529926cbfcfb493.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://carlos-gg.github.io/digitalgarden/>CarlosGG's Knowledge Garden ü™¥</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Reinforcement learning (RL)</h1><p class=meta>Last updated
Sep 5, 2022
<a href=https://github.com/jackyzha0/quartz/tree/hugo/content/AI/Reinforcement%20learning.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details open><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#resources>Resources</a></li><li><a href=#courses-talks-and-books>Courses, talks and books</a></li><li><a href=#code>Code</a></li><li><a href=#references>References</a></li></ol></nav></details></aside><blockquote><p>Reinforcement learning (RL) is an area of <a href=/digitalgarden/AI/Machine-Learning rel=noopener class=internal-link data-src=/digitalgarden/AI/Machine-Learning>AI/Machine Learning</a> concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward</p></blockquote><a href=#resources><h2 id=resources><span class=hanchor arialabel=Anchor># </span>Resources</h2></a><ul><li><a href=https://en.wikipedia.org/wiki/Reinforcement_learning rel=noopener>https://en.wikipedia.org/wiki/Reinforcement_learning</a></li><li>Reinforcement learning is the task of learning what actions to take, given a certain situation/environment, so as to maximize a reward signal. The interesting difference between supervised and reinforcement learning is that this reward signal simply tells you whether the action (or input) that the agent takes is good or bad. It doesn‚Äôt tell you anything about what the best action is. Contrast this to CNNs where the corresponding label for each image input is a definite instruction of what the output should be for each input. Another unique component of RL is that an agent‚Äôs actions will affect the subsequent data it receives. For example, an agent‚Äôs action of moving left instead of right means that the agent will receive different input from the environment at the next time step.</li><li><a href=https://lilianweng.github.io/lil-log/2020/01/29/curriculum-for-reinforcement-learning.html rel=noopener>Curriculum for Reinforcement Learning</a></li><li><a href=http://karpathy.github.io/2016/05/31/rl/ rel=noopener>Andrej Karpathy&rsquo;s introduction to RL</a></li><li><a href=https://spinningup.openai.com/en/latest/spinningup/spinningup.html rel=noopener>Spinning Up as a Deep RL Researcher</a></li><li><a href=https://blog.openai.com/evolution-strategies/ rel=noopener>Evolution strategies vs RL</a><ul><li><a href=https://github.com/openai/evolution-strategies-starter rel=noopener>https://github.com/openai/evolution-strategies-starter</a></li></ul></li><li><a href=http://www.alexirpan.com/rl-derivations/ rel=noopener>Reinforcement learning derivations (math)</a></li><li><a href=https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287 rel=noopener>Introduction to various RL algos</a></li><li><a href=https://en.wikipedia.org/wiki/Q-learning rel=noopener>Q-learning</a></li><li>Temporal differencing (TD) learning is a prediction-based machine learning method.<ul><li>It has primarily been used for the reinforcement learning problem, and is said to be &ldquo;a combination ofMonte Carlo ideas and dynamic programming (DP) ideas.&rdquo;</li><li>TD resembles a Monte Carlo method because it learns by sampling the environment according to some policy, and is related to dynamic programming techniques as it approximates its current estimate based on previously learned estimates (a process known as bootstrapping). The TD learning algorithm is related to the temporal difference model of animal learning. As a prediction method, TD learning considers that subsequent predictions are often correlated in some sense.</li><li>TD-Lambda: This algorithm was famously applied by Gerald Tesauro to createTD-Gammon, a program that learned to play the game of backgammon at the level of expert human players. The lambda parameter refers to the trace decay parameter, with 0&lt;= lambda &lt;=1. Higher settings lead to longer lasting traces; that is, a larger proportion of credit from a reward can be given to more distant states and actions when lambda is higher, with lambda=1 producing parallel learning to Monte Carlo RL algorithms.</li></ul></li><li><a href=https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action rel=noopener>SARSA</a></li></ul><a href=#courses-talks-and-books><h2 id=courses-talks-and-books><span class=hanchor arialabel=Anchor># </span>Courses, talks and books</h2></a><ul><li>#COURSE
<a href=https://www.davidsilver.uk/teaching/ rel=noopener>Reinforcement Learning (UCL)</a><ul><li><a href="https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-" rel=noopener>Videos</a></li></ul></li><li>#COURSE
<a href="https://www.youtube.com/playlist?list=PLkFD6_40KJIwTmSbCv9OVJB3YaO4sFwkX" rel=noopener>CS294-112 Deep Reinforcement Learning Sp17</a></li><li>#COURSE
<a href=https://github.com/yandexdataschool/Practical_RL rel=noopener>Practical Reinforcement Learning (Yandex)</a></li><li>#COURSE
<a href=https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/tutorials/W3D2_BasicReinforcementLearning/student/W3D2_Tutorial1.ipynb rel=noopener>Tutorial: Introduction to Reinforcement Learning</a></li><li>#TALK
<a href=http://videolectures.net/DLRLsummerschool2018_toronto/ rel=noopener>Deep Learning and Reinforcement Learning Summer School, Toronto 2018</a></li><li>#TALK
<a href=https://sites.google.com/view/deep-rl-bootcamp/lectures rel=noopener>Deep RL Bootcamp</a></li><li>#BOOK
<a href=https://www.springer.com/gp/book/9789811540943 rel=noopener>Deep Reinforcement Learning (2020 SPRINGER)</a><ul><li><a href=https://deepreinforcementlearningbook.org/ rel=noopener>https://deepreinforcementlearningbook.org/</a></li></ul></li></ul><a href=#code><h2 id=code><span class=hanchor arialabel=Anchor># </span>Code</h2></a><ul><li>#CODE
<a href=https://github.com/deepmind/acme rel=noopener>Acme: a research framework for reinforcement learning</a></li><li>#CODE
<a href=https://github.com/tensorlayer/tensorlayer/tree/master/examples/reinforcement_learning rel=noopener>Deep Reinforcement Learning Model ZOO</a></li><li>#CODE
<a href=https://github.com/openai/gym rel=noopener>Open.ai Gym - A toolkit for developing and comparing reinforcement learning algorithms</a><ul><li><a href=https://gym.openai.com/ rel=noopener>https://gym.openai.com/</a></li><li>#PAPER
<a href=http://arxiv.org/abs/1606.01540 rel=noopener>http://arxiv.org/abs/1606.01540</a></li></ul></li><li>#CODE
<a href=https://github.com/facebookresearch/Horizon rel=noopener>Horizon (Facebook) - The first open source reinforcement learning platform for large-scale products and services</a></li><li>#CODE
<a href=https://github.com/keras-rl/keras-rl rel=noopener>Keras-rl - Deep Reinforcement Learning for Keras</a></li><li>#CODE
<a href=https://github.com/deepmind/trfl/ rel=noopener>TRFL (pronounced &ldquo;truffle&rdquo;) is a library built on top of TensorFlow that exposes several useful building blocks for implementing Reinforcement Learning agents</a></li><li>#CODE
<a href=https://github.com/SurrealAI/surreal rel=noopener>Surreal - Open-Source Distributed Reinforcement Learning Framework by Stanford Vision and Learning Lab</a><ul><li><a href=https://surreal.stanford.edu rel=noopener>https://surreal.stanford.edu</a></li></ul></li><li>#CODE Tensorforce - Tensorforce is an open-source deep reinforcement learning framework, with an emphasis on modularized flexible library design and straightforward usability for applications in research and practice.<ul><li><a href=https://github.com/tensorforce/tensorforce rel=noopener>https://github.com/tensorforce/tensorforce</a></li><li><a href=https://reinforce.io/blog/introduction-to-tensorforce/ rel=noopener>https://reinforce.io/blog/introduction-to-tensorforce/</a></li></ul></li><li>#CODE
<a href=https://github.com/tensorlayer/tensorlayer rel=noopener>Tensorlayer</a><ul><li>Deep Learning and Reinforcement Learning Library for Scientists and Engineers</li><li><a href=https://tensorlayer.readthedocs.io/en/latest/index.html rel=noopener>https://tensorlayer.readthedocs.io/en/latest/index.html</a></li></ul></li></ul><a href=#references><h2 id=references><span class=hanchor arialabel=Anchor># </span>References</h2></a><ul><li>#PAPER
<a href=https://deepmind.com/research/dqn/ rel=noopener>DQN: Human-level control through Deep Reinforcement Learning (Mnih 2015)</a><ul><li><a href=https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf rel=noopener>https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1606.01885 rel=noopener>Learning to Optimize (Li 2016)</a><ul><li><a href=https://bair.berkeley.edu/blog/2017/09/12/learning-to-optimize-with-rl/ rel=noopener>Learning to Optimize with Reinforcement Learning</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1507.06527 rel=noopener>Deep Recurrent Q-Learning for Partially Observable MDPs (Hausknecht 2017)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1703.01988 rel=noopener>Neural Episodic Control (Pritzel 2017)</a><ul><li>Deep reinforcement learning methods attain super-human performance in a wide range of environments. Such methods are grossly inefficient, often taking orders of magnitudes more data than humans to achieve reasonable performance</li><li>Neural Episodic Control: a deep reinforcement learning agent that is able to rapidly assimilate new experiences and act upon them.</li><li>The agent uses a semi-tabular representation of the value function: a buffer of past experience containing slowly changing state representations and rapidly updated estimates of the value function</li><li><a href=https://www.technologyreview.es/s/6656/olvidese-del-aprendizaje-profundo-el-nuevo-enfoque-de-google-funciona-mucho-mejor rel=noopener>https://www.technologyreview.es/s/6656/olvidese-del-aprendizaje-profundo-el-nuevo-enfoque-de-google-funciona-mucho-mejor</a></li><li><a href=https://rylanschaeffer.github.io/content/research/neural_episodic_control/main.html rel=noopener>Explanation of Neural Episodic Control</a></li></ul></li><li>#PAPER #REVIEW
<a href=https://arxiv.org/abs/1708.05866 rel=noopener>A Brief Survey of Deep Reinforcement Learning (Arulkumaran 2017)</a><ul><li>Many of the successes in DRL have been based on scaling up prior work in RL to high-dimensional problems. This is due to the learning of low-dimensional feature representations and the powerful function approximation properties of neural networks. By means of representation learning, DRL can deal efficiently with the curse of dimensionality, unlike tabular and traditional non-parametric methods.</li><li><a href=https://adeshpande3.github.io/adeshpande3.github.io/Deep-Learning-Research-Review-Week-2-Reinforcement-Learning rel=noopener>https://adeshpande3.github.io/adeshpande3.github.io/Deep-Learning-Research-Review-Week-2-Reinforcement-Learning</a></li></ul></li><li>#PAPER #REVIEW
<a href=https://arxiv.org/abs/1811.12560 rel=noopener>An Introduction to Deep Reinforcement Learning (Fancois-Lavet 2018)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1810.08575 rel=noopener>Supervising strong learners by amplifying weak experts (Christiano 2018)</a><ul><li><a href=https://blog.openai.com/amplifying-ai-training/ rel=noopener>Learning Complex Goals with Iterated Amplification</a></li></ul></li><li>#PAPER
<a href=https://deepmind.com/research/publications/MasterinModel rel=noopener>MuZero - Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model (Schrittwieser 2019)</a><ul><li><a href=https://medium.com/dataseries/deepminds-muzero-is-one-of-the-most-important-deep-learning-systems-ever-created-347442a6793g-Atari-Go-Chess-and-Shogi-by-Planning-with-a-Learned- rel=noopener>https://medium.com/dataseries/deepminds-muzero-is-one-of-the-most-important-deep-learning-systems-ever-created-347442a6793g-Atari-Go-Chess-and-Shogi-by-Planning-with-a-Learned-</a></li></ul></li><li>#PAPER #REVIEW
<a href=https://arxiv.org/abs/2005.01643 rel=noopener>Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems (Levine 2020)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/2106.01345v1 rel=noopener>Decision Transformer: Reinforcement Learning via Sequence Modeling (Chen 2021)</a> ^decisiontransformer<ul><li>#CODE
<a href=https://paperswithcode.com/paper/decision-transformer-reinforcement-learning rel=noopener>https://paperswithcode.com/paper/decision-transformer-reinforcement-learning</a></li><li><a href="https://www.youtube.com/watch?v=-buULmf7dec" rel=noopener>Paper explained</a></li></ul></li><li>#PAPER
<a href=https://www.sciencedirect.com/science/article/pii/S0004370221000862 rel=noopener>Reward is enough (Silver 2021)</a><ul><li><a href=https://towardsdatascience.com/reward-is-enough-ml-paper-review-e448ee0a6092 rel=noopener>https://towardsdatascience.com/reward-is-enough-ml-paper-review-e448ee0a6092</a></li><li>From the authors of ‚ÄúAttention is all you need‚Äù, this paper proposes an intriguing hypothesis that incentivizing AI agents with reward is enough to achieve General Artificial Intelligence</li><li>&ldquo;General intelligence, of the sort possessed by humans and perhaps also other animals, may be defined as the ability to flexibly achieve a variety of goals in different contexts. According to our hypothesis, general intelligence can instead be understood as, and implemented by, maximising a singular reward in a single, complex environment4&rdquo;</li></ul></li></ul></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/digitalgarden/AI/Deep-learning/Transformers/ data-ctx="AI/Reinforcement learning" data-src=/AI/Deep-learning/Transformers class=internal-link>Transformers</a></li><li><a href=/digitalgarden/AI/Machine-Learning/ data-ctx="AI/Reinforcement learning" data-src=/AI/Machine-Learning class=internal-link>Machine Learning (ML)</a></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://carlos-gg.github.io/digitalgarden/js/graph.abd4bc2af3869a96524d7d23b76152c7.js></script></div></div><div id=contact_buttons><footer><p>Made by Carlos Alberto Gomez Gonzalez using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, ¬© 2022</p><ul><li><a href=https://carlos-gg.github.io/digitalgarden/>Home</a></li><li><a href=https://carlos-gg.github.io>Carlos'Homepage</a></li></ul></footer></div></div></body></html>