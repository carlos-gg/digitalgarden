<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Explainable AI (XAI), or Interpretable AI, is artificial intelligence (AI) in which the results of the solution can be understood by humans"><title>XAI</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://carlos-gg.github.io/digitalgarden//icon.png><link href=https://carlos-gg.github.io/digitalgarden/styles.b3e1e36b0403ac565c9392b3e23ef3b6.min.css rel=stylesheet><link href=https://carlos-gg.github.io/digitalgarden/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://carlos-gg.github.io/digitalgarden/js/darkmode.18b7c6dfe67ae3bf2317338cf4189144.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/util.9825137f5e7825e8553c68ce39ac9e44.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/popover.abe6a51cc7138c5dff00f151dd627ad1.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/code-title.b35124ad8db0ba37162b886afb711cbc.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/clipboard.c20857734e53a3fb733b7443879efa61.min.js></script>
<script src=https://carlos-gg.github.io/digitalgarden/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const BASE_URL="https://carlos-gg.github.io/digitalgarden/",fetchData=Promise.all([fetch("https://carlos-gg.github.io/digitalgarden/indices/linkIndex.0b9ba37b3815f025016c0e5f1fba0cf7.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://carlos-gg.github.io/digitalgarden/indices/contentIndex.91f2ce950593b1ee3e6b28f0d95812fd.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://carlos-gg.github.io/digitalgarden",!0,!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!0;drawGraph("https://carlos-gg.github.io/digitalgarden",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/carlos-gg.github.io\/digitalgarden\/js\/router.9d4974281069e9ebb189f642ae1e3ca2.min.js"
    attachSPARouting(init, render)
  </script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-S103D50NQ0"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S103D50NQ0",{anonymize_ip:!1})}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://carlos-gg.github.io/digitalgarden/js/full-text-search.24827f874defbbc6d529926cbfcfb493.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://carlos-gg.github.io/digitalgarden/>CarlosGG's Knowledge Garden 🪴</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>XAI</h1><p class=meta>Last updated
Mar 22, 2023
<a href=https://github.com/jackyzha0/quartz/tree/hugo/content/AI/XAI.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details open><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#resources>Resources</a></li><li><a href=#events-talks>Events, talks</a></li><li><a href=#books>Books</a></li><li><a href=#code>Code</a></li><li><a href=#references>References</a><ol><li><a href=#model-agnostic-methods>Model-agnostic methods</a><ol><li><a href=#partial-dependence-plot>Partial Dependence Plot</a></li><li><a href=#individual-conditional-expectation>Individual Conditional Expectation</a></li><li><a href=#permutation-feature-importance>Permutation Feature Importance</a></li><li><a href=#surrogate-models>Surrogate models</a></li></ol></li><li><a href=#maximum-activation-analysis>Maximum activation analysis</a></li><li><a href=#sensitivity-analysis>Sensitivity analysis</a></li><li><a href=#variable-importance-measures>Variable importance measures</a></li><li><a href=#explainability-methods-for-neural-networks>Explainability methods for Neural Networks</a></li></ol></li></ol></nav></details></aside><blockquote><p>Explainable AI (XAI), or Interpretable AI, is artificial intelligence (AI) in which the results of the solution can be understood by humans</p></blockquote><a href=#resources><h2 id=resources><span class=hanchor arialabel=Anchor># </span>Resources</h2></a><ul><li><a href=https://en.wikipedia.org/wiki/Explainable_artificial_intelligence rel=noopener>https://en.wikipedia.org/wiki/Explainable_artificial_intelligence</a></li><li><a href=https://github.com/anguyen8/XAI-papers rel=noopener>https://github.com/anguyen8/XAI-papers</a></li><li><a href=https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning rel=noopener>Ideas on interpreting machine learning</a></li><li><a href=https://lrpserver.hhi.fraunhofer.de/ rel=noopener>Explainable AI demos</a></li><li><a href=https://medium.com/james-blogs/why-you-need-to-care-about-explainable-machine-learning-d01196a6af76 rel=noopener>Why you need to care about Explainable Machine Learning</a></li><li><a href=https://towardsdatascience.com/interpretability-in-machine-learning-70c30694a05f rel=noopener>Interpreting machine learning models</a></li><li><a href=https://www.i-am.ai/ rel=noopener>I.am.ai. Explaining artificial intelligence</a></li><li><a href=https://cloud.google.com/blog/topics/developers-practitioners/baking-recipes-made-ai rel=noopener>Baking recipes made by AI</a></li><li><a href=https://www.nature.com/articles/d41586-022-00858-1 rel=noopener>Breaking into the black box of artificial intelligence</a></li><li>A Review of Different Interpretation Methods:<ul><li><a href=https://mrsalehi.medium.com/a-review-of-different-interpretation-methods-in-deep-learning-part-1-saliency-map-cam-grad-cam-3a34476bc24d rel=noopener>https://mrsalehi.medium.com/a-review-of-different-interpretation-methods-in-deep-learning-part-1-saliency-map-cam-grad-cam-3a34476bc24d</a></li><li><a href=https://mrsalehi.medium.com/a-review-of-different-interpretation-methods-in-deep-learning-part-2-input-gradient-layerwise-e077609b6377 rel=noopener>https://mrsalehi.medium.com/a-review-of-different-interpretation-methods-in-deep-learning-part-2-input-gradient-layerwise-e077609b6377</a></li><li><a href=https://mrsalehi.medium.com/a-review-of-different-interpretation-methods-in-deep-learning-part-3-shap-integrated-gradients-918fc9fedd9b rel=noopener>https://mrsalehi.medium.com/a-review-of-different-interpretation-methods-in-deep-learning-part-3-shap-integrated-gradients-918fc9fedd9b</a></li></ul></li></ul><a href=#events-talks><h2 id=events-talks><span class=hanchor arialabel=Anchor># </span>Events, talks</h2></a><ul><li><a href=http://visxai.io/ rel=noopener>Workshop on Visualization for AI Explainability</a></li><li><a href=https://facctconference.org/ rel=noopener>ACM Conference on Fairness, Accountability, and Transparency</a></li><li><a href=https://human-centered.ai/explainable-ai-2020/ rel=noopener>Explainable AI xAI 2020</a></li><li>#TALK
<a href=https://yochan-lab.github.io/tutorial/AAAI-2020/ rel=noopener>Synthesizing Explainable and Deceptive Behavior for Human-AI Interaction (AAAI 2020 Tutorial)</a><ul><li><a href="https://www.youtube.com/watch?v=r6KhJ3ORYnc" rel=noopener>https://www.youtube.com/watch?v=r6KhJ3ORYnc</a></li></ul></li><li>#TALK
<a href=https://sites.google.com/view/explainable-ai-tutorial rel=noopener>Explainable AI in Industry (Tutorial)</a><ul><li><a href="https://www.youtube.com/watch?list=PLewjn-vrZ7d3x0M4Uu_57oaJPRXkiS221&v=rcUw7PXHWF4" rel=noopener>https://www.youtube.com/watch?list=PLewjn-vrZ7d3x0M4Uu_57oaJPRXkiS221&v=rcUw7PXHWF4</a></li></ul></li><li>#TALK
<a href=https://xaitutorial2020.github.io/ rel=noopener>Explainable AI: Foundations, Industrial Applications, Practical Challenges, and Lessons Learned (AAAI 2020)</a><ul><li><a href=https://xaitutorial2020.github.io/raw/master/slides/aaai_2020_xai_tutorial.pdf rel=noopener>https://xaitutorial2020.github.io/raw/master/slides/aaai_2020_xai_tutorial.pdf</a></li></ul></li></ul><a href=#books><h2 id=books><span class=hanchor arialabel=Anchor># </span>Books</h2></a><ul><li>#BOOK
<a href=https://christophm.github.io/interpretable-ml-book/ rel=noopener>Interpretable Machine Learning (Molnar 2021)</a></li></ul><a href=#code><h2 id=code><span class=hanchor arialabel=Anchor># </span>Code</h2></a><p>See &ldquo;Code&rdquo; section in <a href=/digitalgarden/AI/Deep-learning/Explainability-methods-for-NNs rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Explainability-methods-for-NNs>AI/Deep learning/Explainability methods for NNs</a></p><ul><li><p><a href=https://towardsdatascience.com/explainable-ai-xai-a-guide-to-7-packages-in-python-to-explain-your-models-932967f0634b rel=noopener>https://towardsdatascience.com/explainable-ai-xai-a-guide-to-7-packages-in-python-to-explain-your-models-932967f0634b</a></p></li><li><p>#CODE
<a href=https://github.com/interpretml/interpret rel=noopener>InterpretML</a> - Microsoft open-source package that incorporates state-of-the-art machine learning interpretability techniques under one roof</p><ul><li><a href=https://interpret.ml/docs/intro.html rel=noopener>https://interpret.ml/docs/intro.html</a></li></ul></li><li><p>#CODE
<a href=https://github.com/MAIF/shapash rel=noopener>Shapash</a></p><ul><li>Shapash is a Python library which aims to make machine learning interpretable and understandable by everyone. It provides several types of visualization that display explicit labels that everyone can understand.</li><li><a href=https://shapash.readthedocs.io/en/latest/ rel=noopener>https://shapash.readthedocs.io/en/latest/</a></li></ul></li><li><p>#CODE
<a href=https://github.com/oegedijk/explainerdashboard rel=noopener>ExplainerDashboard</a></p><ul><li><a href=https://explainerdashboard.readthedocs.io/en/latest/index.html# rel=noopener>https://explainerdashboard.readthedocs.io/en/latest/index.html#</a></li><li>library for quickly building interactive dashboards for analyzing and explaining the predictions and workings of (scikit-learn compatible) machine learning models, including xgboost, catboost and lightgbm</li><li>#TALK
<a href="https://www.youtube.com/watch?v=1nMlfrDvwc8" rel=noopener>https://www.youtube.com/watch?v=1nMlfrDvwc8</a></li></ul></li><li><p>#CODE
<a href=https://github.com/marcotcr/lime rel=noopener>LIME</a> - Local Interpretable Model-agnostic Explanations</p></li><li><p>#CODE
<a href=https://github.com/slundberg/shap rel=noopener>SHAP</a> - Unified approach to explain the output of any machine learning model</p><ul><li>SHAP connects game theory with local explanations, uniting several previous methods and representing the only possible consistent and locally accurate additive feature attribution method based on expectations.</li></ul></li><li><p>#CODE
<a href=https://github.com/linkedin/FastTreeSHAP rel=noopener>FastTreeSHAP</a> - Fast SHAP value computation for interpreting tree-based models</p><ul><li><a href=https://www.marktechpost.com/2022/03/20/linkedin-researchers-open-source-fasttreeshap-a-python-package-that-enables-an-efficient-interpretation-of-tree-based-machine-learning-models/ rel=noopener>LinkedIn Researchers Open-Source FastTreeSHAP</a></li></ul></li><li><p>#CODE
<a href=https://github.com/carla-recourse/CARLA rel=noopener>CARLA</a> - library for benchmarking counterfactual explanations and recourse models</p><ul><li>#PAPER
<a href=https://arxiv.org/abs/2108.00783 rel=noopener>CARLA: A Python Library to Benchmark Algorithmic Recourse and Counterfactual Explanation Algorithms (Pawelczyk 2021)</a></li><li><a href=https://www.marktechpost.com/2021/08/22/university-of-tubingen-researchers-open-source-carla-a-python-library-for-benchmarking-counterfactual-explanation-methods-across-data-sets-and-machine-learning-models/ rel=noopener>https://www.marktechpost.com/2021/08/22/university-of-tubingen-researchers-open-source-carla-a-python-library-for-benchmarking-counterfactual-explanation-methods-across-data-sets-and-machine-learning-models/</a></li></ul></li><li><p>#CODE
<a href=https://github.com/csinva/imodels rel=noopener>imodels</a></p><ul><li>Python package for concise, transparent, and accurate predictive modeling. All sklearn-compatible and easy to use</li><li><a href=https://www.marktechpost.com/2022/02/10/uc-berkeley-researchers-introduce-imodels-a-python-package-for-fitting-interpretable-machine-learning-models/ rel=noopener>UC Berkeley Researchers Introduce ‘imodels: A Python Package For Fitting Interpretable Machine Learning Models</a></li></ul></li><li><p>#CODE
<a href=https://github.com/Trusted-AI/AIX360 rel=noopener>AIX360</a> - Interpretability and explainability of data and ML models</p><ul><li><a href=http://aix360.mybluemix.net/ rel=noopener>http://aix360.mybluemix.net/</a></li></ul></li></ul><a href=#references><h2 id=references><span class=hanchor arialabel=Anchor># </span>References</h2></a><ul><li>#PAPER
<a href=https://arxiv.org/abs/1606.03490 rel=noopener>The Mythos of Model Interpretability (Lipton 2017)</a></li><li>#PAPER
<a href=https://dl.acm.org/doi/10.1145/3236009 rel=noopener>A Survey of Methods for Explaining Black Box Models (Guidotti, 2018)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1909.12072 rel=noopener>Towards Explainable Artificial Intelligence (Samek & Muller 2019)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1806.00069 rel=noopener>Explaining Explanations: An Overview of Interpretability of Machine Learning (Gilpin et al. 2019)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1909.03012 rel=noopener>One Explanation Does Not Fit All: A Toolkit and Taxonomy of AI Explainability Techniques (Arya 2019)</a><ul><li>#CODE See Code section</li></ul></li><li>#PAPER
<a href=https://www.nature.com/articles/s42256-019-0048-x rel=noopener>Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead (Rudin 2019)</a><ul><li><a href=https://arxiv.org/abs/1811.10154 rel=noopener>https://arxiv.org/abs/1811.10154</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1905.08883 rel=noopener>Explainable Machine Learning for Scientific Insights and Discoveries (Roscher 2020)</a></li><li>#PAPER
<a href=https://ieeexplore.ieee.org/document/9234594 rel=noopener>Review Study of Interpretation Methods for Future Interpretable Machine Learning (Jian-Xun 2020)</a></li><li>#PAPER
<a href=https://www.nature.com/articles/s43588-021-00132-w rel=noopener>Explainable neural networks that simulate reasoning (Blazek 2021)</a></li><li>#PAPER
<a href=https://journals.sagepub.com/doi/full/10.1177/20539517211020775 rel=noopener>Turning biases into hypotheses through method: A logic of scientific discovery for machine learning (Aagaard Enni 2021)</a><ul><li>Bridging the gap in the understanding of ML models and their reasonableness requires a focus on developing an improved methodology for their creation</li><li>This process has been likened to “alchemy” and criticized for involving a large degree of “black art,” owing to its reliance on poorly understood “best practices”</li><li>Authors soften this critique and argue that the seeming arbitrariness often is the result of a lack of explicit hypothesizing stemming from an empiricist and myopic focus on optimizing for predictive performance rather than from an occult or mystical process</li></ul></li><li>#PAPER #REVIEW
<a href=https://arxiv.org/pdf/2112.11407 rel=noopener>Toward Explainable AI for Regression Models (Letzgus 2021)</a><ul><li>Related to <a href=/digitalgarden/AI/Time-Series-analysis rel=noopener class=internal-link data-src=/digitalgarden/AI/Time-Series-analysis>AI/Time Series analysis</a> and <a href=/digitalgarden/AI/Supervised-Learning/Regression rel=noopener class=internal-link data-src=/digitalgarden/AI/Supervised-Learning/Regression>AI/Supervised Learning/Regression</a></li></ul></li></ul><a href=#model-agnostic-methods><h3 id=model-agnostic-methods><span class=hanchor arialabel=Anchor># </span>Model-agnostic methods</h3></a><ul><li><p><a href=https://christophm.github.io/interpretable-ml-book/agnostic.html rel=noopener>https://christophm.github.io/interpretable-ml-book/agnostic.html</a></p><ul><li>The great advantage of model-agnostic interpretation methods over model-specific ones is their flexibility</li><li>An alternative to model-agnostic interpretation methods is to use only interpretable models, which often has the big disadvantage that predictive performance is lost compared to other machine learning models and you limit yourself to one type of model</li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/abs/1309.6392 rel=noopener>Peeking Inside the Black Box: Visualizing Statistical Learning With Plots of Individual Conditional Expectation (Goldstein 2014)</a></p></li><li><p>#PAPER
<a href=https://arxiv.org/abs/1606.05386 rel=noopener>Model-Agnostic Interpretability of Machine Learning (Tulio Ribeiro 2016)</a></p></li><li><p>#PAPER
<a href=https://arxiv.org/abs/1705.07874 rel=noopener>SHAP - A Unified Approach to Interpreting Model Predictions (Lundberg 2017)</a></p><ul><li>SHAP (SHapley Additive exPlanations)</li><li>#CODE See Code section</li><li>Can be used for computer vision tasks</li></ul></li><li><p>#PAPER
<a href=https://arxiv.org/abs/2109.09847 rel=noopener>Fast TreeSHAP: Accelerating SHAP Value Computation for Trees (Yang 2021)</a></p></li><li><p>#PAPER
<a href=https://arxiv.org/pdf/2205.00130 rel=noopener>ExSum: From Local Explanations to Model Understanding (Zhou 2022)</a></p><ul><li><a href=https://yilunzhou.github.io/exsum/ rel=noopener>https://yilunzhou.github.io/exsum/</a></li><li>#CODE
<a href=https://github.com/YilunZhou/ExSum rel=noopener>https://github.com/YilunZhou/ExSum</a></li><li><a href=https://techxplore.com/news/2022-05-framework-individual-machine-learning-decisions.html rel=noopener>https://techxplore.com/news/2022-05-framework-individual-machine-learning-decisions.html</a></li></ul></li></ul><a href=#partial-dependence-plot><h4 id=partial-dependence-plot><span class=hanchor arialabel=Anchor># </span>Partial Dependence Plot</h4></a><ul><li>The partial dependence plot (short PDP or PD plot) shows the marginal effect one or two features have on the predicted outcome of a machine learning model</li><li><a href=https://christophm.github.io/interpretable-ml-book/pdp.html rel=noopener>https://christophm.github.io/interpretable-ml-book/pdp.html</a></li><li><a href=https://scikit-learn.org/stable/modules/partial_dependence.html rel=noopener>https://scikit-learn.org/stable/modules/partial_dependence.html</a></li></ul><a href=#individual-conditional-expectation><h4 id=individual-conditional-expectation><span class=hanchor arialabel=Anchor># </span>Individual Conditional Expectation</h4></a><ul><li>Individual Conditional Expectation (ICE) plots display one line per instance that shows how the instance&rsquo;s prediction changes when a feature changes</li><li><a href=https://christophm.github.io/interpretable-ml-book/ice.html rel=noopener>https://christophm.github.io/interpretable-ml-book/ice.html</a></li><li><a href=https://scikit-learn.org/stable/modules/partial_dependence.html rel=noopener>https://scikit-learn.org/stable/modules/partial_dependence.html</a></li></ul><a href=#permutation-feature-importance><h4 id=permutation-feature-importance><span class=hanchor arialabel=Anchor># </span>Permutation Feature Importance</h4></a><ul><li>Permutation feature importance measures the increase in the prediction error of the model after we permuted the feature&rsquo;s values, which breaks the relationship between the feature and the true outcome</li><li><a href=https://christophm.github.io/interpretable-ml-book/feature-importance.html rel=noopener>https://christophm.github.io/interpretable-ml-book/feature-importance.html</a></li><li><a href=https://scikit-learn.org/stable/modules/permutation_importance.html rel=noopener>https://scikit-learn.org/stable/modules/permutation_importance.html</a><ul><li>The permutation feature importance is defined to be the decrease in a model score when a single feature value is randomly shuffled</li><li>Tree-based models provide an alternative measure of feature importances based on the mean decrease in impurity (MDI). Impurity is quantified by the splitting criterion of the decision trees (Gini, Entropy or Mean Squared Error). However, this method can give high importance to features that may not be predictive on unseen data when the model is overfitting. Permutation-based feature importance, on the other hand, avoids this issue, since it can be computed on unseen data</li></ul></li></ul><a href=#surrogate-models><h4 id=surrogate-models><span class=hanchor arialabel=Anchor># </span>Surrogate models</h4></a><ul><li>A surrogate model is a simple model that is used to explain a complex model. Surrogate models are usually created by training a linear regression or decision tree on the original inputs and predictions of a complex model. Coefficients, variable importance, trends, and interactions displayed in the surrogate model are then assumed to be indicative of the internal mechanisms of the complex model. There are few, possibly no, theoretical guarantees that the simple surrogate model is highly representative of the more complex model.</li><li>The globally interpretable attributes of a simple model are used to explain global attributes of a more complex model. However, there is nothing to preclude fitting surrogate models to more local regions of a complex model&rsquo;s conditional distribution, such as clusters of input records and their corresponding predictions and their corresponding input rows. Because small sections of the conditional distribution are more likely to be linear, monotonic, or otherwise well-behaved, local surrogate models can be more accurate than global surrogate models.</li><li>#PAPER
<a href=https://arxiv.org/abs/1602.04938 rel=noopener>LIME - &ldquo;Why Should I Trust You?&rdquo;: Explaining the Predictions of Any Classifier (2016)</a> ^lime<ul><li>#CODE See Code section</li><li>Formalized approach for local surrogate models. It is meant to shed light on how decisions are made for specific observations. LIME requires that a set of explainable records be found, simulated, or created.</li><li><a href=https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime rel=noopener>https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime</a></li><li><a href=https://github.com/albahnsen/Talk_Demystifying_Machine_Learning rel=noopener>https://github.com/albahnsen/Talk_Demystifying_Machine_Learning</a></li><li><a href=http://www.datasciencecentral.com/profiles/blogs/deep-learning-epic-fail-right-answer-wrong-reason rel=noopener>Interpreting ML models prediction power</a></li><li><a href=https://medium.com/@ageitgey/natural-language-processing-is-fun-part-3-explaining-model-predictions-486d8616813c rel=noopener>https://medium.com/@ageitgey/natural-language-processing-is-fun-part-3-explaining-model-predictions-486d8616813c</a></li><li><a href=https://www.slideshare.net/albahnsen/demystifying-machine-learning-using-lime rel=noopener>https://www.slideshare.net/albahnsen/demystifying-machine-learning-using-lime</a></li><li><a href=https://github.com/albahnsen/Talk_Demystifying_Machine_Learning rel=noopener>https://github.com/albahnsen/Talk_Demystifying_Machine_Learning</a></li></ul></li></ul><a href=#maximum-activation-analysis><h3 id=maximum-activation-analysis><span class=hanchor arialabel=Anchor># </span>Maximum activation analysis</h3></a><ul><li>See <a href=/digitalgarden/AI/Deep-learning/Explainability-methods-for-NNs rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Explainability-methods-for-NNs>AI/Deep learning/Explainability methods for NNs</a></li><li>In maximum activation analysis, examples are found or simulated that maximally activate certain neurons, layers, or filters in a neural network or certain trees in decision tree ensembles. For the purposes of maximum activation analysis, low residuals for a certain tree are analogous to high-magnitude neuron output in a neural network.</li><li>Maximum activation analysis elucidates internal mechanisms of complex models by determining the parts of the response function that specific observations or groups of similar observations excite to the highest degree, either by high-magnitude output from neurons or by low residual output from trees.</li></ul><a href=#sensitivity-analysis><h3 id=sensitivity-analysis><span class=hanchor arialabel=Anchor># </span>Sensitivity analysis</h3></a><ul><li>Sensitivity analysis investigates whether model behavior and outputs remain stable when data is intentionally perturbed or other changes are simulated in data.</li><li>Beyond traditional assessment practices, sensitivity analysis of machine learning model predictions is perhaps the most important validation technique for machine learning models.</li><li>Machine learning models can make drastically differing predictions from minor changes in input variable values. In practice, many linear model validation techniques focus on the numerical instability of regression parameters due to correlation between input variables or between input variables and the dependent variable</li><li>Sensitivity analysis can also test model behavior and outputs when interesting situations or known corner cases are simulated. Output distributions, error measurements, plots, and interpretation techniques can be used to explore the way models behave in important scenarios, how they change over time, or if models remain stable when data is subtly and intentionally corrupted</li></ul><a href=#variable-importance-measures><h3 id=variable-importance-measures><span class=hanchor arialabel=Anchor># </span>Variable importance measures</h3></a><ul><li>Variable importance measures are typically seen in tree-based models but are sometimes also reported for other models.</li><li>A simple heuristic rule for variable importance in a decision tree is related to the depth and frequency at which a variable is split on in a tree, where variables used higher in the tree and more frequently in the tree are more important.</li><li>For a single decision tree, a variable&rsquo;s importance is quantitatively determined by the cumulative change in the splitting criterion for every node in which that variable was chosen as the best splitting candidate.</li><li>For a gradient boosted tree ensemble, variable importance is calculated as it is for a single tree but aggregated for the ensemble.</li><li>For random forests:<ul><li>Variable importance is also calculated as it is for a single tree and aggregated, but an additional measure of variable importance is provided by the change in out-of-bag accuracy caused by shuffling the independent variable of interest, where larger decreases in accuracy are taken as larger indications of importance</li><li>The default method to compute variable importance is the mean decrease in impurity (or gini importance) mechanism: At each split in each tree, the improvement in the split-criterion is the importance measure attributed to the splitting variable, and is accumulated over all the trees in the forest separately for each variable. Note that this measure is quite like the R^2 in regression on the training set</li><li><a href=https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html rel=noopener>This example highlights the limitations of impurity-based feature importance in contrast to permutation-based feature importance</a></li><li>#PAPER
<a href=https://papers.nips.cc/paper/4928-understanding-variable-importances-in-forests-of-randomized-trees.pdf rel=noopener>Understanding variable importances in forests of randomized trees (Louppe 2013)</a><ul><li>#POSTER
<a href=https://orbi.uliege.be/bitstream/2268/155642/3/poster.pdf rel=noopener>https://orbi.uliege.be/bitstream/2268/155642/3/poster.pdf</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2001.04295 rel=noopener>Trees, forests, and impurity-based variable importance (Scornet 2020)</a></li></ul></li><li>For neural networks, variable importance measures are typically associated with the aggregated, absolute magnitude of model parameters for a given variable of interest.</li><li>Global variable importance techniques are typically model specific, and practitioners should be aware that unsophisticated measures of variable importance can be biased toward larger scale variables or variables with a high number of categories.</li></ul><a href=#explainability-methods-for-neural-networks><h3 id=explainability-methods-for-neural-networks><span class=hanchor arialabel=Anchor># </span>Explainability methods for Neural Networks</h3></a><p>See <a href=/digitalgarden/AI/Deep-learning/Explainability-methods-for-NNs rel=noopener class=internal-link data-src=/digitalgarden/AI/Deep-learning/Explainability-methods-for-NNs>AI/Deep learning/Explainability methods for NNs</a></p></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/digitalgarden/AI/AI/ data-ctx=AI/XAI data-src=/AI/AI class=internal-link>Artificial Intelligence (AI)</a></li><li><a href=/digitalgarden/AI/Deep-learning/CNNs/ data-ctx=AI/XAI data-src=/AI/Deep-learning/CNNs class=internal-link>Convolutional Neural Networks (CNNs)</a></li><li><a href=/digitalgarden/AI/ML/ data-ctx=AI/XAI data-src=/AI/ML class=internal-link>Machine Learning (ML)</a></li><li><a href=/digitalgarden/AI/Time-Series-analysis/ data-ctx=AI/XAI data-src=/AI/Time-Series-analysis class=internal-link>Time series analysis</a></li><li><a href=/digitalgarden/AI4ES/XAI-for-ES/ data-ctx=AI/XAI data-src=/AI4ES/XAI-for-ES class=internal-link>XAI for Earth Sciences</a></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://carlos-gg.github.io/digitalgarden/js/graph.abd4bc2af3869a96524d7d23b76152c7.js></script></div></div><div id=contact_buttons><footer><p>Made by Carlos Alberto Gomez Gonzalez using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2023</p><ul><li><a href=https://carlos-gg.github.io/digitalgarden/>Home</a></li><li><a href=https://carlos-gg.github.io>Carlos'Homepage</a></li></ul></footer></div></div></body></html>