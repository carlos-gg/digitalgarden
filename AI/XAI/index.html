<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Resources  https://github.com/anguyen8/XAI-papers https://en.wikipedia.org/wiki/Explainable_artificial_intelligence Ideas on interpreting [[machine learning]]: https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning Explainable AI demos: https://lrpserver.hhi.fraunhofer.de/  Why you need to care about Explainable Machine Learning  Interpreting machine learning models I."><title>Explainable AI (XAI)</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=/icon.png><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Source+Sans+Pro:wght@400;600;700&family=Fira+Code:wght@400;700&display=swap" rel=stylesheet><style>:root{--lt-colours-light:var(--light) !important;--lt-colours-lightgray:var(--lightgray) !important;--lt-colours-dark:var(--secondary) !important;--lt-colours-secondary:var(--tertiary) !important;--lt-colours-gray:var(--outlinegray) !important}h1,h2,h3,h4,h5,h6,ol,ul,thead{font-family:Inter;color:var(--dark);font-weight:revert;margin:revert;padding:revert}p,ul,text{font-family:source sans pro,sans-serif;color:var(--gray);fill:var(--gray);font-weight:revert;margin:revert;padding:revert}#TableOfContents>ol{counter-reset:section;margin-left:0;padding-left:1.5em}#TableOfContents>ol>li{counter-increment:section}#TableOfContents>ol>li>ol{counter-reset:subsection}#TableOfContents>ol>li>ol>li{counter-increment:subsection}#TableOfContents>ol>li>ol>li::marker{content:counter(section)"." counter(subsection)"  "}#TableOfContents>ol>li::marker{content:counter(section)"  "}#TableOfContents>ol>li::marker,#TableOfContents>ol>li>ol>li::marker{font-family:Source Sans Pro;font-weight:700}footer{margin-top:4em;text-align:center}table{width:100%}img{width:100%;border-radius:3px;margin:1em 0}p>img+em{display:block;transform:translateY(-1em)}sup{line-height:0}p,tbody,li{font-family:Source Sans Pro;color:var(--gray);line-height:1.5em}blockquote{margin-left:0;border-left:3px solid var(--secondary);padding-left:1em;transition:border-color .2s ease}blockquote:hover{border-color:var(--tertiary)}table{padding:1.5em}td,th{padding:.1em .5em}.footnotes p{margin:.5em 0}.pagination{list-style:none;padding-left:0;display:flex;margin-top:2em;gap:1.5em;justify-content:center}.pagination>li{text-align:center;display:inline-block}.pagination>li a{background-color:initial!important}.pagination>li a[href$="#"]{opacity:.2}.section h3>a{font-weight:700;font-family:Inter;margin:0}.section p{margin-top:0}article>.meta{margin:-1.5em 0 1em;opacity:.7}article>.tags{list-style:none;padding-left:0}article>.tags .meta>h1{margin:0}article>.tags .meta>p{margin:0}article>.tags>li{display:inline-block}article>.tags>li>a{border-radius:8px;border:var(--outlinegray)1px solid;padding:.2em .5em}article>.tags>li>a::before{content:"#";margin-right:.3em;color:var(--outlinegray)}article a{font-family:Source Sans Pro;font-weight:600}article a.internal-link{text-decoration:none;background-color:rgba(143,159,169,.15);padding:0 .1em;margin:auto -.1em;border-radius:3px}.backlinks a{font-weight:600;font-size:.9rem}sup>a{text-decoration:none;padding:0 .1em 0 .2em}a{font-family:Inter,sans-serif;font-size:1em;font-weight:700;text-decoration:none;transition:all .2s ease;color:var(--secondary)}a:hover{color:var(--tertiary)!important}pre{font-family:fira code;padding:.75em;border-radius:3px;overflow-x:scroll}code{font-family:fira code;font-size:.85em;padding:.15em .3em;border-radius:5px;background:var(--lightgray)}html{scroll-behavior:smooth}html:lang(ar) p,html:lang(ar) h1,html:lang(ar) h2,html:lang(ar) h3,html:lang(ar) article{direction:rtl;text-align:right}body{margin:0;height:100vh;width:100vw;overflow-x:hidden;background-color:var(--light)}@keyframes fadeIn{0%{opacity:0}100%{opacity:1}}footer{margin-top:4em}footer>a{font-size:1em;color:var(--secondary);padding:0 .5em 3em}hr{width:25%;margin:4em auto;height:2px;border-radius:1px;border-width:0;color:var(--dark);background-color:var(--dark)}.singlePage{margin:4em 30vw}@media all and (max-width:1200px){.singlePage{margin:25px 5vw}}.page-end{display:flex;flex-direction:row;gap:2em}@media all and (max-width:780px){.page-end{flex-direction:column}}.page-end>*{flex:1 0}.page-end>.backlinks-container>ul{list-style:none;padding-left:0}.page-end>.backlinks-container>ul>li{margin:.5em 0;padding:.25em 1em;border:var(--outlinegray)1px solid;border-radius:5px}.page-end #graph-container{border:var(--outlinegray)1px solid;border-radius:5px}.centered{margin-top:30vh}article>h1{font-size:2em}header{display:flex;flex-direction:row;align-items:center}header>h1{font-size:2em}@media all and (max-width:600px){header>nav{display:none}}header>.spacer{flex:auto}header>svg{cursor:pointer;width:18px;min-width:18px;margin:0 1em}header>svg:hover .search-path{stroke:var(--tertiary)}header>svg .search-path{stroke:var(--gray);stroke-width:2px;transition:stroke .5s ease}#search-container{position:fixed;z-index:9999;left:0;top:0;width:100vw;height:100%;overflow:scroll;display:none;backdrop-filter:blur(4px);-webkit-backdrop-filter:blur(4px)}#search-container>div{width:50%;margin-top:15vh;margin-left:auto;margin-right:auto}@media all and (max-width:1200px){#search-container>div{width:90%}}#search-container>div>*{width:100%;border-radius:4px;background:var(--light);box-shadow:0 14px 50px rgba(27,33,48,.12),0 10px 30px rgba(27,33,48,.16);margin-bottom:2em}#search-container>div>input{box-sizing:border-box;padding:.5em 1em;font-family:Inter,sans-serif;color:var(--dark);font-size:1.1em;border:1px solid var(--outlinegray)}#search-container>div>input:focus{outline:none}#search-container>div>#results-container>.result-card{padding:1em;cursor:pointer;transition:background .2s ease;border:1px solid var(--outlinegray);border-bottom:none;width:100%;font-family:inherit;font-size:100%;line-height:1.15;margin:0;overflow:visible;text-transform:none;text-align:left;background:var(--light);outline:none}#search-container>div>#results-container>.result-card:hover,#search-container>div>#results-container>.result-card:focus{background:rgba(180,180,180,.15)}#search-container>div>#results-container>.result-card:first-of-type{border-top-left-radius:5px;border-top-right-radius:5px}#search-container>div>#results-container>.result-card:last-of-type{border-bottom-left-radius:5px;border-bottom-right-radius:5px;border-bottom:1px solid var(--outlinegray)}#search-container>div>#results-container>.result-card>h3,#search-container>div>#results-container>.result-card>p{margin:0}#search-container>div>#results-container>.result-card .search-highlight{background-color:#afbfc966;padding:.05em .2em;border-radius:3px}.section-ul{list-style:none;padding-left:0}.section-ul>li{border:1px solid var(--outlinegray);border-radius:5px;padding:0 1em;margin-bottom:1em}.section-ul>li h3{opacity:1;font-weight:700;margin-bottom:0}.section-ul>li .meta{opacity:.6}.popover{z-index:999;position:absolute;width:20em;display:inline-block;visibility:hidden;background-color:var(--light);padding:1em;border:1px solid var(--outlinegray);border-radius:5px;transform:translate(-50%,40%);opacity:0;pointer-events:none;transition:opacity .2s ease,transform .2s ease;transition-delay:.3s;user-select:none}@media all and (max-width:600px){.popover{display:none}}.popover.visible{opacity:1;visibility:visible;transform:translate(-50%,20%)}.popover>h3{font-size:1rem;margin:.25em 0}.popover>.meta{margin-top:.25em;opacity:.5}.popover>p{margin:0;font-weight:400;user-select:none}</style><style>.darkmode{float:right;padding:1em;min-width:30px;position:relative}@media all and (max-width:450px){.darkmode{padding:1em}}.darkmode>.toggle{display:none;box-sizing:border-box}.darkmode svg{opacity:0;position:absolute;width:20px;height:20px;top:calc(50% - 10px);margin:0 7px;fill:var(--gray);transition:opacity .1s ease}.toggle:checked~label>#dayIcon{opacity:0}.toggle:checked~label>#nightIcon{opacity:1}.toggle:not(:checked)~label>#dayIcon{opacity:1}.toggle:not(:checked)~label>#nightIcon{opacity:0}</style><style>.chroma{color:#f8f8f2;background-color:#282a36;overflow:hidden}.chroma .lntd{vertical-align:top;padding:0;margin:0;border:0}.chroma .lntable{border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block}.chroma .hl{display:block;width:100%;background-color:#ffc}.chroma .lnt{margin-right:.4em;padding:0 .4em;color:#7f7f7f}.chroma .ln{margin-right:.4em;padding:0 .4em;color:#7f7f7f}.chroma .k{color:#ff79c6}.chroma .kc{color:#ff79c6}.chroma .kd{color:#8be9fd;font-style:italic}.chroma .kn{color:#ff79c6}.chroma .kp{color:#ff79c6}.chroma .kr{color:#ff79c6}.chroma .kt{color:#8be9fd}.chroma .na{color:#50fa7b}.chroma .nb{color:#8be9fd;font-style:italic}.chroma .nc{color:#50fa7b}.chroma .nf{color:#50fa7b}.chroma .nl{color:#8be9fd;font-style:italic}.chroma .nt{color:#ff79c6}.chroma .nv{color:#8be9fd;font-style:italic}.chroma .vc{color:#8be9fd;font-style:italic}.chroma .vg{color:#8be9fd;font-style:italic}.chroma .vi{color:#8be9fd;font-style:italic}.chroma .s{color:#f1fa8c}.chroma .sa{color:#f1fa8c}.chroma .sb{color:#f1fa8c}.chroma .sc{color:#f1fa8c}.chroma .dl{color:#f1fa8c}.chroma .sd{color:#f1fa8c}.chroma .s2{color:#f1fa8c}.chroma .se{color:#f1fa8c}.chroma .sh{color:#f1fa8c}.chroma .si{color:#f1fa8c}.chroma .sx{color:#f1fa8c}.chroma .sr{color:#f1fa8c}.chroma .s1{color:#f1fa8c}.chroma .ss{color:#f1fa8c}.chroma .m{color:#bd93f9}.chroma .mb{color:#bd93f9}.chroma .mf{color:#bd93f9}.chroma .mh{color:#bd93f9}.chroma .mi{color:#bd93f9}.chroma .il{color:#bd93f9}.chroma .mo{color:#bd93f9}.chroma .o{color:#ff79c6}.chroma .ow{color:#ff79c6}.chroma .c{color:#6272a4}.chroma .ch{color:#6272a4}.chroma .cm{color:#6272a4}.chroma .c1{color:#6272a4}.chroma .cs{color:#6272a4}.chroma .cp{color:#ff79c6}.chroma .cpf{color:#ff79c6}.chroma .gd{color:#8b080b}.chroma .ge{text-decoration:underline}.chroma .gh{font-weight:700}.chroma .gi{font-weight:700}.chroma .go{color:#44475a}.chroma .gu{font-weight:700}.chroma .gl{text-decoration:underline}.lntd:first-of-type>.chroma{padding-right:0}.chroma code{font-family:fira code!important;font-size:.85em;line-height:1em;background:0 0;padding:0}.chroma{border-radius:3px;margin:0}</style><style>:root{--light:#faf8f8;--dark:#141021;--secondary:#284b63;--tertiary:#84a59d;--visited:#afbfc9;--primary:#f28482;--gray:#4e4e4e;--lightgray:#f0f0f0;--outlinegray:#dadada}[saved-theme=dark]{--light:#1e1e21 !important;--dark:#fbfffe !important;--secondary:#6b879a !important;--visited:#4a575e !important;--tertiary:#84a59d !important;--primary:#f58382 !important;--gray:#d4d4d4 !important;--lightgray:#292633 !important;--outlinegray:#343434 !important}</style><script>const userPref=window.matchMedia('(prefers-color-scheme: light)').matches?'light':'dark',currentTheme=localStorage.getItem('theme')??userPref;currentTheme&&document.documentElement.setAttribute('saved-theme',currentTheme);const switchTheme=a=>{a.target.checked?(document.documentElement.setAttribute('saved-theme','dark'),localStorage.setItem('theme','dark')):(document.documentElement.setAttribute('saved-theme','light'),localStorage.setItem('theme','light'))};window.addEventListener('DOMContentLoaded',()=>{const a=document.querySelector('#darkmode-toggle');a.addEventListener('change',switchTheme,!1),currentTheme==='dark'&&(a.checked=!0)})</script><script>let saved=!1;const fetchData=async()=>{if(saved)return saved;const promises=[fetch("https://carlos-gg.github.io/AIDigitalGarden/linkIndex.json").then(a=>a.json()).then(a=>({index:a.index,links:a.links})),fetch("https://carlos-gg.github.io/AIDigitalGarden/contentIndex.json").then(a=>a.json())],[{index,links},content]=await Promise.all(promises),res={index,links,content};return saved=res,res};fetchData()</script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-S103D50NQ0"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-S103D50NQ0',{anonymize_ip:!1})}</script><script>function htmlToElement(a){const b=document.createElement('template');return a=a.trim(),b.innerHTML=a,b.content.firstChild}const baseUrl="https://carlos-gg.github.io/AIDigitalGarden".replace(window.location.origin,"");document.addEventListener("DOMContentLoaded",()=>{fetchData().then(({content:a})=>{const b=[...document.getElementsByClassName("internal-link")];b.forEach(b=>{const c=a[b.dataset.src.replace(baseUrl,"")];if(c){const d=`<div class="popover">
    <h3>${c.title}</h3>
    <p>${removeMarkdown(c.content).split(" ",20).join(" ")}...</p>
    <p class="meta">${new Date(c.lastmodified).toLocaleDateString()}</p>
</div>`,a=htmlToElement(d);b.appendChild(a),b.addEventListener("mouseover",()=>{a.classList.add("visible")}),b.addEventListener("mouseout",()=>{a.classList.remove("visible")})}})})})</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/gh/nextapps-de/flexsearch@0.7.2/dist/flexsearch.bundle.js></script><script>const removeMarkdown=(c,b={listUnicodeChar:!1,stripListLeaders:!0,gfm:!0,useImgAltText:!1,preserveLinks:!1})=>{let a=c||"";a=a.replace(/^(-\s*?|\*\s*?|_\s*?){3,}\s*$/gm,"");try{b.stripListLeaders&&(b.listUnicodeChar?a=a.replace(/^([\s\t]*)([\*\-\+]|\d+\.)\s+/gm,b.listUnicodeChar+" $1"):a=a.replace(/^([\s\t]*)([\*\-\+]|\d+\.)\s+/gm,"$1")),b.gfm&&(a=a.replace(/\n={2,}/g,"\n").replace(/~{3}.*\n/g,"").replace(/~~/g,"").replace(/`{3}.*\n/g,"")),b.preserveLinks&&(a=a.replace(/\[(.*?)\][\[\(](.*?)[\]\)]/g,"$1 ($2)")),a=a.replace(/<[^>]*>/g,"").replace(/^[=\-]{2,}\s*$/g,"").replace(/\[\^.+?\](\: .*?$)?/g,"").replace(/\s{0,2}\[.*?\]: .*?$/g,"").replace(/\!\[(.*?)\][\[\(].*?[\]\)]/g,b.useImgAltText?"$1":"").replace(/\[(.*?)\][\[\(].*?[\]\)]/g,"$1").replace(/^\s{0,3}>\s?/g,"").replace(/(^|\n)\s{0,3}>\s?/g,"\n\n").replace(/^\s{1,2}\[(.*?)\]: (\S+)( ".*?")?\s*$/g,"").replace(/^(\n)?\s{0,}#{1,6}\s+| {0,}(\n)?\s{0,}#{0,} {0,}(\n)?\s{0,}$/gm,"$1$2$3").replace(/([\*_]{1,3})(\S.*?\S{0,1})\1/g,"$2").replace(/([\*_]{1,3})(\S.*?\S{0,1})\1/g,"$2").replace(/(`{3,})(.*?)\1/gm,"$2").replace(/`(.+?)`/g,"$1").replace(/\n{2,}/g,"\n\n")}catch(a){return console.error(a),c}return a}</script><script>async function run(){const g=new FlexSearch.Document({cache:!0,charset:"latin:extra",optimize:!0,worker:!0,document:{index:[{field:"content",tokenize:"strict",context:{resolution:5,depth:3,bidirectional:!0},suggest:!0},{field:"title",tokenize:"forward"}]}}),{content:d}=await fetchData();for(const[b,a]of Object.entries(d))g.add({id:b,title:a.title,content:removeMarkdown(a.content)});const j=(i,j)=>{const a=20,k=j.split(/\s+/).filter(a=>a!==""),b=i.split(/\s+/).filter(a=>a!==""),f=a=>k.some(b=>a.toLowerCase().startsWith(b.toLowerCase())),d=b.map(f);let e=0,g=0;for(let b=0;b<Math.max(d.length-a,0);b++){const f=d.slice(b,b+a),c=f.reduce((a,b)=>a+b,0);c>=e&&(e=c,g=b)}const c=Math.max(g-a,0),h=Math.min(c+2*a,b.length),l=b.slice(c,h).map(a=>{return f(a)?`<span class="search-highlight">${a}</span>`:a}).join(" ").replaceAll('</span> <span class="search-highlight">'," ");return`${c===0?"":"..."}${l}${h===b.length?"":"..."}`},m=({url:b,title:c,content:d,term:a})=>{const e=removeMarkdown(d),f=j(c,a),g=j(e,a);return`<button class="result-card" id="${b}">
        <h3>${f}</h3>
        <p>${g}</p>
    </button>`},h=(a,b)=>{window.location.href="https://carlos-gg.github.io/AIDigitalGarden"+`${a}#:~:text=${encodeURIComponent(b)}`},l=a=>({id:a,url:a,title:d[a].title,content:d[a].content}),c=document.getElementById('search-bar'),e=document.getElementById("results-container");let b;c.addEventListener("keyup",a=>{if(a.key==="Enter"){const a=document.getElementsByClassName("result-card")[0];h(a.id,b)}}),c.addEventListener('input',a=>{b=a.target.value,g.search(b,[{field:"content",limit:10,suggest:!0},{field:"title",limit:5}]).then(d=>{const a=b=>{const a=d.filter(a=>a.field===b);return a.length===0?[]:[...a[0].result]},f=new Set([...a('title'),...a('content')]),c=[...f].map(l);if(c.length===0)e.innerHTML=`<button class="result-card">
                    <h3>No results.</h3>
                    <p>Try another search term?</p>
                </button>`;else{e.innerHTML=c.map(a=>m({...a,term:b})).join("\n");const a=document.getElementsByClassName("result-card");[...a].forEach(a=>{a.onclick=()=>h(a.id,b)})}})});const a=document.getElementById("search-container");function f(){a.style.display==="none"||a.style.display===""?(c.value="",e.innerHTML="",a.style.display="block",c.focus()):a.style.display="none"}function i(){a.style.display="none"}document.addEventListener('keydown',a=>{a.key==="/"&&(a.preventDefault(),f()),a.key==="Escape"&&(a.preventDefault(),i())});const k=document.getElementById("search-icon");k.addEventListener('click',a=>{f()}),k.addEventListener('keydown',a=>{f()}),a.addEventListener('click',a=>{i()}),document.getElementById("search-space").addEventListener('click',a=>{a.stopPropagation()})}run()</script><div class=singlePage><header><h1 id=page-title><a href=https://carlos-gg.github.io/AIDigitalGarden>AI Digital Garden ðŸ¤–ðŸª´</a></h1><svg tabindex="0" id="search-icon" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg><div class=spacer></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Explainable AI (XAI)</h1><p class=meta>Last updated March 7, 2022</p><ul class=tags></ul><aside class=mainTOC><h3>Table of Contents</h3><nav id=TableOfContents><ol><li><a href=#model-agnostic-methods>Model-agnostic methods</a><ol><li><a href=#partial-dependence-plot>Partial Dependence Plot</a></li><li><a href=#individual-conditional-expectation>Individual Conditional Expectation</a></li><li><a href=#permutation-feature-importance>Permutation Feature Importance</a></li><li><a href=#surrogate-models>Surrogate models</a></li></ol></li><li><a href=#maximum-activation-analysis>Maximum activation analysis</a></li><li><a href=#sensitivity-analysis>Sensitivity analysis</a></li><li><a href=#variable-importance-measures>Variable importance measures</a></li><li><a href=#neural-networks-explainability>Neural Networks explainability</a><ol><li><a href=#resources-1>Resources</a></li><li><a href=#code-1>Code</a></li><li><a href=#references-1>References</a></li></ol></li></ol></nav></aside><h1 id=resources>Resources</h1><ul><li><a href=https://github.com/anguyen8/XAI-papers>https://github.com/anguyen8/XAI-papers</a></li><li><a href=https://en.wikipedia.org/wiki/Explainable_artificial_intelligence>https://en.wikipedia.org/wiki/Explainable_artificial_intelligence</a></li><li>Ideas on interpreting [[machine learning]]: <a href=https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning>https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning</a></li><li>Explainable AI demos: <a href=https://lrpserver.hhi.fraunhofer.de/>https://lrpserver.hhi.fraunhofer.de/</a></li><li><a href=https://medium.com/james-blogs/why-you-need-to-care-about-explainable-machine-learning-d01196a6af76 rel=noopener>Why you need to care about Explainable Machine Learning</a></li><li><a href=https://towardsdatascience.com/interpretability-in-machine-learning-70c30694a05f rel=noopener>Interpreting machine learning models</a></li><li>I.am.ai. Explaining artificial intelligence: <a href=https://www.i-am.ai/>https://www.i-am.ai/</a></li><li><a href=https://cloud.google.com/blog/topics/developers-practitioners/baking-recipes-made-ai rel=noopener>Baking recipes made by AI</a></li><li>A Review of Different Interpretation Methods:<ul><li><a href=https://mrsalehi.medium.com/a-review-of-different-interpretation-methods-in-deep-learning-part-1-saliency-map-cam-grad-cam-3a34476bc24d>https://mrsalehi.medium.com/a-review-of-different-interpretation-methods-in-deep-learning-part-1-saliency-map-cam-grad-cam-3a34476bc24d</a></li><li><a href=https://mrsalehi.medium.com/a-review-of-different-interpretation-methods-in-deep-learning-part-2-input-gradient-layerwise-e077609b6377>https://mrsalehi.medium.com/a-review-of-different-interpretation-methods-in-deep-learning-part-2-input-gradient-layerwise-e077609b6377</a></li><li><a href=https://mrsalehi.medium.com/a-review-of-different-interpretation-methods-in-deep-learning-part-3-shap-integrated-gradients-918fc9fedd9b>https://mrsalehi.medium.com/a-review-of-different-interpretation-methods-in-deep-learning-part-3-shap-integrated-gradients-918fc9fedd9b</a></li></ul></li></ul><h1 id=events-talks>Events, talks</h1><ul><li>Workshop on Visualization for [[AI]] Explainability: <a href=http://visxai.io/>http://visxai.io/</a></li><li>ACM Conference on Fairness, Accountability, and Transparency: <a href=https://facctconference.org/>https://facctconference.org/</a></li><li>Explainable AI xAI 2020: <a href=https://human-centered.ai/explainable-ai-2020/>https://human-centered.ai/explainable-ai-2020/</a></li><li>#TALK Synthesizing Explainable and Deceptive Behavior for Human-[[AI]] Interaction (AAAI 2020 Tutorial): <a href=https://yochan-lab.github.io/tutorial/AAAI-2020/>https://yochan-lab.github.io/tutorial/AAAI-2020/</a><ul><li><a href="https://www.youtube.com/watch?v=r6KhJ3ORYnc">https://www.youtube.com/watch?v=r6KhJ3ORYnc</a></li></ul></li><li>#TALK Explainable [[AI]] in Industry (Tutorial): <a href=https://sites.google.com/view/explainable-ai-tutorial>https://sites.google.com/view/explainable-ai-tutorial</a><ul><li><a href="https://www.youtube.com/watch?list=PLewjn-vrZ7d3x0M4Uu_57oaJPRXkiS221&v=rcUw7PXHWF4">https://www.youtube.com/watch?list=PLewjn-vrZ7d3x0M4Uu_57oaJPRXkiS221&v=rcUw7PXHWF4</a></li></ul></li><li>#TALK Explainable AI: Foundations, Industrial Applications, Practical Challenges, and Lessons Learned (AAAI 2020): <a href=https://xaitutorial2020.github.io/>https://xaitutorial2020.github.io/</a><ul><li><a href=https://xaitutorial2020.github.io/raw/master/slides/aaai_2020_xai_tutorial.pdf>https://xaitutorial2020.github.io/raw/master/slides/aaai_2020_xai_tutorial.pdf</a></li></ul></li></ul><h1 id=books>Books</h1><ul><li>#BOOK Interpretable Machine Learning (Molnar 2021): <a href=https://christophm.github.io/interpretable-ml-book/>https://christophm.github.io/interpretable-ml-book/</a></li></ul><h1 id=code>Code</h1><p>See [[#Neural Networks explainability#Code]]</p><ul><li><p><a href=https://towardsdatascience.com/explainable-ai-xai-a-guide-to-7-packages-in-python-to-explain-your-models-932967f0634b>https://towardsdatascience.com/explainable-ai-xai-a-guide-to-7-packages-in-python-to-explain-your-models-932967f0634b</a></p></li><li><p>#CODE CARLA: <a href=https://github.com/carla-recourse/CARLA>https://github.com/carla-recourse/CARLA</a></p><ul><li>CARLA is a python library to benchmark counterfactual explanation and recourse models</li><li>#PAPER CARLA: A Python Library to Benchmark Algorithmic Recourse and Counterfactual Explanation Algorithms (Pawelczyk 2021): <a href=https://arxiv.org/abs/2108.00783>https://arxiv.org/abs/2108.00783</a></li><li><a href=https://www.marktechpost.com/2021/08/22/university-of-tubingen-researchers-open-source-carla-a-python-library-for-benchmarking-counterfactual-explanation-methods-across-data-sets-and-machine-learning-models/>https://www.marktechpost.com/2021/08/22/university-of-tubingen-researchers-open-source-carla-a-python-library-for-benchmarking-counterfactual-explanation-methods-across-data-sets-and-machine-learning-models/</a></li></ul></li><li><p>#CODE Shapash: <a href=https://github.com/MAIF/shapash>https://github.com/MAIF/shapash</a></p><ul><li><a href=https://shapash.readthedocs.io/en/latest/>https://shapash.readthedocs.io/en/latest/</a></li></ul></li><li><p>#CODE ExplainerDashboard: <a href=https://github.com/oegedijk/explainerdashboard>https://github.com/oegedijk/explainerdashboard</a></p><ul><li><a href=https://explainerdashboard.readthedocs.io/en/latest/index.html#>https://explainerdashboard.readthedocs.io/en/latest/index.html#</a></li><li>library for quickly building interactive dashboards for analyzing and explaining the predictions and workings of (scikit-learn compatible) machine learning models, including xgboost, catboost and lightgbm</li><li>#TALK <a href="https://www.youtube.com/watch?v=1nMlfrDvwc8">https://www.youtube.com/watch?v=1nMlfrDvwc8</a></li></ul></li><li><p>#CODE AIX360: <a href=https://github.com/Trusted-AI/AIX360>https://github.com/Trusted-AI/AIX360</a> ^aix360</p><ul><li>Interpretability and explainability of data and machine learning models</li><li><a href=http://aix360.mybluemix.net/>http://aix360.mybluemix.net/</a></li></ul></li><li><p>#CODE LIME: Local Interpretable Model-agnostic Explanations: <a href=https://github.com/marcotcr/lime>https://github.com/marcotcr/lime</a> ^limegithub</p></li><li><p>#CODE Skater: <a href=https://github.com/datascienceinc/Skater>https://github.com/datascienceinc/Skater</a></p><ul><li>Skater is a python package for model agnostic interpretation of predictive models. With Skater, you can unpack the internal mechanics of arbitrary models; as long as you can obtain inputs, and use a function to obtain outputs, you can use Skater to learn about the models internal decision policies.</li><li><a href=https://datascienceinc.github.io/Skater/overview.html>https://datascienceinc.github.io/Skater/overview.html</a></li><li>Understanding How and Why Your Model Works: <a href=https://www.datascience.com/learn-data-science/fundamentals/model-interpretation-algorithms>https://www.datascience.com/learn-data-science/fundamentals/model-interpretation-algorithms</a></li><li><a href=https://www.datascience.com/resources/tools/skater>https://www.datascience.com/resources/tools/skater</a></li></ul></li><li><p>#CODE FairML - Auditing Black-Box Predictive Models: <a href=https://github.com/adebayoj/fairml>https://github.com/adebayoj/fairml</a></p><ul><li>FairML is a python toolbox auditing the machine learning models for bias.</li><li><a href=http://blog.fastforwardlabs.com/2017/03/09/fairml-auditing-black-box-predictive-models.html>http://blog.fastforwardlabs.com/2017/03/09/fairml-auditing-black-box-predictive-models.html</a></li></ul></li><li><p>#CODE ELI5: <a href=https://github.com/TeamHG-Memex/eli5>https://github.com/TeamHG-Memex/eli5</a></p><ul><li>ELI5 is a Python library which allows to visualize and debug various Machine Learning models using unified API. It has built-in support for several ML frameworks and provides a way to explain black-box models.</li><li><a href=http://eli5.readthedocs.io/en/latest/>http://eli5.readthedocs.io/en/latest/</a></li></ul></li><li><p>#CODE BlackBox Auditing: <a href=https://github.com/algofairness/BlackBoxAuditing>https://github.com/algofairness/BlackBoxAuditing</a></p></li><li><p>#CODE SHAP: <a href=https://github.com/slundberg/shap>https://github.com/slundberg/shap</a> ^shapgithub</p><ul><li>Unified approach to explain the output of any machine learning model. SHAP connects game theory with local explanations, uniting several previous methods and representing the only possible consistent and locally accurate additive feature attribution method based on expectations.</li></ul></li><li><p>#CODE InterpretML - Microsoft open-source package that incorporates state-of-the-art machine learning interpretability techniques under one roof: <a href=https://github.com/interpretml/interpret>https://github.com/interpretml/interpret</a></p></li><li><p>#CODE uncertainty-toolbox: <a href=https://github.com/uncertainty-toolbox/uncertainty-toolbox>https://github.com/uncertainty-toolbox/uncertainty-toolbox</a>Ã§</p></li><li><p>#CODE imodels: <a href=https://github.com/csinva/imodels>https://github.com/csinva/imodels</a></p><ul><li>Python package for concise, transparent, and accurate predictive modeling. All sklearn-compatible and easy to use.</li></ul></li></ul><h1 id=references>References</h1><ul><li>#PAPER The Mythos of Model Interpretability (Lipton 2017): <a href=https://arxiv.org/abs/1606.03490>https://arxiv.org/abs/1606.03490</a></li><li>#PAPER A Survey of Methods for Explaining Black Box Models (Guidotti, 2018): <a href=https://dl.acm.org/doi/10.1145/3236009>https://dl.acm.org/doi/10.1145/3236009</a></li><li>#PAPER Making the Black Box More Transparent: Understanding the Physical Implications of Machine Learning (McGovern et al. 2019): <a href=https://journals.ametsoc.org/bams/article/100/11/2175/343787/Making-the-Black-Box-More-Transparent>https://journals.ametsoc.org/bams/article/100/11/2175/343787/Making-the-Black-Box-More-Transparent</a></li><li>#PAPER Towards Explainable Artificial Intelligence (Samek & Muller 2019): <a href=https://arxiv.org/abs/1909.12072>https://arxiv.org/abs/1909.12072</a></li><li>#PAPER Explaining Explanations: An Overview of Interpretability of Machine Learning (Gilpin et al. 2019): <a href=https://arxiv.org/abs/1806.00069>https://arxiv.org/abs/1806.00069</a></li><li>#PAPER One Explanation Does Not Fit All: A Toolkit and Taxonomy of AI Explainability Techniques (Arya 2019): <a href=https://arxiv.org/abs/1909.03012>https://arxiv.org/abs/1909.03012</a><ul><li>#CODE [[#^aix360]]</li></ul></li><li>#PAPER Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead (Rudin 2019): <a href=https://www.nature.com/articles/s42256-019-0048-x>https://www.nature.com/articles/s42256-019-0048-x</a><ul><li><a href=https://arxiv.org/abs/1811.10154>https://arxiv.org/abs/1811.10154</a></li></ul></li><li>#PAPER Explainable Machine Learning for Scientific Insights and Discoveries (Roscher 2020): <a href=https://arxiv.org/abs/1905.08883>https://arxiv.org/abs/1905.08883</a></li><li>#PAPER Review Study of Interpretation Methods for Future Interpretable Machine Learning (Jian-Xun 2020): <a href=https://ieeexplore.ieee.org/document/9234594>https://ieeexplore.ieee.org/document/9234594</a></li></ul><h2 id=model-agnostic-methods>Model-agnostic methods</h2><ul><li><p><a href=https://christophm.github.io/interpretable-ml-book/agnostic.html>https://christophm.github.io/interpretable-ml-book/agnostic.html</a></p><ul><li>The great advantage of model-agnostic interpretation methods over model-specific ones is their flexibility</li><li>An alternative to model-agnostic interpretation methods is to use only interpretable models, which often has the big disadvantage that predictive performance is lost compared to other machine learning models and you limit yourself to one type of model</li></ul></li><li><p>#PAPER Model-Agnostic Interpretability of Machine Learning (Tulio Ribeiro 2016): <a href=https://arxiv.org/abs/1606.05386>https://arxiv.org/abs/1606.05386</a></p></li><li><p>#PAPER SHAP - A Unified Approach to Interpreting Model Predictions (Lundberg 2017): <a href=https://arxiv.org/abs/1705.07874>https://arxiv.org/abs/1705.07874</a></p><ul><li>SHAP (SHapley Additive exPlanations)</li><li>#CODE [[#^shapgithub]]</li><li>Can be used for computer vision tasks</li></ul></li><li><p>#PAPER Peeking Inside the Black Box: Visualizing Statistical Learning With Plots of Individual Conditional Expectation (Goldstein 2014): <a href=https://arxiv.org/abs/1309.6392>https://arxiv.org/abs/1309.6392</a></p></li></ul><h3 id=partial-dependence-plot>Partial Dependence Plot</h3><ul><li>The partial dependence plot (short PDP or PD plot) shows the marginal effect one or two features have on the predicted outcome of a machine learning model</li><li><a href=https://christophm.github.io/interpretable-ml-book/pdp.html>https://christophm.github.io/interpretable-ml-book/pdp.html</a></li><li><a href=https://scikit-learn.org/stable/modules/partial_dependence.html>https://scikit-learn.org/stable/modules/partial_dependence.html</a></li></ul><h3 id=individual-conditional-expectation>Individual Conditional Expectation</h3><ul><li>Individual Conditional Expectation (ICE) plots display one line per instance that shows how the instance&rsquo;s prediction changes when a feature changes</li><li><a href=https://christophm.github.io/interpretable-ml-book/ice.html>https://christophm.github.io/interpretable-ml-book/ice.html</a></li><li><a href=https://scikit-learn.org/stable/modules/partial_dependence.html>https://scikit-learn.org/stable/modules/partial_dependence.html</a></li></ul><h3 id=permutation-feature-importance>Permutation Feature Importance</h3><ul><li>Permutation feature importance measures the increase in the prediction error of the model after we permuted the feature&rsquo;s values, which breaks the relationship between the feature and the true outcome</li><li><a href=https://christophm.github.io/interpretable-ml-book/feature-importance.html>https://christophm.github.io/interpretable-ml-book/feature-importance.html</a></li><li><a href=https://scikit-learn.org/stable/modules/permutation_importance.html>https://scikit-learn.org/stable/modules/permutation_importance.html</a><ul><li>The permutation feature importance is defined to be the decrease in a model score when a single feature value is randomly shuffled</li><li>Tree-based models provide an alternative measure of feature importances based on the mean decrease in impurity (MDI). Impurity is quantified by the splitting criterion of the decision trees (Gini, Entropy or Mean Squared Error). However, this method can give high importance to features that may not be predictive on unseen data when the model is overfitting. Permutation-based feature importance, on the other hand, avoids this issue, since it can be computed on unseen data</li></ul></li></ul><h3 id=surrogate-models>Surrogate models</h3><ul><li>A surrogate model is a simple model that is used to explain a complex model. Surrogate models are usually created by training a linear regression or decision tree on the original inputs and predictions of a complex model. Coefficients, variable importance, trends, and interactions displayed in the surrogate model are then assumed to be indicative of the internal mechanisms of the complex model. There are few, possibly no, theoretical guarantees that the simple surrogate model is highly representative of the more complex model.</li><li>The globally interpretable attributes of a simple model are used to explain global attributes of a more complex model. However, there is nothing to preclude fitting surrogate models to more local regions of a complex model&rsquo;s conditional distribution, such as clusters of input records and their corresponding predictions and their corresponding input rows. Because small sections of the conditional distribution are more likely to be linear, monotonic, or otherwise well-behaved, local surrogate models can be more accurate than global surrogate models.</li><li>#PAPER LIME - &ldquo;Why Should I Trust You?": Explaining the Predictions of Any Classifier (2016). <a href=https://arxiv.org/abs/1602.04938>https://arxiv.org/abs/1602.04938</a> ^lime<ul><li>#CODE [[#^limegithub]]</li><li>Formalized approach for local surrogate models. It is meant to shed light on how decisions are made for specific observations. LIME requires that a set of explainable records be found, simulated, or created.</li><li><a href=https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime>https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime</a></li><li><a href=https://github.com/albahnsen/Talk_Demystifying_Machine_Learning>https://github.com/albahnsen/Talk_Demystifying_Machine_Learning</a></li><li>Interpreting ML models prediction power: <a href=http://www.datasciencecentral.com/profiles/blogs/deep-learning-epic-fail-right-answer-wrong-reason>http://www.datasciencecentral.com/profiles/blogs/deep-learning-epic-fail-right-answer-wrong-reason</a></li><li><a href=https://medium.com/@ageitgey/natural-language-processing-is-fun-part-3-explaining-model-predictions-486d8616813c>https://medium.com/@ageitgey/natural-language-processing-is-fun-part-3-explaining-model-predictions-486d8616813c</a></li><li>Machine Learning Explained - Easysol: <a href=http://blog.easysol.net/machine_learning_explained/>http://blog.easysol.net/machine_learning_explained/</a></li><li><a href=https://www.slideshare.net/albahnsen/demystifying-machine-learning-using-lime>https://www.slideshare.net/albahnsen/demystifying-machine-learning-using-lime</a></li><li><a href=https://github.com/albahnsen/Talk_Demystifying_Machine_Learning>https://github.com/albahnsen/Talk_Demystifying_Machine_Learning</a></li></ul></li></ul><h2 id=maximum-activation-analysis>Maximum activation analysis</h2><ul><li>See
<a href=#Neural%20Networks%20explainability rel=noopener class=internal-link data-src=#Neural%20Networks%20explainability>Neural Networks explainability</a></li><li>In maximum activation analysis, examples are found or simulated that maximally activate certain neurons, layers, or filters in a neural network or certain trees in decision tree ensembles. For the purposes of maximum activation analysis, low residuals for a certain tree are analogous to high-magnitude neuron output in a neural network.</li><li>Maximum activation analysis elucidates internal mechanisms of complex models by determining the parts of the response function that specific observations or groups of similar observations excite to the highest degree, either by high-magnitude output from neurons or by low residual output from trees.</li></ul><h2 id=sensitivity-analysis>Sensitivity analysis</h2><ul><li>See
<a href=#Neural%20Networks%20explainability rel=noopener class=internal-link data-src=#Neural%20Networks%20explainability>Neural Networks explainability</a></li><li>Sensitivity analysis investigates whether model behavior and outputs remain stable when data is intentionally perturbed or other changes are simulated in data.</li><li>Beyond traditional assessment practices, sensitivity analysis of machine learning model predictions is perhaps the most important validation technique for machine learning models.</li><li>Machine learning models can make drastically differing predictions from minor changes in input variable values. In practice, many linear model validation techniques focus on the numerical instability of regression parameters due to correlation between input variables or between input variables and the dependent variable</li><li>Sensitivity analysis can also test model behavior and outputs when interesting situations or known corner cases are simulated. Output distributions, error measurements, plots, and interpretation techniques can be used to explore the way models behave in important scenarios, how they change over time, or if models remain stable when data is subtly and intentionally corrupted</li></ul><h2 id=variable-importance-measures>Variable importance measures</h2><ul><li>Variable importance measures are typically seen in tree-based models but are sometimes also reported for other models.</li><li>A simple heuristic rule for variable importance in a decision tree is related to the depth and frequency at which a variable is split on in a tree, where variables used higher in the tree and more frequently in the tree are more important.</li><li>For a single decision tree, a variable&rsquo;s importance is quantitatively determined by the cumulative change in the splitting criterion for every node in which that variable was chosen as the best splitting candidate.</li><li>For a gradient boosted tree ensemble, variable importance is calculated as it is for a single tree but aggregated for the ensemble.</li><li>For random forests:<ul><li>Variable importance is also calculated as it is for a single tree and aggregated, but an additional measure of variable importance is provided by the change in out-of-bag accuracy caused by shuffling the independent variable of interest, where larger decreases in accuracy are taken as larger indications of importance</li><li>The default method to compute variable importance is the mean decrease in impurity (or gini importance) mechanism: At each split in each tree, the improvement in the split-criterion is the importance measure attributed to the splitting variable, and is accumulated over all the trees in the forest separately for each variable. Note that this measure is quite like the R^2 in regression on the training set</li><li>This example highlights the limitations of impurity-based feature importance in contrast to permutation-based feature importance: <a href=https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html>https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html</a></li><li>#PAPER Understanding variable importances in forests of randomized trees (Louppe 2013): <a href=https://papers.nips.cc/paper/4928-understanding-variable-importances-in-forests-of-randomized-trees.pdf>https://papers.nips.cc/paper/4928-understanding-variable-importances-in-forests-of-randomized-trees.pdf</a><ul><li>#POSTER <a href=https://orbi.uliege.be/bitstream/2268/155642/3/poster.pdf>https://orbi.uliege.be/bitstream/2268/155642/3/poster.pdf</a></li></ul></li><li>#PAPER Trees, forests, and impurity-based variable importance (Scornet 2020): <a href=https://arxiv.org/abs/2001.04295>https://arxiv.org/abs/2001.04295</a></li></ul></li><li>For neural networks, variable importance measures are typically associated with the aggregated, absolute magnitude of model parameters for a given variable of interest.</li><li>Global variable importance techniques are typically model specific, and practitioners should be aware that unsophisticated measures of variable importance can be biased toward larger scale variables or variables with a high number of categories.</li></ul><h2 id=neural-networks-explainability>Neural Networks explainability</h2><h3 id=resources-1>Resources</h3><ul><li>Using ML to Explore Neural Network Architecture: <a href=https://research.googleblog.com/2017/05/using-machine-learning-to-explore.html>https://research.googleblog.com/2017/05/using-machine-learning-to-explore.html</a></li><li>The Building Blocks of Interpretability: <a href=https://distill.pub/2018/building-blocks/>https://distill.pub/2018/building-blocks/</a></li><li>Feature Visualization: <a href=https://distill.pub/2017/feature-visualization/>https://distill.pub/2017/feature-visualization/</a></li><li><a href=https://medium.com/merantix/applying-deep-learning-to-real-world-problems-ba2d86ac5837 rel=noopener>Applying deep learning to real-world problems (labeled data, imbalance, black box models)</a></li><li>Unblackboxing webinar (deepsense.io): <a href=https://github.com/deepsense-io/unblackboxing_webinar>https://github.com/deepsense-io/unblackboxing_webinar</a></li><li>The Dark Secret at the Heart of AI: <a href=https://www.technologyreview.com/s/604087/the-dark-secret-at-the-heart-of-ai/amp/>https://www.technologyreview.com/s/604087/the-dark-secret-at-the-heart-of-ai/amp/</a></li><li><a href=http://www.sciencemag.org/news/2017/07/how-ai-detectives-are-cracking-open-black-box-deep-learning rel=noopener>How AI detectives are cracking open the black box of deep learning</a></li><li><a href=https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html rel=noopener>Visualization of activations and filters</a><ul><li><a href=https://github.com/jacobgil/keras-filter-visualization>https://github.com/jacobgil/keras-filter-visualization</a></li></ul></li><li><a href=https://towardsdatascience.com/understanding-your-convolution-network-with-visualizations-a4883441533b>https://towardsdatascience.com/understanding-your-convolution-network-with-visualizations-a4883441533b</a></li><li><a href=https://imatge.upc.edu/web/publications/visual-saliency-prediction-using-deep-learning-techniques>https://imatge.upc.edu/web/publications/visual-saliency-prediction-using-deep-learning-techniques</a></li><li><a href=http://www.unofficialgoogledatascience.com/2017/03/attributing-deep-networks-prediction-to.html rel=noopener>Attributing a deep networkâ€™s prediction to its input features</a><ul><li>Integrated gradients method</li><li>It involves a few calls to a gradient operator yielding insightful results for a variety of deep networks</li></ul></li><li>Pixel Attribution (Saliency Maps): <a href=https://christophm.github.io/interpretable-ml-book/pixel-attribution.html>https://christophm.github.io/interpretable-ml-book/pixel-attribution.html</a></li></ul><h3 id=code-1>Code</h3><ul><li>#CODE TruLens (tf.keras and pytorch): Explainability for Neural Networks: <a href=https://github.com/truera/trulens>https://github.com/truera/trulens</a><ul><li><a href=https://www.trulens.org/>https://www.trulens.org/</a></li></ul></li><li>#CODE Captum (pytorch): <a href=https://github.com/pytorch/captum>https://github.com/pytorch/captum</a><ul><li>Interpretability of models across modalities including vision, text, and more</li><li><a href=https://captum.ai/>https://captum.ai/</a></li><li><a href=https://captum.ai/api/>https://captum.ai/api/</a></li></ul></li><li>#CODE Saliency: <a href=https://github.com/PAIR-code/saliency>https://github.com/PAIR-code/saliency</a><ul><li>XRAI, SmoothGrad, Vanilla Gradients, Guided Backpropogation, Integrated Gradients, Occlusion, Grad-CAM, Blur IG</li></ul></li><li>#CODE iNNvestigate: <a href=https://github.com/albermax/innvestigate>https://github.com/albermax/innvestigate</a> ^innvestigate<ul><li>Vanilla gradient, SmoothGrad, DeConvNet, Guided BackProp, PatternNet, DeepTaylor, PatternAttribution, LRP, IntegratedGradients, DeepLIFT</li></ul></li><li>#CODE TF-explain: <a href=https://github.com/sicara/tf-explain>https://github.com/sicara/tf-explain</a><ul><li>implements interpretability methods as Tensorflow 2.x callbacks to ease neural network&rsquo;s understanding</li></ul></li><li>#CODE TensorSpace (Tensorflow.js): <a href=https://github.com/tensorspace-team/tensorspace>https://github.com/tensorspace-team/tensorspace</a><ul><li>Neural network 3D visualization framework</li><li><a href=https://tensorspace.org>https://tensorspace.org</a></li></ul></li><li>#CODE Lucid (Tensorflow 1) - A collection of infrastructure and tools for research in neural network interpretability. <a href=https://github.com/tensorflow/lucid>https://github.com/tensorflow/lucid</a></li><li>#CODE tf-keras-vis: <a href=https://github.com/keisen/tf-keras-vis>https://github.com/keisen/tf-keras-vis</a><ul><li>Neural network visualization toolkit for tf.keras</li><li>Activation Maximization</li><li>Class Activation Maps (GradCAM, GradCAM++, ScoreCAM, Faster-ScoreCAM)</li><li>Saliency Maps (Vanilla Saliency, SmoothGrad)</li></ul></li><li>#CODE Keras-vis: <a href=https://github.com/raghakot/keras-vis>https://github.com/raghakot/keras-vis</a><ul><li><a href=https://raghakot.github.io/keras-vis/>https://raghakot.github.io/keras-vis/</a></li><li>Activation maximization, Saliency maps, Class activation maps</li></ul></li><li>#CODE DeepExplain (TensorFlow 1): <a href=https://github.com/marcoancona/DeepExplain>https://github.com/marcoancona/DeepExplain</a><ul><li>Saliency maps, Gradient * Input, Integrated Gradients, DeepLIFT, Îµ-LRP</li></ul></li><li>#CODE LRP toolbox: <a href=https://github.com/sebastian-lapuschkin/lrp_toolbox>https://github.com/sebastian-lapuschkin/lrp_toolbox</a></li></ul><h3 id=references-1>References</h3><ul><li>#PAPER
<a href=https://www.researchgate.net/publication/3623243_Visualization_of_neural_networks_using_saliency_maps rel=noopener>Visualization of neural networks using saliency maps (Morch 1995)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1312.6034 rel=noopener>Deep inside CNNs: Visualising Image Classification Models and Saliency Maps (Simonyan 2014)</a><ul><li>Presented two visualisation techniques for deep classification ConvNets<ul><li>The first generates an artificial image, which is representative of a class of interest</li><li>The second computes an image-specific class saliency map, highlighting the areas of the given image, discriminative wrt the given class</li></ul></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1506.06579 rel=noopener>Understanding Neural Networks Through Deep Visualization (Yosinski 2015)</a><ul><li><a href=http://yosinski.com/deepvis>http://yosinski.com/deepvis</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1706.05806 rel=noopener>SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability (Raghu 2017)</a><ul><li>Interpreting Deep Neural Networks with SVCCA: <a href=https://ai.googleblog.com/2017/11/interpreting-deep-neural-networks-with.html>https://ai.googleblog.com/2017/11/interpreting-deep-neural-networks-with.html</a></li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1703.01365 rel=noopener>Axiomatic Attribution for Deep Networks (Sundararajan 2017)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1706.03825 rel=noopener>SmoothGrad: removing noise by adding noise (Smilkov 2017)</a><ul><li><a href=https://pair-code.github.io/saliency/>https://pair-code.github.io/saliency/</a></li></ul></li><li>#PAPER
<a href=http://arxiv.org/abs/1808.04260 rel=noopener>iNNvestigate Neural Networks! (Alber 2018)</a><ul><li>#CODE [[#^innvestigate]]</li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/1906.02825 rel=noopener>XRAI: Better Attributions Through Regions (Kapishnikov 2019)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1704.02685 rel=noopener>DeepLIFT - Learning Important Features Through Propagating Activation Differences (Shrikumar 2019)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1810.03716 rel=noopener>Saliency Prediction in the Deep Learning Era: Successes, Limitations, and Future Challenges (Borji 2019)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/2012.05766 rel=noopener>DAX: Deep Argumentative eXplanation for Neural Networks (Albini 2020)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1901.09839 rel=noopener>Interpreting Deep Neural Networks Through Variable Importance (Ish-Horowicz 2020)</a><ul><li>Their strategy is specifically designed to leverage partial covariance structures and incorporate variable interactions into our proposed feature ranking.</li><li>Extended the recently proposed â€œRelATive cEntralityâ€ (RATE) measure (Crawford et al., 2019) to the Bayesian deep learning setting</li><li>Given a trained network, RATE applies an information theoretic criterion to the posterior distribution of effect sizes to assess feature significance</li></ul></li><li>#PAPER
<a href=https://link.springer.com/chapter/10.1007%2F978-3-030-58574-7_20 rel=noopener>Determining the Relevance of Features for Deep Neural Networks (Reimers 2020)</a><ul><li>Their approach builds upon concepts from causal inference</li><li>Interpret machine learning in a structural causal model and use Reichenbachâ€™s common cause principle to infer whether a feature is relevant</li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2005.13799 rel=noopener>Explainable Deep Learning Models in Medical Image Analysis (Singh 2020)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1911.11293 rel=noopener>Efficient Saliency Maps for Explainable AI (Mundhenk 2020)</a></li><li>#PAPER
<a href=https://ieeexplore.ieee.org/document/9369420 rel=noopener>Explaining Deep Neural Networks and Beyond: A Review of Methods and Applications (Samek 2021)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/2108.05149 rel=noopener>Logic Explained Networks (Ciravegna 2021)</a><ul><li><a href=https://syncedreview.com/2021/08/18/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-85/>https://syncedreview.com/2021/08/18/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-85/</a></li></ul></li><li>#PAPER Toward Explainable AI for Regression Models (Letzgus 2021): <a href=https://arxiv.org/abs/2112.11407>https://arxiv.org/abs/2112.11407</a></li></ul><h4 id=layer-wise-relevance-propagation-lrp>Layer-wise Relevance Propagation (LRP)</h4><ul><li><p>LRP is an inverse method which calculates the contribution of a single pixel to the prediction made by a DNN in an image classification task</p></li><li><p><a href=http://heatmapping.org/>http://heatmapping.org/</a></p></li><li><p>Interactive demo: <a href=https://lrpserver.hhi.fraunhofer.de/image-classification>https://lrpserver.hhi.fraunhofer.de/image-classification</a></p></li><li><p><a href=https://medium.com/@ODSC/layer-wise-relevance-propagation-means-more-interpretable-deep-learning-219ff5158914>https://medium.com/@ODSC/layer-wise-relevance-propagation-means-more-interpretable-deep-learning-219ff5158914</a></p></li><li><p><a href=https://towardsdatascience.com/indepth-layer-wise-relevance-propagation-340f95deb1ea>https://towardsdatascience.com/indepth-layer-wise-relevance-propagation-340f95deb1ea</a></p></li><li><p>Saliency map is a broader term from the field of computer vision (<a href=https://en.wikipedia.org/wiki/Saliency>https://en.wikipedia.org/wiki/Saliency</a>_map). The first reference of saliency maps applied to the predictions of DNNs is Morch et al 1995. Simonyan et al (2014) first proposed a method to produce saliency maps using back-propagation through a CNN, but note that you could compute &ldquo;saliency&rdquo; from an image in many ways that do not deal with back-propagating the prediction scores of DNNs.</p></li><li><p>There are several approaches for calculating attributions by back-propagating the prediction score through each layer of the network, back to the input features /pixels (DeConvNet, SmoothGrad, GradCam, LRP, XRAI). LRP is just one of them. In the first LRP paper, they talk about heatmaps or relevance maps, probably to avoid confusion with older saliency map techniques</p></li><li><p>#PAPER On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation (Bach 2015): <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140">https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140</a></p><ul><li>#CODE Tutorial: Implementing Layer-Wise Relevance Propagation: <a href=https://git.tu-berlin.de/gmontavon/lrp-tutorial>https://git.tu-berlin.de/gmontavon/lrp-tutorial</a></li></ul></li><li><p>#PAPER Understanding Individual Decisions of CNNs via Contrastive Backpropagation (Gu 2019): <a href=https://arxiv.org/abs/1812.02100>https://arxiv.org/abs/1812.02100</a></p></li><li><p>#PAPER Beyond saliency: understanding convolutional neural networks from saliency prediction on layer-wise relevance propagation (Li 2019): <a href=https://arxiv.org/abs/1712.08268>https://arxiv.org/abs/1712.08268</a></p><ul><li>proposed a novel two-step understanding method, namely Salient Relevance (SR) map, which aims to shed light on how deep CNNs recognize images and learn features from attention areas</li><li>starts out with a layer-wise relevance propagation (LRP) step which estimates a pixel-wise relevance map over the input image. Following, we construct a context-aware saliency map, SR map, from the LRP-generated map which predicts areas close to the foci of attention instead of isolated pixels that LRP reveals</li></ul></li><li><p>#PAPER Towards Best Practice in Explaining Neural Network Decisions with LRP (Kohlbrenner 2020): <a href=https://arxiv.org/abs/1910.09840>https://arxiv.org/abs/1910.09840</a></p></li></ul></article><hr><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script>async function run(){const{index:a,links:p,content:u}=await fetchData(),j="https://carlos-gg.github.io/AIDigitalGarden/AI/XAI".replace("https://carlos-gg.github.io/AIDigitalGarden",""),n=[{"/moc":"#4388cc"}];let h=-1;const m=a=>[...new Set(a.flatMap(a=>[a.source,a.target]))],e=new Set,f=[j||"/","__SENTINEL"];if(h>=0)while(h>=0&&f.length>0){const b=f.shift();if(b==="__SENTINEL")h--,f.push("__SENTINEL");else{e.add(b);const c=a.links[b]||[],d=a.backlinks[b]||[];f.push(...c.map(a=>a.target),...d.map(a=>a.source))}}else m(p).forEach(a=>e.add(a));const g={nodes:[...e].map(a=>({id:a})),links:p.filter(a=>e.has(a.source)&&e.has(a.target))},k=a=>{if(a.id===j||a.id==="/"&&j==="")return"var(--g-node-active)";for(const b of n){const c=Object.keys(b)[0],d=b[c];if(a.id.startsWith(c))return d}return"var(--g-node)"},l=c=>{function d(b,a){b.active||c.alphaTarget(1).restart(),a.fx=a.x,a.fy=a.y}function e(a,b){b.fx=a.x,b.fy=a.y}function f(b,a){b.active||c.alphaTarget(0),a.fx=null,a.fy=null}const a=!0,b=()=>{};return d3.drag().on("start",a?d:b).on("drag",a?e:b).on("end",a?f:b)},c=250,b=document.getElementById("graph-container").offsetWidth,i=d3.forceSimulation(g.nodes).force("charge",d3.forceManyBody().strength(-30)).force("link",d3.forceLink(g.links).id(a=>a.id)).force("center",d3.forceCenter()),d=d3.select('#graph-container').append('svg').attr('width',b).attr('height',c).attr("viewBox",[-b/2,-c/2,b,c]),t=!1;if(t){const a=[{Current:"var(--g-node-active)"},{Note:"var(--g-node)"},...n];a.forEach((a,e)=>{const f=Object.keys(a)[0],g=a[f];d.append("circle").attr("cx",-b/2+20).attr("cy",c/2-30*(e+1)).attr("r",6).style("fill",g),d.append("text").attr("x",-b/2+40).attr("y",c/2-30*(e+1)).text(f).style("font-size","15px").attr("alignment-baseline","middle")})}const r=d.append("g").selectAll("line").data(g.links).join("line").attr("class","link").attr("stroke","var(--g-link)").attr("stroke-width",2).attr("data-source",a=>a.source.id).attr("data-target",a=>a.target.id),s=d.append("g").selectAll("g").data(g.nodes).enter().append("g"),q=s.append("circle").attr("class","node").attr("id",a=>a.id).attr("r",b=>{const c=a.links[b.id]?.length||0,d=a.backlinks[b.id]?.length||0;return 3+(c+d)/4}).attr("fill",k).style("cursor","pointer").on("click",(b,a)=>{window.location.href="https://carlos-gg.github.io/AIDigitalGarden"+decodeURI(a.id).replace(/\s+/g,'-')}).on("mouseover",function(g,b){d3.selectAll(".node").transition().duration(100).attr("fill","var(--g-node-inactive)");const d=m([...a.links[b.id]||[],...a.backlinks[b.id]||[]]),e=d3.selectAll(".node").filter(a=>d.includes(a.id)),c=b.id,f=d3.selectAll(".link").filter(a=>a.source.id===c||a.target.id===c);e.transition().duration(200).attr("fill",k),f.transition().duration(200).attr("stroke","var(--g-link-active)"),d3.select(this.parentNode).select("text").raise().transition().duration(200).style("opacity",1)}).on("mouseleave",function(d,b){d3.selectAll(".node").transition().duration(200).attr("fill",k);const a=b.id,c=d3.selectAll(".link").filter(b=>b.source.id===a||b.target.id===a);c.transition().duration(200).attr("stroke","var(--g-link)"),d3.select(this.parentNode).select("text").transition().duration(200).style("opacity",0)}).call(l(i)),o=s.append("text").attr("dx",12).attr("dy",".35em").text(a=>u[decodeURI(a.id).replace(/\s+/g,'-')]?.title||"Untitled").style("opacity",0).style("pointer-events","none").call(l(i)),v=!0;v&&d.call(d3.zoom().extent([[0,0],[b,c]]).scaleExtent([.25,4]).on("zoom",({transform:a})=>{r.attr("transform",a),q.attr("transform",a),o.attr("transform",a)})),i.on("tick",()=>{r.attr("x1",a=>a.source.x).attr("y1",a=>a.source.y).attr("x2",a=>a.target.x).attr("y2",a=>a.target.y),q.attr("cx",a=>a.x).attr("cy",a=>a.y),o.attr("x",a=>a.x).attr("y",a=>a.y)})}run()</script></div></div><div id=contact_buttons><footer><p>Made by Carlos Alberto Gomez Gonzalez using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2022</p><a href=/>Home</a>
<a href=https://carlos-gg.github.io>Website</a><a href=https://github.com/carlos-gg>Github</a></footer></div></div></body></html>