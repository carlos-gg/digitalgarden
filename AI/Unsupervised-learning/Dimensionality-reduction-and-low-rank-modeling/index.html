<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Resources and references   The Beginner&rsquo;s Guide to Dimensionality Reduction  Distances, Neighborhoods, or Dimensions? Projection Literacy for the Analysis of Multivariate Data  Matrix Factorization: A Simple Tutorial and Implementation in Python  Sklearn - Decomposing signals in components (matrix factorization problems) Projection techniques transform high-dimensional data to a lower-dimensional space while preserving its main structure."><title>Dimensionality reduction and low-rank modeling</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=/icon.png><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Source+Sans+Pro:wght@400;600;700&family=Fira+Code:wght@400;700&display=swap" rel=stylesheet><link href=https://carlos-gg.github.io/digitalgarden/styles.cd61336c89ed6e03702366ce4a492b75.min.css rel=stylesheet><script src=https://carlos-gg.github.io/digitalgarden/js/darkmode.46b07878b7f5d9e26ad7a3c40f8a0605.min.js></script>
<script>const BASE_URL="https://carlos-gg.github.io/digitalgarden/",fetchData=Promise.all([fetch("https://carlos-gg.github.io/digitalgarden/indices/linkIndex.2993af0123e17f8315a1f541fff911a6.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://carlos-gg.github.io/digitalgarden/indices/contentIndex.e009ea308e1965076a177bd16cca870e.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n}))</script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-S103D50NQ0"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S103D50NQ0",{anonymize_ip:!1})}</script><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://carlos-gg.github.io/digitalgarden/js/graph.27e8521c25c27c79dea35f434c486167.js></script>
<script>drawGraph("https://carlos-gg.github.io/digitalgarden/AI/Unsupervised-learning/Dimensionality-reduction-and-low-rank-modeling","https://carlos-gg.github.io/digitalgarden",[{"/moc":"#4388cc"}],-1,!0,!1,!0)</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://carlos-gg.github.io/digitalgarden/js/search.bc849b857f2c1b822264d40635bb67b6.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://carlos-gg.github.io/digitalgarden/>CarlosGG's Knowledge Garden 🪴</a></h1><svg tabindex="0" id="search-icon" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg><div class=spacer></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Dimensionality reduction and low-rank modeling</h1><p class=meta>Last updated August 17, 2022</p><ul class=tags></ul><aside class=mainTOC><details open><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#resources-and-references>Resources and references</a><ol><li><a href=#linear-methods>Linear methods</a><ol><li><a href=#principal-component-analysis-pca>Principal component analysis (PCA)</a></li><li><a href=#non-negative-matrix-factorization-nmf>Non-negative matrix factorization (NMF)</a></li><li><a href=#generalized-low-rank-models>Generalized Low Rank Models</a></li><li><a href=#dynamic-mode-decomposition>Dynamic mode decomposition</a></li></ol></li><li><a href=#non-linear-methods>Non-linear methods</a><ol><li><a href=#multidimensional-scaling-mds>Multidimensional scaling (MDS)</a></li><li><a href=#self-organizing-maps-som>Self organizing maps (SOM)</a></li><li><a href=#t-distributed-stochastic-neighbor-embedding-t-sne>T-distributed Stochastic Neighbor Embedding (t-SNE)</a></li><li><a href=#uniform-manifold-approximation-and-projection-umap>Uniform Manifold Approximation and Projection (UMAP)</a></li></ol></li></ol></li><li><a href=#code>Code</a></li></ol></nav></details></aside><h2 id=resources-and-references>Resources and references</h2><ul><li><a href=https://idyll.pub/post/visxai-dimensionality-reduction-1dbad0a67a092b007c526a45/ rel=noopener>The Beginner&rsquo;s Guide to Dimensionality Reduction</a></li><li><a href=https://visxprojections.dbvis.de/client/index.html rel=noopener>Distances, Neighborhoods, or Dimensions? Projection Literacy for the Analysis of Multivariate Data</a></li><li><a href=http://www.quuxlabs.com/blog/2010/09/matrix-factorization-a-simple-tutorial-and-implementation-in-python/ rel=noopener>Matrix Factorization: A Simple Tutorial and Implementation in Python</a></li><li><a href=https://scikit-learn.org/stable/modules/decomposition.html rel=noopener>Sklearn - Decomposing signals in components (matrix factorization problems)</a></li><li>Projection techniques transform high-dimensional data to a lower-dimensional space while preserving its main structure. Often, the data is transformed to two-dimensional space and visualized as a scatter plot as a means to analyze and understand the data</li><li>Two categories: linear and non-linear projection techniques.</li></ul><h3 id=linear-methods>Linear methods</h3><ul><li>Linear projection techniques produce a linear transformation of data dimensions in lower-dimensional space. Proximity between data points indicates similarity. The more similar data points are, the closer they are located to each other and vice versa. This is why linear projection techniques are also known as global techniques.</li></ul><h4 id=principal-component-analysis-pca>Principal component analysis (PCA)</h4><p>See <a href=/digitalgarden/AI/Unsupervised-learning/PCA rel=noopener class=internal-link data-src=/digitalgarden/AI/Unsupervised-learning/PCA>PCA</a></p><h4 id=non-negative-matrix-factorization-nmf>Non-negative matrix factorization (NMF)</h4><ul><li>Non-negative matrix factorization (NNMF, or NMF) is a method for factorizing a matrix into two lower rank matrices with strictly non-negative elements.</li><li><a href=https://en.wikipedia.org/wiki/Non-negative_matrix_factorization rel=noopener>https://en.wikipedia.org/wiki/Non-negative_matrix_factorization</a></li><li><a href=https://yliapis.github.io/Non-Negative-Matrix-Factorization/ rel=noopener>https://yliapis.github.io/Non-Negative-Matrix-Factorization/</a></li><li><a href=https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html rel=noopener>https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html</a></li><li><a href=https://scikit-learn.org/stable/auto_examples/decomposition/plot_faces_decomposition.html#sphx-glr-auto-examples-decomposition-plot-faces-decomposition-py rel=noopener>https://scikit-learn.org/stable/auto_examples/decomposition/plot_faces_decomposition.html#sphx-glr-auto-examples-decomposition-plot-faces-decomposition-py</a></li></ul><h4 id=generalized-low-rank-models>Generalized Low Rank Models</h4><ul><li>Extension of the idea of PCA to handle arbitrary data sets consisting of numerical, Boolean, categorical, ordinal, and other data types. This framework encompasses many well known techniques in data analysis, such as nonnegative matrix factorization, matrix completion, sparse and robust PCA,-means,-SVD, and maximum margin matrix factorization. The method handles heterogeneous data sets, and leads to coherent schemes for compressing, denoising, and imputing missing entries across all data types simultaneously. It also admits a number of interesting interpretations of the low rank factors, which allow clustering of examples or of features.<ul><li><a href=https://github.com/cehorn/GLRM rel=noopener>https://github.com/cehorn/GLRM</a></li><li>#TALK
<a href="https://www.youtube.com/watch?v=zwvzGuS82MA" rel=noopener>Generalized Low Rank Models - Madeleine Udell</a></li><li>#TALK Introduction to generalized low-rank models and missing values (OREILLY):<ul><li><a href=https://conferences.oreilly.com/strata/strata-eu-2016/public/schedule/detail/49771 rel=noopener>https://conferences.oreilly.com/strata/strata-eu-2016/public/schedule/detail/49771</a></li><li><a href=http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/glrm.html rel=noopener>http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/glrm.html</a></li></ul></li></ul></li></ul><h4 id=dynamic-mode-decomposition>Dynamic mode decomposition</h4><ul><li><a href=https://en.wikipedia.org/wiki/Dynamic_mode_decomposition rel=noopener>https://en.wikipedia.org/wiki/Dynamic_mode_decomposition</a></li><li>linear dimensionality reduction technique for high-dimensional time-series originating from fluid dynamics. DMD combines the best of two worlds: PCA and Fourier transform. Mathematically, it is related to a fundamental operator in dynamical system theory known as the Koopman operator</li><li><a href=https://towardsdatascience.com/a-case-against-pca-for-time-series-analysis-ac66b47629e0 rel=noopener>A case against PCA for time-series analysis</a><ul><li>Recent studies have shown that DMD behaves as a source separation algorithm (e.g. ICA), although this framework can be more flexible</li><li>For a similar computational cost, it moreover provides a far more interpretable model than PCA</li></ul></li></ul><h3 id=non-linear-methods>Non-linear methods</h3><ul><li>Non-linear projection techniques, also known as local projection techniques, aim at preserving the local neighborhoods across the features in the data. Hereby, proximity highlights differences and coherences between observations and is not to put on the same level as similarity</li></ul><h4 id=multidimensional-scaling-mds>Multidimensional scaling (MDS)</h4><ul><li>Multidimensional scaling (MDS) is a means of visualizing the level of similarity of individual cases of a dataset. It refers to a set of related ordination techniques used in information visualization, in particular to display the information contained in a distance matrix. It is a form of non-linear dimensionality reduction.</li></ul><h4 id=self-organizing-maps-som>Self organizing maps (SOM)</h4><ul><li><a href=https://en.wikipedia.org/wiki/Self-organizing_map rel=noopener>https://en.wikipedia.org/wiki/Self-organizing_map</a></li><li>unsupervised machine learning technique used to produce a low-dimensional (typically two-dimensional) representation of a higher dimensional data set while preserving the topological structure of the data</li><li><a href=https://stackabuse.com/self-organizing-maps-theory-and-implementation-in-python-with-numpy/ rel=noopener>https://stackabuse.com/self-organizing-maps-theory-and-implementation-in-python-with-numpy/</a></li></ul><h4 id=t-distributed-stochastic-neighbor-embedding-t-sne>T-distributed Stochastic Neighbor Embedding (t-SNE)</h4><ul><li><a href=http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html rel=noopener>http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html</a></li><li>t-SNE is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. with different initializations we can get different results.</li><li>t-SNE is a technique for nonlinear dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets. The technique can be implemented via Barnes-Hut approximations, allowing it to be applied on large real-world datasets. It is particularly well-suited for embedding high-dimensional data into a space of two or three dimensions, which can then be visualized in a scatter plot. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points.</li><li><a href=https://mark-borg.github.io/blog/2016/tsne/ rel=noopener>https://mark-borg.github.io/blog/2016/tsne/</a></li><li><a href=https://blog.alookanalytics.com/2017/02/28/analytical-market-segmentation-with-t-sne-and-clustering-pipeline/ rel=noopener>https://blog.alookanalytics.com/2017/02/28/analytical-market-segmentation-with-t-sne-and-clustering-pipeline/</a></li><li><a href=http://distill.pub/2016/misread-tsne/ rel=noopener>How to Use t-SNE Effectively (Interactive)</a></li></ul><h4 id=uniform-manifold-approximation-and-projection-umap>Uniform Manifold Approximation and Projection (UMAP)</h4><ul><li>#PAPER
<a href=https://arxiv.org/abs/1802.03426 rel=noopener>UMAP - Uniform Manifold Approximation and Projection for Dimension Reduction (McInnes 2020)</a><ul><li>Uniform Manifold Approximation and Projection (UMAP) is a dimension reduction technique that can be used for visualisation similarly to t-SNE, but also for general non-linear dimension reduction</li><li>#CODE
<a href=https://github.com/lmcinnes/umap rel=noopener>Umap</a></li><li><a href=https://umap-learn.readthedocs.io/en/latest/ rel=noopener>https://umap-learn.readthedocs.io/en/latest/</a></li></ul></li><li><a href=https://pair-code.github.io/understanding-umap/ rel=noopener>Understanding UMAP</a><ul><li>Nice interactive visualizations</li></ul></li></ul><h2 id=code>Code</h2><ul><li>#CODE
<a href=https://github.com/marinkaz/nimfa rel=noopener>Nimfa</a></li><li>#CODE
<a href=https://github.com/cthurau/pymf rel=noopener>Pymf</a></li><li>#CODE
<a href=https://github.com/hyperspy/hyperspy rel=noopener>HyperSpy</a><ul><li><a href=https://hyperspy.readthedocs.io/en/stable/user_guide/mva.html rel=noopener>https://hyperspy.readthedocs.io/en/stable/user_guide/mva.html</a></li><li>HyperSpy provides easy access to several “machine learning” algorithms that can be useful when analysing multi-dimensional data. In particular, decomposition algorithms, such as principal component analysis (PCA), or blind source separation (BSS) algorithms, such as independent component analysis (ICA), are available</li></ul></li></ul></article><hr><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/digitalgarden/AI/Math-and-Statistics/Linear-Algebra>Linear Algebra</a></li><li><a href=/digitalgarden/AI/Unsupervised-learning/Unsupervised-learning>Unsupervised learning</a></li></ul></div></div><div id=contact_buttons><footer><p>Made by Carlos Alberto Gomez Gonzalez using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2022</p><ul><li><a href=https://carlos-gg.github.io/digitalgarden/>Home</a></li><a href=https://carlos-gg.github.io>Carlos'Homepage</a></ul></footer></div><script src=https://carlos-gg.github.io/digitalgarden/js/popover.e57188d2e4c06b0654e020b3a734bb62.min.js></script>
<script>initPopover("https://carlos-gg.github.io/digitalgarden")</script></div></body></html>