<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Resources  Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters)."><title>Clustering</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=/icon.png><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Source+Sans+Pro:wght@400;600;700&family=Fira+Code:wght@400;700&display=swap" rel=stylesheet><link href=https://carlos-gg.github.io/digitalgarden/styles.fc271f84b56ab4040866b55bab91ea59.min.css rel=stylesheet><script src=https://carlos-gg.github.io/digitalgarden/js/darkmode.46b07878b7f5d9e26ad7a3c40f8a0605.min.js></script><script>const BASE_URL="https://carlos-gg.github.io/digitalgarden/",fetchData=Promise.all([fetch("https://carlos-gg.github.io/digitalgarden/indices/linkIndex.ec68a1664e9f59f7d667605a294052cc.min.json").then(a=>a.json()).then(a=>({index:a.index,links:a.links})),fetch("https://carlos-gg.github.io/digitalgarden/indices/contentIndex.605d8f8b047cb06ff5894fd9b318ff19.min.json").then(a=>a.json())]).then(([{index:a,links:b},c])=>({index:a,links:b,content:c}))</script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-S103D50NQ0"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-S103D50NQ0',{anonymize_ip:!1})}</script><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://carlos-gg.github.io/digitalgarden/js/graph.9e0d7f85fd8bcbb9da33fd6f8acdfeb9.js></script><script>drawGraph("https://carlos-gg.github.io/digitalgarden/AI/Unsupervised-learning/Clustering","https://carlos-gg.github.io/digitalgarden",[{"/moc":"#4388cc"}],-1,!0,!1,!0)</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script><script defer src=https://carlos-gg.github.io/digitalgarden/js/search.3c0dba36397221e42c2e229db4303dc2.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://carlos-gg.github.io/digitalgarden/>CarlosGG's Knowledge Garden 🪴</a></h1><svg tabindex="0" id="search-icon" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg><div class=spacer></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Clustering</h1><p class=meta>Last updated April 1, 2022</p><ul class=tags></ul><aside class=mainTOC><h3>Table of Contents</h3><nav id=TableOfContents><ol><li><a href=#resources>Resources</a></li><li><a href=#references>References</a></li></ol></nav></aside><h2 id=resources>Resources</h2><ul><li>Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters).</li><li><a href=http://scikit-learn.org/stable/modules/clustering.html>http://scikit-learn.org/stable/modules/clustering.html</a></li><li>Hierarhical clustering: Method of cluster analysis which seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two types:<ul><li>Agglomerative: This is a &ldquo;bottom up&rdquo; approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.</li><li>Divisive: This is a &ldquo;top down&rdquo; approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.</li><li>The results of hierarchical clustering are usually presented in a dendrogram. The complexity of agglomerative clustering is O(n^2log(n)), slow for large data. Divisive clustering with an exhaustive search is O(2^n), which is even worse.</li><li><a href=https://blog.alookanalytics.com/2017/04/11/intuition-vs-unsupervised-learning-agglomerative-clustering-in-practice/>https://blog.alookanalytics.com/2017/04/11/intuition-vs-unsupervised-learning-agglomerative-clustering-in-practice/</a></li><li>Single linkage: Single-linkage clustering is one of several methods of hierarchical clustering. It is based on grouping clusters in bottom-up fashion (agglomerative clustering), at each step combining two clusters that contain the closest pair of elements not yet belonging to the same cluster as each other. A drawback of this method is that it tends to produce long thin clusters in which nearby elements of the same cluster have small distances, but elements at opposite ends of a cluster may be much farther from each other than to elements of other clusters. The naive version has a time complexity of O(n^3). There are improvements: SLINK and Kruskal&rsquo;s algo, with time complexity O(n^2).</li><li>Mean linkage<ul><li><a href=https://en.wikipedia.org/wiki/UPGMA rel=noopener>Unweighted Pair Group Method with Arithmetic Mean</a></li><li><a href=https://en.wikipedia.org/wiki/WPGMA rel=noopener>Weighted Pair Group Method with Arithmetic Mean</a></li></ul></li><li>Ward’s method: Ward&rsquo;s minimum variance criterion minimizes the total within-cluster variance.
To implement this method, at each step find the pair of clusters that leads to minimum increase in total within-cluster variance after merging. This increase is a weighted squared distance between cluster centers. At the initial step, all clusters are singletons (clusters containing a single point).</li><li>Complete linkage: The method is also known as farthest neighbour clustering. At the beginning of the process, each element is in a cluster of its own. The clusters are then sequentially combined into larger clusters until all elements end up being in the same cluster. At each step, the two clusters separated by the shortest distance are combined.
The definition of &lsquo;shortest distance&rsquo; is what differentiates between the different agglomerative clustering methods.
In complete-linkage clustering, the link between two clusters contains all element pairs, and the distance between clusters equals the distance between those two elements (one in each cluster) that are farthest away from each other. Complexity O(n^3), CLINK version with O(n^2).</li></ul></li><li><a href=https://en.wikipedia.org/wiki/K-means_clustering rel=noopener>K-means</a><ul><li>k-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. The standard k-means algorithm is called Lloyd&rsquo;s algorithm:<ul><li>Initialization: Forgy method (taking k random observations from the data set), random partitioning or k-means++ methods</li><li>2 steps: assignment and update (the &ldquo;assignment&rdquo; step is also referred to as expectation step, the &ldquo;update step&rdquo; as maximization step, making this algorithm a variant of the generalized expectation-maximization algorithm):<ul><li>Assignment: Assign each observation to the cluster whose mean yields the least within-cluster sum of squares (WCSS). Also called “Expectation step” because it involves updating our expectation of which cluster each point belongs to</li><li>Update step: Calculate the new means to be the centroids of the observations in the new clusters. This also minimizes the within-cluster sum of squares (WCSS) objective. Also called “Maximization step” because it involves maximizing some fitness function that defines the location of the cluster centers — in this case, that maximization is accomplished by taking a simple mean of the data in each cluster</li></ul></li><li><a href=http://stanford.edu/~cpiech/cs221/handouts/kmeans.html>http://stanford.edu/~cpiech/cs221/handouts/kmeans.html</a></li><li><a href="https://www.youtube.com/watch?v=wGzumILN5ww" rel=noopener>Five Minutes With Ingo - K Means Clustering</a></li></ul></li></ul></li><li>DBSCAN<ul><li><a href=https://en.wikipedia.org/wiki/DBSCAN rel=noopener>Density-based spatial clustering of applications with noise</a></li></ul></li><li><a href=http://hdbscan.readthedocs.io/en/latest/soft_clustering_explanation.html rel=noopener>HDBSCAN</a></li><li>Embeddings<ul><li><a href=http://projector.tensorflow.org>http://projector.tensorflow.org</a></li><li><a href=https://www.tensorflow.org/versions/master/how_tos/embedding_viz/index.html>https://www.tensorflow.org/versions/master/how_tos/embedding_viz/index.html</a></li></ul></li></ul><h2 id=references>References</h2><ul><li>#PAPER
<a href=https://dl.acm.org/doi/10.5555/1283383.1283494 rel=noopener>k-means++: the advantages of careful seeding (Arthur 2007)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/1209.1960 rel=noopener>A Comparative Study of Efficient Initialization Methods for the K-Means Clustering Algorithm (Celebi 2012)</a></li><li>#PAPER
<a href=https://arxiv.org/abs/2005.12320 rel=noopener>SCAN: Learning to Classify Images without Labels (Van Gansbeke 2020)</a><ul><li>#CODE <a href=https://github.com/wvangansbeke/Unsupervised-Classification>https://github.com/wvangansbeke/Unsupervised-Classification</a></li><li><a href="https://www.youtube.com/watch?v=hQEnzdLkPj4" rel=noopener>Paper explained</a></li><li>grouping images into semantically meaningful clusters when ground-truth annotations. This is tackling the task of unsupervised image classification in
<a href=/digitalgarden/AI/Computer-Vision/Computer-vision rel=noopener class=internal-link data-src=/digitalgarden/AI/Computer-Vision/Computer-vision>Computer vision</a></li><li>advocate a two-step approach where feature learning and clustering are decoupled. First, a self-supervised task from representation learning is employed to obtain semantically meaningful features.Second, we use the obtained features as a prior in a learnable clustering approach. In doing so, we remove the ability for cluster learning to depend on low-level features, which is present in current end-to-end learning approaches</li></ul></li><li>#PAPER
<a href=https://arxiv.org/abs/2008.03030 rel=noopener>Deep Robust Clustering by Contrastive Learning (Zhong 2020)</a></li></ul></article><hr><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=https://carlos-gg.github.io/digitalgarden//AI/Unsupervised-learning/Unsupervised-learning>Unsupervised learning</a></li></ul></div></div><div id=contact_buttons><footer><p>Made by Carlos Alberto Gomez Gonzalez using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2022</p><a href=https://carlos-gg.github.io/digitalgarden/>Root</a><a href=https://carlos-gg.github.io>CarlosGG</a></footer></div><script src=https://carlos-gg.github.io/digitalgarden/js/popover.718bae159805a590cda4e9bcb14b9dcc.min.js></script><script>initPopover("https://carlos-gg.github.io/digitalgarden")</script></div></body></html>