---
title: Fine tuning LLMs
---

> 

## Resources
- [Maxime Labonne - A Beginnerâ€™s Guide to LLM Fine-Tuning](https://mlabonne.github.io/blog/posts/A_Beginners_Guide_to_LLM_Finetuning.html)
- [Finetune LLMs on your own consumer hardware using tools from PyTorch and Hugging Face ecosystem | PyTorch](https://pytorch.org/blog/finetune-llms/?hss_channel=lcp-78618366)
- [Fine-Tuning - LlamaIndex](https://docs.llamaindex.ai/en/stable/optimizing/fine-tuning/fine-tuning/)
- [Maxime Labonne - Fine-tune Mistral-7b with Direct Preference Optimization](https://mlabonne.github.io/blog/posts/Fine_tune_Mistral_7b_with_DPO.html)
- [Maxime Labonne - Fine-tune Llama 3 with ORPO](https://mlabonne.github.io/blog/posts/2024-04-19_Fine_tune_Llama_3_with_ORPO.html)
- [Maxime Labonne - Fine-tune Llama 3.1 Ultra-Efficiently with Unsloth](https://mlabonne.github.io/blog/posts/2024-07-29_Finetune_Llama31.html)
- PEFT:
	- [Efficient Fine-tuning with PEFT and LoRA | Niklas Heidloff](https://heidloff.net/article/efficient-fine-tuning-lora/)

## Code
- #CODE [Axolotl](https://github.com/axolotl-ai-cloud/axolotl)
	- Axolotl is a tool designed to streamline the fine-tuning of various AI models, offering support for multiple configurations and architectures
	- [Axolotl](https://axolotl-ai-cloud.github.io/axolotl/)
- #CODE [Unsloth](https://github.com/unslothai/unsloth)
- #CODE [Torchtune](https://github.com/pytorch/torchtune)

## References
- #PAPER [FineTuneBench: How well do commercial fine-tuning APIs infuse knowledge  into LLMs?](https://arxiv.org/pdf/2411.05059)
	- [FineTuneBench](https://kevinwu.ai/StanfordFineTuneBench-web/)


