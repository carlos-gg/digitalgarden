---
title: "Maths and Statistics"
disableToc: false 
---

## Resources
- Statistics cheatsheet: https://stanford.edu/~shervine/teaching/cme-106/cheatsheet-statistics
- https://github.com/rouseguy/intro2stats
- Stanford-cs-229 ML, probability and stats refresher: https://github.com/afshinea/stanford-cs-229-machine-learning/blob/master/en/refresher-probabilities-statistics.pdf
- https://www.khanacademy.org/math/statistics-probability
- http://christopherroach.com/articles/statistics-for-hackers/
- http://students.brown.edu/seeing-theory/
- https://scipy-latinamerica.github.io/revista.io/blog/2018/10/22/probabilidad-y-estadistica-con-python/
- Trigonometry refresher: https://stanford.edu/~shervine/teaching/cme-102/trigonometry

## Books
- #BOOK Essential Mathematics and Statistics for Science (Currell 2009, WILEY): https://www.wiley.com/en-us/Essential+Mathematics+and+Statistics+for+Science%2C+2nd+Edition-p-9780470694480
	- http://www.stewartschultz.com/statistics/books/Essential%20Mathematics.pdf
- #BOOK Think Stats - Exploratory Data Analysis in Python (Downey 2014): https://greenteapress.com/wp/think-stats-2e/
	- Think Stats is an introduction to Probability and Statistics for Python programmers
- #BOOK An Introduction to Statistics with Python (Haslwanter, 2015 6 SPRINGER): https://www.springer.com/fr/book/9783319283159
	- Applications in the life sciences
	- https://es.scribd.com/document/338198132/An-Introduction-to-Statistics-With-Python-With-Applications-in-the-Life-Sciences
- #BOOK Statistical Thinking for the 21st Century (Poldrack 2018): http://statsthinking21.org/index.html
	- R language
- #BOOK Jupyter Guide to Linear Algebra: https://bvanderlei.github.io/jupyter-guide-to-linear-algebra/intro.html

## Courses
- #COURSE Statistical inference for data science: https://www.coursera.org/learn/statistical-inference
	- https://leanpub.com/LittleInferenceBook
	- Coursera Inference Version 3: https://www.youtube.com/playlist?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ
- #COURSE Probability and Statistics (Stanford online): https://lagunita.stanford.edu/courses/course-v1:OLI+ProbStat+Open_Jan2017/about
- #COURSE Modern Applied Statistics: Elements of Statistical Learning (Statistics 315a, Stanford): http://statweb.stanford.edu/~tibs/stat315a/
- #COURSE Introduction to Probability and Statistics (MIT): https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/
- #TALK Statistics in Python (Varoquaux 2015 Euroscipy): https://www.youtube.com/watch?v=yaSgoGLXKOg

## Code
- #CODE Numpy: https://numpy.org/
	- #PAPER Array programming with NumPy (Harris 2020): https://www.nature.com/articles/s41586-020-2649-2
- #CODE Scipy: https://www.scipy.org/
- #CODE Statsmodels: http://www.statsmodels.org/
- #CODE PyLops: https://github.com/PyLops/pylops
	- https://pylops.readthedocs.io/en/latest/index.html
	- Python library is inspired by the MATLAB Spot – A Linear-Operator Toolbox project


## Subtopics
### Calculus
- Calculus refresher: https://stanford.edu/~shervine/teaching/cme-102/calculus
- Ordinary Differential Equations
	- https://stanford.edu/~shervine/teaching/cme-102/cheatsheet-first-ode
	- https://stanford.edu/~shervine/teaching/cme-102/cheatsheet-second-ode
	- https://stanford.edu/~shervine/teaching/cme-102/cheatsheet-applications
- Stanford-cs-229 ML, algebra and calculus refresher: https://github.com/afshinea/stanford-cs-229-machine-learning/blob/master/en/refresher-algebra-calculus.pdf
- https://scipy-latinamerica.github.io/revista.io/blog/2018/10/20/introduccion-al-calculo-con-python/
- https://www.khanacademy.org/math/multivariable-calculus

- #COURSE Calculus introductory courses (MIT): https://ocw.mit.edu/high-school/mathematics/
	- Single Variable Calculus (18.01SC): https://ocw.mit.edu/courses/mathematics/18-01sc-single-variable-calculus-fall-2010
	- Multivariable Calculus (18.02SC): https://ocw.mit.edu/courses/mathematics/18-02sc-multivariable-calculus-fall-2010
	- Highlights of calculus (Strang): https://ocw.mit.edu/resources/res-18-005-highlights-of-calculus-spring-2010

### Linear Algebra
- https://en.wikipedia.org/wiki/Linear_algebra
- The Matrix Cookbook (Brandt 2012): https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf
- Stanford, Linear algebra refresher: https://stanford.edu/~shervine/teaching/cme-102/linear-algebra
- Stanford CS229, algebra and calculus refresher: 
	- https://github.com/afshinea/stanford-cs-229-machine-learning/blob/master/en/refresher-algebra-calculus.pdf
	- http://cs229.stanford.edu/section/cs229-linalg.pdf
- http://people.duke.edu/~ccc14/sta-663/LinearAlgebraReview.html
- https://www.khanacademy.org/math/linear-algebra
- http://nbviewer.jupyter.org/github/relopezbriega/relopezbriega.github.io/blob/master/downloads/LinearAlgebraPython.ipynb

- #CODE Nimfa: https://github.com/marinkaz/nimfa
- #CODE Pymf: https://github.com/cthurau/pymf
- #CODE Tensorly - fast and simple Python library for tensor learning: 
	- https://tensorly.github.io/stable/home.html
	- http://tensorly.org/stable/modules/api.html#module-tensorly.decomposition
	- https://github.com/JeanKossaifi/tensorly-notebooks/blob/master/02_tensor_decomposition/cp_decomposition.ipynb
- #CODE Eigen - Eigen is a C++ template library for linear algebra: matrices, vectors, numerical solvers, and related algorithms: http://eigen.tuxfamily.org/index.php?title=Main_Page

#### Matrix decompositions
- https://en.wikipedia.org/wiki/Matrix_decomposition
- http://people.duke.edu/~ccc14/sta-663/LinearAlgebraMatrixDecompWithSolutions.html
- http://hameddaily.blogspot.be/2016/12/simple-matrix-factorization-with.html
- https://sites.google.com/site/igorcarron2/matrixfactorizations
- http://blog.ethanrosenthal.com/2017/06/20/matrix-factorization-in-pytorch/
- https://tryolabs.com/blog/introduction-to-recommender-systems/
- SVD: https://jeremykun.com/2016/04/18/singular-value-decomposition-part-1-perspectives-on-linear-algebra/
- Alternating Least-squares
	- Fast Python Collaborative Filtering for Implicit Datasets: https://github.com/benfred/implicit
	- Recommender system using matrix factorization (SVD, ALS): http://www.benfrederickson.com/matrix-factorization/
- Eigen decomposition: 
	- https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix
	- http://setosa.io/ev/eigenvectors-and-eigenvalues/
- LU decomposition: https://en.wikipedia.org/wiki/LU_decomposition
- QR decomposition: https://en.wikipedia.org/wiki/QR_decomposition


### Distances
- Anscombe dataset: http://datascienceplus.com/the-importance-of-data-visualization/
- Distance = 1 - Similarity 
- Having a set of points (space), a distance d is a function d(x,y) that takes 2 point in the space and produces a real number. It must satisfy 4 axioms:
	1. d(x,y)>=0, no negative distances
	2. d(x,y)=0 if and only if x=y, positive distances except the from a point to itself
	3. d(x,y) = d(y,x), distance is symmetric 
	4. d(x,y) <= d(x,z)+d(z,y), the triangle inequality says that to travel from x to y, we cannot obtain any benefit if we are forced to travel via some particular third point z. 
- http://www.benfrederickson.com/distance-metrics/ (notebook kind of post using pandas, d3)
- https://github.com/andrecosta90/distance-similarity-measures

#### Minkowski
- The Minkowski distance is the generalized Lp -norm of the difference. 
- Lp-norm is the distance d defined as: d = (sum|x_i - y_i|^p)^1/p 

#### Euclidean
- Same as L2-norm (Lp-norm when p=2)
- Most familiar distance measure, defined as the square root of the sum of the square distances: d = sqrt(sum((x_i - y_i)^2)
- An equivalent to the L2-norm is the Squared Euclidean distance or sum of squared difference (SSD). This is the fundamental metric in least squares problems and linear algebra. It’s very sensitive to outliers (because of the square). The Mean Squared Error (MSE) is the normalized version of the SSD. 

#### Manhattan
- https://en.wikipedia.org/wiki/Taxicab_geometry
- Same as L1-norm (Lp-norm when p=1)
- Also known as Taxicab norm and SAD
- Distance defined as the sum of the absolute differences of the coordinates: d = sum(|x_i - y_i|)
- In solving an underdetermined system of linear equations, the regularisation term for the parameter vector is expressed in terms of the-norm (taxicab geometry) of the vector. This approach appears in the signal recovery framework called compressed sensing.
- The Mean-Absolute Error (MAE) is a normalized version of the SAD: d_MAE(x,y) = d_SAD(x,y)/n = 1/n sum(|x_i - y_i|)

#### Cosine
- The cosine distance contains the dot product scaled by the product of the Euclidean distances from the origin. It represents the angular distance of two vectors while ignoring their scale. 

#### Jaccard
- The Jaccard distance, is a measure of how _dissimilar_ two sets are. It is the complement of the Jaccard index and can be found by subtracting the Jaccard Index from 100%
- https://en.wikipedia.org/wiki/Jaccard_index

#### Hamming
- The hamming distance represents the number of entries in the two sample vectors which are different. It is a fundamental distance measure in information theory but less relevant in non-integer numerical problems. 

#### Pearson
- The Pearson distance is a correlation distance based on Pearson's product-momentum correlation coefficient of the two sample vectors. Since the correlation coefficient falls between [-1, 1], the Pearson distance lies in [0, 2] and measures the linear relationship between the two vectors. 
d_pearson(x,y) = 1 - Correlation(x,y)


### Descriptive stats
- https://en.wikipedia.org/wiki/Descriptive_statistics
- http://debrouwere.org/2017/02/01/unlearning-descriptive-statistics/

#### Correlation and dependance
- https://www.datascience.com/blog/introduction-to-correlation-learn-data-science-tutorials
- https://en.wikipedia.org/wiki/Correlation_and_dependence
- Correlation is a statistical measure that describes the association between random variables. Why is correlation a useful metric?
	- Correlation can help in predicting one quantity from another
	- Correlation can (but often does not, as we will see in some examples below) indicate the presence of a causal relationship
	- Correlation is used as a basic quantity and foundation for many other modeling techniques
- Types:
	- Pearson’s Correlation: 
		- Pearson is the most widely used correlation coefficient. Pearson correlation measures the linear association between continuous variables. In other words, this coefficient quantifies the degree to which a relationship between two variables can be described by a line. Raw observations are centered by subtracting their means and re-scaled by a measure of standard deviations.
		- Ro_X,Y = E[(X - mu_X)(Y - mu_Y)] / simga_X sigma_Y
		- numerator  -> covariance
		- Dividing the covariance between two variables by the product of standard deviations ensures that correlation will always fall between -1 and 1 (much easier to interpret)
	- Spearman's Correlation:
		- Spearman's rank correlation coefficient can be defined as a special case of Pearson ρapplied to ranked (sorted) variables. Rather than comparing means and variances, Spearman's coefficient looks at the relative order of values for each variable. The formula for Spearman's coefficient looks very similar to that of Pearson, with the distinction of being computed on ranks instead of raw scores. 
	- Kendall's Tau:
		- Also based on variable ranks, however, unlike Spearman's coefficient, Kendall’s tau does not take into account the difference between ranks— only directional agreement.
- Covariance (matrix)
	- https://en.wikipedia.org/wiki/Covariance
	- In probability theory and statistics, covariance is a measure of the joint variability of two random variables. The sign of the covariance therefore shows the tendency in the linear relationship between the variables. The magnitude of the covariance is not easy to interpret. The normalized version of the covariance, the correlation coefficient, however, shows by its magnitude the strength of the linear relation. 
	- In multidimensional case: covariance matrix. https://en.wikipedia.org/wiki/Covariance_matrix
- Correlation & Causation 
	- https://en.wikipedia.org/wiki/Correlation_does_not_imply_causation
	- "Correlation does not imply causation" is a phrase used in statistics to emphasize that a correlation between two variables does not imply that one causes the other. Spurious statistical associations can be found in a multitude of quantities, simply due to chance.
	- Often, a relationship may appear to be causal through high correlation due to some unobserved variables. For example, the number of grocery stores in a city can be strongly correlated with the number of ice cream creameries. However, there is an obvious hidden variable here— the population size of the city.
	- https://medium.com/@akelleh/a-technical-primer-on-causality-181db2575e41#.4csn3na8j
	- https://www.khanacademy.org/math/probability/scatterplots-a1/creating-interpreting-scatterplots/v/correlation-and-causality
	- Also, weak or no correlation does not imply lack of association. Correlation is only one data summary statistic that by no means tells the complete story of relationships in the data.


### Experimental design
See [Active learning](Active%20learning.md)
- Coursera - data scientist's toolbox: https://www.youtube.com/watch?v=vSXOJnGNtM4
- Good experiment: replication, measure variability, generalise to the problem, transparent
- Confounding variable - strategies: randomization, stratifying 
- Prediction is not and inference. Both are important and depend on the problem. Prediction is more challenging that inference. For prediction there are key quantities (metrics): sensitivity, specificity, positive predictive value, negative predictive value, accuracy

#### A/B testing
- https://en.wikipedia.org/wiki/A/B_testing
- In marketing and business intelligence, A/B testing is a term for a randomized experiment with two variants, A and B, which are the control and variation in the controlled experiment. A/B testing is a form of statistical hypothesis testing with two variants leading to the technical term, two-sample hypothesis testing, used in the field of statistics. Other terms used for this method include bucket tests and split-run testing.
- https://www.optimizely.com/ab-testing/
- https://www.wired.com/2012/04/ff_abtesting/
- http://data36.com/ab-testing-5-rules/
- https://www.udacity.com/course/ab-testing--ud257
- https://tech.okcupid.com/the-pitfalls-of-a-b-testing-in-social-networks/
- https://www.quora.com/What-kind-of-A-B-testing-questions-should-I-expect-in-a-data-scientist-interview-and-how-should-I-prepare-for-such-questions
- http://www.kdnuggets.com/2017/05/must-know-key-issues-problems-ab-testing.html

- #CODE Bootstrapped (Facebook) - Generate Bootstrapped confidence intervals for A/B testing: https://github.com/facebookincubator/bootstrapped
- #CODE Sixpack: https://github.com/sixpack/sixpack
- #CODE Expan (Zalando) - A Python library for statistical analysis of randomised control trials (A/B tests): 
	- https://github.com/zalando/expan
	- #TALK https://www.youtube.com/watch?v=furJxiZlo6w
- #CODE Proctor (Indeed): 
	- https://github.com/indeedeng/proctor
	- http://opensource.indeedeng.io/proctor/

#### Multi-armed bandit
- http://blog.actblue.com/2015/04/29/the-multi-armed-bandit-new-and-much-improved-ab-testing-tools-2/
- https://conversionxl.com/bandit-tests/
- https://support.google.com/analytics/answer/2844870?hl=en
- https://vwo.com/blog/multi-armed-bandit-algorithm/

### Statistical Inference
See [Bayesian modelling](Bayesian%20modelling.md)

- https://en.wikipedia.org/wiki/Statistical_inference
- https://www.youtube.com/watch?v=WkOinijQmPU&feature=youtu.be&list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ
- https://leanpub.com/LittleInferenceBook/
- Statistical inference : the process of generating conclusions about a population from a noisy sample. Without statistical inference we’re simply living within our data. With statistical inference, we’re trying to generate new knowledge. The use of probability models as the connection between our data and a populations represents the most effective way to obtain inference.
- Question to answer: Are the statistics calculated on a small sample representative of the ones of the whole population?
- http://www.datasciencecentral.com/profiles/blogs/the-death-of-the-statistical-test-of-hypothesis

#### Frequentist inference
- https://en.wikipedia.org/wiki/Frequentist_inference
- Statistical Hypothesis testing: http://youtu.be/Wqvx6_12ZMs?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ
	- https://leanpub.com/LittleInferenceBook/read#leanpub-auto-hypothesis-testing
	- Hypothesis testing is concerned with making decisions using data. 
	- To make decisions using data, we need to characterize the kinds of conclusions we can make. Classical hypothesis testing is concerned with deciding between two decisions (things get much harder if there’s more than two). The first, a null hypothesis is specified that represents the status quo. This hypothesis is usually labeled, H_0. This is what we assume by default. The alternative or research hypothesis is what we require evidence to conclude. This hypothesis is usually labeled H_a, or sometimes H_1 (or some other number other than 0). So to reiterate, the null hypothesis is assumed true and statistical evidence is required to reject it in favor of a research or alternative hypothesis	
	- t-test
		- http://www.cs.cornell.edu/~asampson/blog/statsmistakes.html
		- https://www.quora.com/What-is-an-intuitive-explanation-of-the-t-test-in-hypothesis-testing
		- https://medium.freecodecamp.org/the-t-distribution-a-key-statistical-concept-discovered-by-a-beer-brewery-dbfdc693184
	- z-test
	- f-test
- Confidence intervals
	- http://youtu.be/u85aQ0mtiZ8?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ
	- https://leanpub.com/LittleInferenceBook/read#leanpub-auto-confidence-intervals
	- Confidence intervals are methods for quantifying uncertainty in our estimates. The fact that the interval has width characterizes that there is randomness that prevents us from getting a perfect estimate.
	- t-confidence intervals
		- http://youtu.be/pHXrDMjzyYg?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ
		- https://leanpub.com/LittleInferenceBook/read#leanpub-auto-t-confidence-intervals
- Null hypothesis
	- https://en.wikipedia.org/wiki/Null_hypothesis
	- The term "null hypothesis" is a general statement or default position that there is no relationship between two measured phenomena, or no association among groups.
	- H_0 is generally assumed to be true until evidence indicates otherwise.
- P-value
	- https://en.wikipedia.org/wiki/P-value
	- A p-value is the probability that, using a given statistical model, the statistical summary (such as the sample mean difference between two compared groups) would be the same as or more extreme than the actual observed results, when the null hypothesis is true.
	- After choosing the models H_0, H_1 and a threshold value alpha for p (the significance level of the test, traditionally 5% or 1%), if the p-value is less than or equal to alpha, the test suggests that the observed data is inconsistent with the null hypothesis, so the null hypothesis must be rejected. However, that does not prove that the tested hypothesis is true. When the p-value is calculated correctly, this test guarantees that the Type I error rate is at most alpha. For typical analysis, using the standard alpha= 0.05 cutoff, the null hypothesis is rejected when p< .05 and not rejected when p> .05. 
	- The p-value does not, in itself, support reasoning about the probabilities of hypotheses but is only a tool for deciding whether to reject the null hypothesis (it can only provide evidence against a hypothesis).
	- http://youtu.be/Ky68x_7iK6c?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ
	- https://leanpub.com/LittleInferenceBook/read#leanpub-auto-p-values
	- P-values are the most common measure of statistical significance. Their ubiquity, along with concern over their interpretation and use makes them controversial among statisticians.
	- http://blog.minitab.com/blog/adventures-in-statistics-2/how-to-correctly-interpret-p-values
	- A P value is the probability of obtaining an effect at least as extreme as the one in your sample data, assuming the truth of the null hypothesis.
	- For example, suppose that a vaccine study produced a P value of 0.04. This P value indicates that if the vaccine had no effect, you’d obtain the observed difference or more in 4% of studies due to random sampling error. 
	- P values address only one question: how likely are your data, assuming a true null hypothesis? It does not measure support for the alternative hypothesis.
	- http://machinelearningmastery.com/use-statistical-significance-tests-interpret-machine-learning-results/
	- https://www.nature.com/news/big-names-in-statistics-want-to-shake-up-much-maligned-p-value-1.22375
- Statistical Power
	- https://en.wikipedia.org/wiki/Statistical_power
	- http://youtu.be/-TsBOLiW4rQ?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ
	- http://youtu.be/GRS2b1aedmk?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ
	- https://leanpub.com/LittleInferenceBook/read#leanpub-auto-power
	- Power is the probability of rejecting the null hypothesis when it is false. Ergo, power (as its name would suggest) is a good thing; you want more power. A type II error (a bad thing, as its name would suggest) is failing to reject the null hypothesis when it’s false; the probability of a type II error is usually called Beta. Note Power = 1 - Beta.
- Effect size: https://en.wikipedia.org/wiki/Effect_size
- Goodness of fit: https://en.wikipedia.org/wiki/Goodness_of_fit
	- Chi squared: https://en.wikipedia.org/wiki/Chi-squared_test

### Bootstrap and permutation tests
- http://youtu.be/0hNQx9nagq4?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ
- https://leanpub.com/LittleInferenceBook/read#leanpub-auto-the-bootstrap-and-resampling
- The bootstrap is a tremendously useful tool for constructing confidence intervals and calculating standard errors for difficult statistics. That’s the bootstrap principle: investigate the sampling distribution of a statistic by simulating repeated realizations from the observed distribution.
- https://leanpub.com/LittleInferenceBook/read#leanpub-auto-permutation-tests
- Consider comparing means between the group. However, let’s use the calculate the distribution of our statistic under a null hypothesis that the labels are irrelevant (exchangeable). This is a handy way to create a null distribution for our test statistic by simply permuting the labels over and over and seeing how extreme our data are with respect to this permuted distribution. 
- The procedure would be as follows: 
	- consider a data from with count and spray,
	- permute the spray (group) labels,
	- recalculate the statistic (such as the difference in means),
	- calculate the percentage of simulations where the simulated statistic was more extreme (toward the alternative) than the observed.

#### Bayesian bootstrap
http://www.sumsar.net/blog/2015/04/the-non-parametric-bootstrap-as-a-bayesian-model/
- #CODE Bayesian bootstrap: https://github.com/lmc2179/bayesian_bootstrap

### Probability theory
- https://en.wikipedia.org/wiki/Probability_theory
- https://stanford.edu/~shervine/teaching/cme-106/cheatsheet-probability
- https://stanford.edu/~shervine/teaching/cme-106/key-concepts
- The Coursera Statistical Inference class: http://youtu.be/oTERv_vrmJM?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ
- https://leanpub.com/LittleInferenceBook/read#leanpub-auto-probability
- Probability forms the foundation for almost all treatments of statistical inference. In our treatment, probability is a law that assigns numbers to the long run occurrence of random phenomena after repeated unrelated realizations.
- http://nbviewer.jupyter.org/url/norvig.com/ipython/Probability.ipynb
- https://en.wikipedia.org/wiki/Probability_interpretations
- https://en.wikipedia.org/wiki/Bayesian_probability
- Monty Hall problem: https://en.wikipedia.org/wiki/Monty_Hall_problem
- Coded in Python: https://github.com/cs109/2015lab1/blob/master/hw0.ipynb
- Cheatsheets:
	- Probability Cheatsheet (Chen): 
		- http://www.wzchen.com/probability-cheatsheet/
		- This is an 10-page probability cheatsheet compiled from Harvard's Introduction to Probability course, taught by Joe Blitzstein (@stat110). The probability formula sheet summarizes important probability probability concepts, formulas, and distributions, with figures, examples, and stories.
	- Review of Probability Theory (CS229 Stanford)
		- http://cs229.stanford.edu/section/cs229-prob.pdf	
		- http://cs229.stanford.edu/section/cs229-prob-slide.pdf

#### Random variables
- http://youtu.be/Shzt9uZ8BII?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ
- https://leanpub.com/LittleInferenceBook/read#leanpub-auto-random-variables
- Random variable is a numerical outcome of an experiment. The random variables that we study will come in two varieties, discrete or continuous. Discrete random variables are random variables that take on only a countable number of possibilities. Mass functions will assign probabilities that they take specific values. Continuous random variable can conceptually take any value on the real line or some subset of the real line and we talk about the probability that they lie within some range. Densities will characterize these probabilities.
- For all of these kinds of random variables, we need convenient mathematical functions to model the probabilities of collections of realizations. These functions, called mass functions and densities, take possible values of the random variables, and assign the associated probabilities. These entities describe the population of interest.
- PDF - probability density function
	- https://en.wikipedia.org/wiki/Probability_density_function
	- http://youtu.be/mPe0Us4VYDM?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ
	- https://leanpub.com/LittleInferenceBook/read#leanpub-auto-probability-density-functions
	- A probability density function (pdf), is a function associated with a continuous random variable. Because of the peculiarities of treating measurements as having been recorded to infinite decimal expansions, we need a different set of rules. This leads us to the central dogma of probability density functions: Areas under PDFs correspond to probabilities for that random variable. 
	- A PDF, or density of a continuous random variable, is a function, whose value at any given sample (or point) in the sample space(the set of possible values taken by the random variable) can be interpreted as providing a relative likelihood that the value of the random variable would equal that sample.
	- The PDF is used to specify the probability of the random variable falling within a particular range of values, as opposed to taking on any one value.

#### Conditional probability
- http://youtu.be/u6AH6qsSVA4?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ
- https://leanpub.com/LittleInferenceBook/read#leanpub-auto-conditional-probability
- Conditioning is a central subject in statistics. If we are given information about a random variable, it changes the probabilities associated with it. For example, the probability of getting a one when rolling a (standard) die is usually assumed to be one sixth. If you were given the extra information that the die roll was an odd number (hence 1, 3 or 5) then conditional on this new information, the probability of a one is now one third.
- http://setosa.io/ev/conditional-probability/

#### Independance
- https://en.wikipedia.org/wiki/Independence_(probability_theory)
- http://youtu.be/MY1EfrR1ZUs?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ
- https://leanpub.com/LittleInferenceBook/read#leanpub-auto-independence
- Statistical independence of events is the idea that the events are unrelated. Consider successive coin flips. Knowledge of the result of the first coin flip tells us nothing about the second.
- The important principle is that probabilities of independent things multiply! This has numerous consequences, including the idea that we shouldn’t multiply non-independent probabilities.
- IID samples : https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables
	- IID - Independent and identically distributed 
	- In probability theory and statistics, a sequence or other collection of random variables is independent and identically distributed(i.i.d.) if each random variable has the same probability distribution as the others and all are mutually independent.
	- https://leanpub.com/LittleInferenceBook/read#leanpub-auto-iid-random-variables
	- Random variables are said to be independent and identically distributed (iid) if they are independent and all are drawn from the same population. The reason iid samples are so important is that they are a model for random samples. This is a default starting point for most statistical inferences.
  
#### Common distributions
- https://stanford.edu/~shervine/teaching/cme-106/distribution-tables
- https://blog.cloudera.com/blog/2015/12/common-probability-distributions-the-data-scientists-crib-sheet/
- https://leanpub.com/LittleInferenceBook/read#leanpub-auto-some-common-distributions
- Normal distribution: http://efavdb.com/normal-distributions/
- Bernoulli
	- https://leanpub.com/LittleInferenceBook/read#leanpub-auto-the-bernoulli-distribution
	- TheBernoulli distribution arises as the result of a binary outcome, such as a coin flip.
- Normal
	- http://youtu.be/dUTWvKa0Leo?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ
	- https://leanpub.com/LittleInferenceBook/read#leanpub-auto-the-normal-distribution
	- The normal distribution is easily the handiest distribution in all of statistics. It can be used in an endless variety of settings. Moreover, as we’ll see later on in the course, sample means follow normal distributions for large sample sizes.
	- The normal distribution only requires two numbers to characterize it, mean and variance.
- Poisson
	- http://youtu.be/ZPLZg7qz4xE?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ
	- https://leanpub.com/LittleInferenceBook/read#leanpub-auto-the-poisson-distribution
	- The Poisson distribution is used to model counts. It is perhaps only second to the normal distribution usefulness. In fact, the Bernoulli, binomial and multinomial distributions can all be modeled by clever uses of the Poisson.
	- The Poisson distribution is especially useful for modeling unbounded counts or counts per unit of time (rates). Like the number of clicks on advertisements, or the number of people who show up at a bus stop. There is also a deep connection between the Poisson distribution and popular models for so-called event-time data.

#### Expected value
- https://en.wikipedia.org/wiki/Expected_value
- In probability theory, the expected value of a random variable, intuitively, is the long-run average value of repetitions of the experiment it represents.
- Less roughly, the law of large numbers states that the arithmetic mean of the values almost surely converges to the expected value as the number of repetitions approaches infinity. The expected value is also known as the expectation, mathematical expectation, EV, average, mean value, mean, or first moment.
- Expected values: http://youtu.be/zljxRbu6jyc?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ
- https://leanpub.com/LittleInferenceBook/read#leanpub-auto-expected-values
- Expected values characterize a distribution. The most useful expected value, the mean, characterizes the center of a density or mass function. Another expected value summary, the variance, characterizes how spread out a density is. Yet another expected value calculation is the skewness, which considers how much a density is pulled toward high or low values.

#### Central limit theorem
- https://en.wikipedia.org/wiki/Central_limit_theorem
- http://youtu.be/FAIyVHmniK0?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ
- https://leanpub.com/LittleInferenceBook/read#leanpub-auto-the-central-limit-theorem
- CLT is one of the most important theorems in statistics. For our purposes, the CLT states that the distribution of averages of iid variables becomes that of a standard normal as the sample size increases.

#### Kullback-Leibler Divergence
- https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence
- https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained


### Bayesian modelling
See [[Bayesian modelling]]

### Regression analysis
See [Regression](AI/Supervised%20Learning/Regression.md)
- https://en.wikipedia.org/wiki/Regression_analysis
- Regression analysis is widely used for prediction and forecasting, where its use has substantial overlap with the field of [[machine learning]]. See [[Regression]] for a supervised machine learning perspective.


### Mathematical Optimization
- https://en.wikipedia.org/wiki/Mathematical_optimization
- A birds-eye view of optimization algorithms (Pedregosa): http://fa.bianp.net/teaching/2018/eecs227at/
- http://people.duke.edu/~ccc14/sta-663/BlackBoxOptimization.html
- http://www.benfrederickson.com/numerical-optimization/ (notebook kind of post with python, d3)
- http://www.kdnuggets.com/2016/12/hard-thing-about-deep-learning.html
- https://www.neuraldesigner.com/blog/5_algorithms_to_train_a_neural_network
- Why Momentum works: http://distill.pub/2017/momentum/

- #CODE Nevergrad (Facebook) - A Python toolbox for performing gradient-free optimization: https://code.fb.com/ai-research/nevergrad/
- #CODE scikit-optimize: https://scikit-optimize.github.io/
- #CODE GPflowOpt - library for Bayesian Optimization with GPflow: https://gpflowopt.readthedocs.io/en/latest/index.html 
- #CODE JAX: https://github.com/google/jax ^jax
	- JAX is Autograd and XLA, brought together for high-performance machine learning research. It can automatically differentiate native Python and NumPy functions
	- #TALK JAX: Accelerated Machine Learning Research | SciPy 2020 | VanderPlas: https://www.youtube.com/watch?v=z-WSrQDXkuM
	- https://towardsdatascience.com/deep-learning-with-jax-and-elegy-c0765e3ec31a
	- Machine Learning with JAX - From Zero to Hero: https://www.youtube.com/watch?v=SstuvS-tVc0

#### Heuristics
A heuristic is any algorithm which is not guaranteed (mathematically) to find the solution, but which is nevertheless useful in certain practical situations.
- Genetic algorithms
	- https://en.wikipedia.org/wiki/Genetic_algorithm
	- Metaheuristic inspired by the process of natural selection that belongs to the larger class of evolutionary algorithms(EA). Genetic algorithms are commonly used to generate high-quality solutions to optimization and search problems by relying on bio-inspired operators such as mutation, crossover and selection.
- Nelder–Mead method
	- https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method
	- The Nelder–Mead method or downhill simplex method or amoeba method is a commonly applied numerical method used to find the minimum or maximum of an objective function in a multidimensional space. It is applied to nonlinear optimization problems for which derivatives may not be known. However, the Nelder–Mead technique is a heuristic search method that can converge to non-stationary points on problems that can be solved by alternative methods.
	- The method uses the concept of a simplex, which is a special polytope of n+ 1 vertices in n dimensions. Examples of simplices include a line segment on a line, a triangle on a plane, a tetrahedron in three-dimensional space and so forth. The method approximates a local optimum of a problem with n variables when the objective function varies smoothly and is unimodal.
	- Nelder–Mead in n dimensions maintains a set of n+1test points arranged as a simplex. It then extrapolates the behavior of the objective function measured at each test point, in order to find a new test point and to replace one of the old test points with the new one, and so the technique progresses. The simplest approach is to replace the worst point with a point reflected through the centroid of the remaining n points. If this point is better than the best current point, then we can try stretching exponentially out along this line. On the other hand, if this new point isn't much better than the previous value, then we are stepping across a valley, so we shrink the simplex towards a better point.

#### Iterative methods
The iterative methods used to solve problems of nonlinear programming differ according to whether they evaluate Hessians, gradients, or only function values.
- Gradient descent
	- https://en.wikipedia.org/wiki/Gradient_descent
	- Gradient descent is a first-order iterative optimization algorithm. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or of the approximate gradient) of the function at the current point. Gradient descent is also known as steepest descent, or the method of steepest descent.
	- An overview of gradient descent optimization algorithms: http://ruder.io/optimizing-gradient-descent/index.html
	- https://towardsdatascience.com/gradient-descent-demystified-bc30b26e432a
	- https://www.jeremyjordan.me/gradient-descent/
- Conjugate gradient method: https://en.wikipedia.org/wiki/Conjugate_gradient_method
- Interior point method: https://en.wikipedia.org/wiki/Interior_point_method


### Monte Carlo methods
- See [Bayesian modelling#Code](Bayesian%20modelling.md#Code)
- https://en.wikipedia.org/wiki/Monte_Carlo_method

#### Sequential Monte Carlo (SMC or particle filter)
- Particle filters or Sequential Monte Carlo (SMC) methods are a set of Monte Carlo algorithms used to solve filtering problems arising in signal processing and Bayesian statistical inference. The filtering problem consists of estimating the internal states in dynamical systems when partial observations are made, and random perturbations are present in the sensors as well as in the dynamical system. The objective is to compute the posterior distributions of the states of some Markov process, given some noisy and partial observations.

#### Markov Process
- https://en.wikipedia.org/wiki/Markov_chain
- https://en.wikipedia.org/wiki/Markov_property
- A stochastic process has the Markov property if the conditional probability distribution of future states of the process (conditional on both past and present states) depends only upon the present state, not on the sequence of events that preceded it. A process with this property is called a Markov process.
- Markov Chains Explained Visually: http://setosa.io/ev/markov-chains/

##### Hidden Markov Model (HMM)
- https://en.wikipedia.org/wiki/Hidden_Markov_model
- Hidden Markov Model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process with unobserved (i.e. hidden) states.
- HMM is a Markov chain for which the state is only partially observable. In other words, observations are related to the state of the system, but they are typically insufficient to precisely determine the state. Several well-known algorithms for hidden Markov models exist. 
- https://www.quora.com/What-is-a-hidden-Markov-Model-HMM-and-how-can-it-be-used-in-speech-recognition
- https://www.quora.com/Why-do-we-use-Hidden-Markov-Models-for-speech-recognition
- http://scikit-learn.sourceforge.net/stable/modules/hmm.html
- https://github.com/hmmlearn/hmmlearn
- http://hmmlearn.readthedocs.io/en/latest/tutorial.html#available-models

#### MCMC
- http://people.duke.edu/~ccc14/sta-663/MCMC.html
- http://people.duke.edu/~ccc14/sta-663/MonteCarlo.html
- http://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html

#### Nested Sampling
- https://en.wikipedia.org/wiki/Nested_sampling_algorithm
- The nested sampling algorithm is a computational approach to the problem of comparing models in Bayesian statistics. 


### Time series analysis
See [Time Series analysis](Time%20Series%20analysis.md)


### Compressed sensing
- https://en.wikipedia.org/wiki/Compressed_sensing
- Compressed sensing(also known as compressive sensing, compressive sampling, or sparse sampling) is a signal processing technique for efficiently acquiring and reconstructing a signal, by finding solutions to underdetermined linear systems. This is based on the principle that, through optimization, the sparsity of a signal can be exploited to recover it from far fewer samples than required by the Shannon-Nyquist sampling theorem.
- There are two conditions under which recovery is possible: 
	- Sparsity, which requires the signal to be sparse in some domain. 
	- Incoherence, which is applied through the isometric property which is sufficient for sparse signals
- https://calculatedcontent.com/2012/12/28/foundations-theory-of-compressed-sensing/amp/

#### Nyquist theorem
- https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem
- In the field of digital signal processing, the sampling theorem is a fundamental bridge between continuous-time signals(often called "analog signals") and discrete-time signals(often called "digital signals"). It establishes a sufficient condition for a sample rate that permits a discrete sequence of samples to capture all the information from a continuous-time signal of finite bandwidth.

#### Matching pursuit
- https://en.wikipedia.org/wiki/Matching_pursuit
- Matching pursuit (MP) is asparse approximation algorithm which involves finding the "best matching" projections of multidimensional data onto the span of an over-complete (i.e., redundant) dictionary D.
- Orthogonal Matching Pursuit
	- Extension of MP: after every step, all the coefficients extracted so far are updated, by computing the orthogonal projection of the signal onto the set of atoms selected so far. This can lead to better results than standard MP, but requires more computation.
	- http://scikit-learn.org/stable/auto_examples/linear_model/plot_omp.html
