---
title: "Deep Learning"
disableToc: false 
---

- DL is a branch of [[Machine Learning]] and [[AI]] based on a set of algorithms that attempt to model high level abstractions in data by using a deep graph with multiple processing layers, composed of multiple linear and non-linear transformations.
- DL uses huge neural networks with many layers of processing units, taking advantage of advances in computing power and improved training techniques to learn complex patterns in large amounts of data. 
- https://github.com/ChristosChristofidis/awesome-deep-learning
- https://github.com/endymecy/awesome-deeplearning-resources
- https://github.com/afshinea/stanford-cs-230-deep-learning/blob/master/super-cheatsheet-deep-learning.pdf
- https://jalammar.github.io/visual-interactive-guide-basics-neural-networks/
- https://frnsys.com/ai_notes/machine_learning/neural_nets.html
- https://hackernoon.com/deep-learning-cheat-sheet-25421411e460
- https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/
- http://www.computervisionblog.com/2016/12/nuts-and-bolts-of-building-deep.html
- https://kevinzakka.github.io/2016/09/26/applying-deep-learning/
- http://www.deepideas.net/deep-learning-from-scratch-theory-and-implementation/
- A Brief History of Neural Nets and Deep Learning (2020): http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning/
- A set of educational DL demos applied to the MNIST dataset: https://github.com/natbusa/deepnumbers
- Time Benchmark of models: https://dawn.cs.stanford.edu/benchmark/
- Deep learning in NNs: An overview: https://www.sciencedirect.com/science/article/pii/S0893608014002135
- Deep Learning Research Directions: Computational Efficiency: http://timdettmers.com/2017/08/31/deep-learning-research-directions/
- A Recipe for Training Neural Networks: http://karpathy.github.io/2019/04/25/recipe/
- When to use deep learning
	- Don't use deep learning your data isn't that big: https://simplystatistics.org/2017/05/31/deeplearning-vs-leekasso/
	- You can probably use deep learning even if your data isn't that big: http://beamandrew.github.io/deeplearning/2017/06/04/deep_learning_works.html
	- When not to use deep learning: http://hyperparameter.space/blog/when-not-to-use-deep-learning/
	- How can I know if DL works better for a specific problem than SVM or random forest?: https://github.com/rasbt/python-machine-learning-book/blob/master/faq/deeplearn-vs-svm-randomforest.md
	- Using ANNs on small data – Deep Learning vs. Xgboost: http://maxberggren.se/2017/06/18/deep-learning-vs-xgboost/
	- The limitations of deep learning: https://blog.keras.io/the-limitations-of-deep-learning.html
- Literature parsing:
	- DeepAI:  https://deepai.org/
	- Papers with code: https://paperswithcode.com/
	- Deep learning monitor: https://deeplearn.org/
	- GroundAI: https://www.groundai.com/
	- #CODE Deep Learning Models (Raschka): https://github.com/rasbt/deeplearning-models
	- #CODE Model Zoo: https://modelzoo.co/


# Books
- #BOOK Deep Learning Interviews: Hundreds of fully solved job interview questions from a wide range of key topics in AI (Kashani 2022): https://arxiv.org/abs/2201.00650
- #BOOK Physics-based Deep Learning Book (Thuerey 2021): https://physicsbaseddeeplearning.org/intro.html ^PBDL
- #BOOK The Principles of Deep Learning Theory: An Effective Theory Approach to Understanding Neural Networks (Roberts 2022): https://deeplearningtheory.com/PDLT.pdf
	- https://ai.facebook.com/blog/advancing-ai-theory-with-a-first-principles-understanding-of-deep-neural-networks/
- #BOOK Deep Learning Book (Goodfellow, 2016 MIT): https://www.deeplearningbook.org/
	- The Deep Learning textbook is a resource intended to help students and practitioners enter the field of machine learning in general and deep learning in particular. The online version of the book is now complete and will remain available online for free.
- #BOOK DL tutorial (LISA Lab, U Montreal): http://deeplearning.net/tutorial/
- #BOOK Neural Networks and Deep Learning: http://neuralnetworksanddeeplearning.com/index.html
- Deep Learning: Methods and Applications (Li Deng, 2014 NOW)
	- https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/DeepLearning-NowPublishing-Vol7-SIG-039.pdf
- #BOOK Deep Learning with Python (Chollet, 2021 MANNING)
	- https://www.manning.com/books/deep-learning-with-python-second-edition
	- 1st edition: http://faculty.neu.edu.cn/yury/AAI/Textbook/Deep%20Learning%20with%20Python.pdf
- #BOOK Machine learning yearning (Andrew Ng, 2018): http://www.mlyearning.org/
	- https://github.com/ajaymache/machine-learning-yearning
- #BOOK Dive into Deep Learning (Zhang): https://d2l.ai/index.html
	- An interactive deep learning book for students, engineers, and researchers. Uses MXNet/Gluon, Pytorch and Tensorflow
	- Jupyter notebooks for each section.  https://en.d2l.ai/d2l-en.zip
- #BOOK Introduccion practica con Keras (Torres 2018): https://torres.ai/deep-learning-inteligencia-artificial-keras/
- #BOOK Technical Book on Deep Learning: https://github.com/tomepel/Technical_Book_DL

# Talks
- #TALK Deep Learning (Yoshua Bengio, MLSS 2020): 
	- Part I: https://www.youtube.com/watch?v=c_U4THknoHE
	- Part II: https://www.youtube.com/watch?v=PDPdIDihPvc
- #TALK Deep Learning Hardware: Past, Present, and Future (Yann LeCun, ISSCC 2019): https://www.youtube.com/watch?v=YzD7Z2yRL7Y
- #TALK Keras, Deep Learning, and the Progress of AI (François Chollet, Lex Fridman Podcast, 2019): https://www.youtube.com/watch?v=Bo8MY4JpiXE
- #TALK Deep Learning and the Future of Artificial Intelligence (Yann LeCun, 2018): https://www.youtube.com/watch?v=RM-Jtc2ryfM&t=5s
- #TALK AI Breakthroughs & Obstacles to Progress, Mathematical and Otherwise (Yann LeCun, 2018): https://www.youtube.com/watch?v=1_KhJv0Em5Y
- #TALK Power & Limits of Deep Learning (Yann Lecun, 2017): https://www.youtube.com/watch?v=0tEhw5t6rhc
- #TALK The Future of Sparsity in Deep Learning (Trevor Gale, Phd student Stanford, 2021): https://events.rainfocus.com/widget/nvidia/nvidiagtc/sessioncatalog/session/1631029840983001jvzq
- #TALK The Deep End of Deep Learning (Hugo Larochelle, TEDxBoston 2016): https://www.youtube.com/watch?v=dz_jeuWx3j0
- #TALK How deep neural networks work (Brandon Rohrer): https://www.youtube.com/watch?v=ILsA4nyG7I0
	- Simple explanations of DL basics and nice graphics


# Courses
- #COURSE Introduction to Deep Learning (COMP0090, UCL): https://github.com/YipengHu/COMP0090 
- #COURSE Full Stack Deep Learning: https://fullstackdeeplearning.com/
	- Full Stack Deep Learning - Spring 2021: https://fullstackdeeplearning.com/spring2021/
		- Lecture 13: ML Teams and Startups: https://fullstackdeeplearning.com/spring2021/lecture-13/
	- https://fall2019.fullstackdeeplearning.com/
		- https://github.com/full-stack-deep-learning/course-gitbook
- #COURSE Deep Learning (NYU): https://atcold.github.io/pytorch-Deep-Learning/
	- https://github.com/Atcold/pytorch-Deep-Learning (pytorch)
- #COURSE Deep Learning (CS230, Stanford): http://cs230.stanford.edu/
	- Cheatsheets: https://github.com/afshinea/stanford-cs-230-deep-learning
- #COURSE Tensorflow for Deep Learning Research (CS20SI, Stanford): http://web.stanford.edu/class/cs20si/syllabus.html
- #COURSE DeepMind x UCL | Deep Learning Lecture Series 2020: https://www.youtube.com/playlist?list=PLqYmG7hTraZCDxZ44o4p3N5Anz3lLRVZF
- #COURSE Introduction to Deep Learning (6.S191, MIT): http://introtodeeplearning.com/
- #COURSE MIT Deep Learning and Artificial Intelligence Lectures: https://deeplearning.mit.edu/
	- Youtube playlist: https://www.youtube.com/watch?v=0VH1Lim8gL8&list=PLrAXtmErZgOeiKm4sgNOknGvNjby9efdf
	- Deep Learning State of the Art (2020): https://www.youtube.com/watch?v=0VH1Lim8gL8
- #COURSE Introduction to Deep Learning (MIT 6.S191): http://introtodeeplearning.com/
- #COURSE Intro to Neural Networks and Machine Learning (CSC 321, UToronto): http://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/
- #COURSE Deep Learning nanodegree (Udacity): https://www.udacity.com/course/deep-learning-nanodegree--nd101
	- https://github.com/udacity/deep-learning-v2-pytorch
	- https://www.udacity.com/course/deep-learning-pytorch--ud188
- #COURSE Deep Learning with PyTorch: Zero to GANs (Jovian): https://jovian.ai/learn/deep-learning-with-pytorch-zero-to-gans
- #COURSE Fast AI - Practical Deep Learning For Coders: http://course.fast.ai/
	- Deep Learning for Coders with fastai and PyTorch: [[AI]] Applications Without a PhD - the book and the course
	- https://github.com/fastai/fastbook
- #COURSE Deep Learning course (U Paris-Saclay): https://m2dsupsdlclass.github.io/lectures-labs/
- #COURSE Introduction to Machine Learning and Neural Networks (Uniandes): https://albahnsen.com/courses/applied-deep-learning/
	- https://github.com/albahnsen/AppliedDeepLearningClass
- #COURSE Deep learning specialization (deeplearning.ai, Coursera, Andrew Ng): https://www.coursera.org/specializations/deep-learning
	- https://www.deeplearning.ai/deep-learning-specialization/
- #COURSE Neural Networks (U Sherbrooke): http://info.usherbrooke.ca/hlarochelle/neural_networks/description.html
- #COURSE The Neural Aesthetic (ITP-NYU): http://ml4a.github.io/classes/itp-F18/


# Code
State of ML frameworks: 
- https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/
- https://towardsdatascience.com/tensorflow-or-pytorch-146f5397278a

- #CODE Tensorflow and Keras. See [[Tensorflow, keras]]
- #CODE Triton: https://github.com/openai/triton
	- language and compiler for writing highly efficient custom Deep-Learning primitives
	- https://openai.com/blog/triton/
	- https://www.infoq.com/news/2021/08/openAI-triton/
	- Triton uses Python as its base. The developer writes code in Python using Triton’s libraries, which are then JIT-compiled to run on the GPU. This allows integration with the rest of the Python ecosystem, currently the biggest destination for developing machine-learning solutions
- #CODE MindSpore (Huawei): https://github.com/mindspore-ai/mindspore ^huaweimindpore
	- https://towardsdatascience.com/program-your-first-neural-network-with-huawei-mindspore-1fc50023e90d
	- https://towardsdatascience.com/huaweis-mindspore-a-new-competitor-for-tensorflow-and-pytorch-d319deff2aec
	- https://www.mindspore.cn/en
- #CODE Tensorlayer - Deep Learning and Reinforcement Learning Library for Scientists and Engineers: https://github.com/tensorlayer/tensorlayer
	- http://tensorlayer.org/
- #CODE Elegy - Neural Networks framework based on Jax and inspired by Keras: https://github.com/poets-ai/elegy
	- https://poets-ai.github.io/elegy/
	- See [[Math and stats#^jax]]
- #CODE PyTorch (Facebook): Tensors and Dynamic neural networks in Python with strong GPU acceleration. https://github.com/pytorch/pytorch
	- http://pytorch.org
	- https://sagivtech.com/2017/09/19/optimizing-pytorch-training-code/
	- #CODE Pytorch-lightning: https://pytorchlightning.ai/
		- https://medium.com/pytorch/introducing-lightning-flash-the-fastest-way-to-get-started-with-deep-learning-202f196b3b98
	- #CODE Pytext (Facebook) - A natural language modeling framework based on PyTorch: https://github.com/facebookresearch/pytext 
		- https://fb.me/pytextdocs
		- PyText is a deep-learning based [[NLP]] modeling framework built on PyTorch
	- #CODE Pytorch tabular: https://github.com/manujosephv/pytorch_tabular ^pytorchtab
		- https://deep-and-shallow.com/2021/01/27/pytorch-tabular-a-framework-for-deep-learning-for-tabular-data/
- #CODE Paddle (Baidu)- PArallel Distributed Deep LEarning: Machine Learning Framework from Industrial Practice. https://github.com/PaddlePaddle/Paddle
	- http://www.paddlepaddle.org/
- #CODE Mxnet (Apache): https://github.com/apache/incubator-mxnet
	- http://mxnet.io/
	- Towards Next Generation Deep Learning Framework: https://mli.github.io/cvpr17/
- #CODE Microsoft Cognitive Toolkit (CNTK): https://github.com/Microsoft/CNTK
	- https://www.microsoft.com/en-us/research/product/cognitive-toolkit/
	- Microsoft Cognitive Toolkit: A free, easy-to-use, open-source, commercial-grade toolkit that trains deep learning algorithms to learn like the human brain.
	- #TALK https://www.youtube.com/watch?v=9gDDO5ldT-4&feature=youtu.be
- #CODE Neupy - NeuPy is a Tensorflow based python library for prototyping and building neural networks: https://github.com/itdxer/neupy
	- http://neupy.com/pages/home.html
- #CODE Chainer - Chainer is a Python-based deep learning framework aiming at flexibility
	- https://github.com/chainer/chainer
- #CODE PySyft: https://github.com/OpenMined/PySyft
	- PySyft is a Python library for secure and private Deep Learning. PySyft decouples private data from model training, using Federated Learning, Differential Privacy, and Encrypted Computation (like Multi-Party Computation (MPC) and Homomorphic Encryption (HE)) within the main Deep Learning frameworks like PyTorch and TensorFlow.
	- #PAPER A generic framework for privacy preserving deep learning: https://arxiv.org/abs/1811.04017


# References
- #PAPER Deep learning (LeCun 2015): https://www.nature.com/articles/nature14539 ^dllecun15
	- https://www.researchgate.net/profile/Y_Bengio/publication/277411157_Deep_Learning/links/55e0cdf908ae2fac471ccf0f/Deep-Learning.pdf
- #PAPER Deep Neural Decision Forests (Kontschieder 2016): https://www.ijcai.org/Proceedings/16/Papers/628.pdf
	- #CODE https://keras.io/examples/structured_data/deep_neural_decision_forests/
- #PAPER On the Origin of Deep Learning (Wang 2017): https://arxiv.org/abs/1702.07800v4 
- #PAPER Representation Learning on Large and Small Data (Chou 2017): https://arxiv.org/abs/1707.09873v1
- #PAPER Deep Learning in Neural Networks: An Overview (Schmidhuber, 2018): https://arxiv.org/abs/1404.7828
- #PAPER Deep Learning as a Mixed Convex-Combinatorial Optimization Problem (Friesen 2018): https://arxiv.org/abs/1710.11573
- #PAPER Using Deep Neural Networks for Inverse Problems in Imaging: Beyond Analytical Methods (Lucas, 2018): https://ieeexplore.ieee.org/document/8253590
	- http://decsai.ugr.es/vip/files/journals/08253590.pdf
- #PAPER Neural Tangent Kernel: Convergence and Generalization in Neural Networks (Jacot 2018): https://arxiv.org/abs/1806.07572#
	- https://www.quantamagazine.org/a-new-link-to-an-old-model-could-crack-the-mystery-of-deep-learning-20211011/
- #PAPER A Survey of Deep Learning for Scientific Discovery (Raghu & Schmidt, 2020): https://arxiv.org/abs/2003.11755 ^dlscience20
- #PAPER Neural circuit policies enabling auditable autonomy (Lechner 2020): https://www.nature.com/articles/s42256-020-00237-3
	- #CODE https://github.com/mlech26l/keras-ncp
	- https://www.csail.mit.edu/news/new-deep-learning-models-require-fewer-neurons
	- https://www.marktechpost.com/2021/10/19/mit-csail-tu-wien-and-ist-researchers-introduce-deep-learning-models-that-require-fewer-neurons/
- #PAPER Implicitly Defined Layers in Neural Networks (Zhang 2020): https://arxiv.org/abs/2003.01822
- #PAPER A Mathematical Principle of Deep Learning: Learn the Geodesic Curve in the Wasserstein Space (Gai 2021): https://arxiv.org/abs/2102.09235
- #PAPER Why is AI hard and Physics simple? (Roberts 2021): https://arxiv.org/abs/2104.00008
- #PAPER Deep Learning for AI (By Yoshua Bengio, Yann Lecun, Geoffrey Hinton, Turing lecture, 2021): https://dl.acm.org/doi/10.1145/3448250
	- https://cacm.acm.org/magazines/2021/7/253464-deep-learning-for-ai/fulltex
- #PAPER Self-Tuning for Data-Efficient Deep Learning (Wang 2021): https://arxiv.org/abs/2102.12903
	- #CODE https://github.com/thuml/Self-Tuning
	- #TALK https://recorder-v3.slideslive.com/#/share?share=40334&s=f7988e61-bece-4a7a-a6ba-3e1a2b49b37b
- #PAPER Neural circuit policies enabling auditable autonomy (Lechner 2021): https://www.nature.com/articles/s42256-020-00237-3
	- #CODE https://github.com/mlech26l/keras-ncp
- #PAPER Uncertainty Baselines: Benchmarks for Uncertainty & Robustness in Deep Learning (Nado 2021): https://arxiv.org/abs/2106.04015
	- https://ai.googleblog.com/2021/10/baselines-for-uncertainty-and.html


## Generalization
See [[XAI#Interpretability of deep learning models]]
- http://www.inference.vc/everything-that-works-works-because-its-bayesian-2/

- #PAPER Understanding deep learning requires re-thinking generalization (Zhang 2016): https://arxiv.org/abs/1611.03530
	- https://blog.acolyer.org/2017/05/11/understanding-deep-learning-requires-re-thinking-generalization/
	- https://www.quora.com/Why-is-the-paper-%E2%80%9CUnderstanding-Deep-Learning-Requires-Rethinking-Generalization%E2%80%9D-important
- #PAPER A Closer Look at Memorization in Deep Networks (Arpit 2017): https://arxiv.org/abs/1706.05394
- #PAPER Deep nets don’t learn via memorization (Krueger 2017): https://openreview.net/pdf?id=rJv6ZgHYg
- #PAPER Rethinking generalization requires revisiting old ideas: statistical mechanics approaches and complex learning behavior (Martin 2017): https://arxiv.org/abs/1710.09553
- #PAPER Ablation Studies in Artificial Neural Networks (Meyes 2019): https://arxiv.org/abs/1901.08644
- #PAPER Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning (Allen-Zhu 2020): https://arxiv.org/abs/2012.09816
	- https://www.microsoft.com/en-us/research/blog/three-mysteries-in-deep-learning-ensemble-knowledge-distillation-and-self-distillation/
- #PAPER The Deep Bootstrap Framework: Good Online Learners are Good Offline Generalizers (Nakkiran 2021): https://arxiv.org/abs/2010.08127
	- https://ai.googleblog.com/2021/03/a-new-lens-on-understanding.html
- #PAPER Predicting trends in the quality of state-of-the-art neural networks without access to training or testing data (Martin 2021): https://www.nature.com/articles/s41467-021-24025-8
- #PAPER Stochastic Training is Not Necessary for Generalization (Geiping 2021): https://arxiv.org/abs/2109.14119
- #PAPER Underspecification Presents Challenges for Credibility in Modern Machine Learning (D'Amour 2021): https://arxiv.org/abs/2011.03395
	- https://ai.googleblog.com/2021/10/how-underspecification-presents.html
- #PAPER Grokking - Generatlization beyond overfitting on small algorithmic datasets (Power 2022): https://arxiv.org/abs/2201.02177v1
	- Paper explained: https://www.youtube.com/watch?v=dND-7llwrpw


## Regularization
- In general, techniques aimed at reducing overfitting and improve generalization
- Overfit and underfit: https://www.tensorflow.org/tutorials/keras/overfit_and_underfit
- https://towardsdatascience.com/regularization-in-deep-learning-l1-l2-and-dropout-377e75acc036
- https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-learning-with-weight-regularization/
- https://medium.com/intelligentmachines/convolutional-neural-network-and-regularization-techniques-with-tensorflow-and-keras-5a09e6e65dc7

### Data augmentation
- https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html
- https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/
- #PAPER A survey on Image Data Augmentation for Deep Learning (Shorten 2019): https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0

### Dropout
- http://www.cs.toronto.edu/~hinton/absps/dropout.pdf
- https://pgaleone.eu/deep-learning/regularization/2017/01/10/anaysis-of-dropout/
- 12 Main Dropout Methods: Mathematical and Visual Explanation for DNNs, CNNs, and RNNs: https://towardsdatascience.com/12-main-dropout-methods-mathematical-and-visual-explanation-58cdc2112293

- #PAPER Dropout: A Simple Way to Prevent Neural Networks from Overfitting (Srivastava 2014): http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf
- #PAPER Efficient Object Localization Using Convolutional Networks (Tompson 2015): https://arxiv.org/abs/1411.4280v3
	- Proposed spatial dropout
- #PAPER Analysis on the Dropout Effect in Convolutional Neural Networks (Park 2017): https://link.springer.com/chapter/10.1007/978-3-319-54184-6_12
	- http://mipal.snu.ac.kr/images/1/16/Dropout_ACCV2016.pdf
- #PAPER Effective and Efficient Dropout for Deep Convolutional Neural Networks (Cai 2020): https://arxiv.org/abs/1904.03392


### Normalization
- Normalization techniques also improve generalization error, providing some regularization
- Normalization Techniques in Deep Neural Networks: https://medium.com/techspace-usict/normalization-techniques-in-deep-neural-networks-9121bf100d8
- Different Types of Normalization in Tensorflow: https://towardsdatascience.com/different-types-of-normalization-in-tensorflow-dac60396efb0
- Normalization in Deep Learning: https://arthurdouillard.com/post/normalization/
- https://sebastianraschka.com/faq/docs/scale-training-test.html 
- Data normalization/standardization can be used as an alternative (before training) to synch batchnorm (multi-gpu training)
- Spectral normalization: https://sthalles.github.io/advanced_gans/

- #PAPER Normalization Techniques in Training DNNs: Methodology, Analysis and Application (Huang 2020): https://arxiv.org/abs/2009.12836

#### BatchNorm
- #PAPER  Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (Ioffe 2015): https://arxiv.org/abs/1502.03167
	- #TALK https://www.youtube.com/watch?v=ZOabsYbmBRM&feature=youtu.be
	- http://stackoverflow.com/questions/34716454/where-do-i-call-the-batchnormalization-function-in-keras
	- Slower convergence w/o BN, BN can be applied on top of standardization 
	- Synch BatchNorm appears in TF 2.2, for multi-gpu training 
		- https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/SyncBatchNormalization 
- #PAPER Rethinking the Usage of Batch Normalization and Dropout (Chen 2019): https://arxiv.org/abs/1905.05928

## Activations
- https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html
- https://mlfromscratch.com/activation-functions-explained/#/
- RELU: https://en.wikipedia.org/wiki/Rectifier_(neural_networks)
	- https://www.quora.com/What-is-special-about-rectifier-neural-units-used-in-NN-learning
- http://stats.stackexchange.com/questions/126238/what-are-the-advantages-of-relu-over-sigmoid-function-in-deep-neural-network
- https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions

## Loss/Cost functions
- Cross entropy
	- http://neuralnetworksanddeeplearning.com/chap3.html
	- https://en.wikipedia.org/wiki/Cross_entropy
	- http://www.kdnuggets.com/2017/02/gentlest-introduction-tensorflow-part-4.html
- Perceptual loss, image reconstruction
	- https://arxiv.org/pdf/1511.06409.pdf (Learning to Generate Images With Perceptual Similarity Metrics) 
	- #PAPER Loss Functions for Image Restoration with Neural Networks (Zhao 2018): https://arxiv.org/abs/1511.08861
	- https://medium.com/@sanari85/rediscovery-of-ssim-index-in-image-reconstruction-ssim-as-a-loss-function-a1ffef7d2be 
		- We use three different metric for comparing each different methods such  as  DSSIM,  MSE,  and  MAE.  Structural  dissimilarity(DSSIM)[14]  is  an  image  distance  metric,  that  corresponds better to the human perception than MAE or RMSE. MeanSquared  Error(MSE)  measures  the  average  of  the  squares of the errors that is, the average squared difference between the  estimated  values  and  the  actual  value.  Mean  AbsoluteError  (MAE)  is  the  average  distance  between  each  pixel point. https://arxiv.org/pdf/2001.05372.pdf 
- Deep learning image enhancement insights on loss function engineering: https://towardsdatascience.com/deep-learning-image-enhancement-insights-on-loss-function-engineering-f57ccbb585d7
- Mean squared logarithmic error 
	- https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/mean-squared-logarithmic-error-(msle) 
	- https://medium.com/@olegrybkin_20684/the-reasonable-ineffectiveness-of-mse-pixel-loss-for-future-prediction-and-what-to-do-about-it-4dca8152355d 

## Optimizers and backprop
- How to use Learning Curves to Diagnose Machine Learning Model Performance: https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/
- https://www.quora.com/Intuitively-how-does-mini-batch-size-affect-the-performance-of-stochastic-gradient-descent
- Keras optimizers: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/ 
- Adam: http://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/
- An overview of gradient descent optimization algorithms (2016): https://ruder.io/optimizing-gradient-descent/index.html#otherrecentoptimizers 
- https://hackernoon.com/some-state-of-the-art-optimizers-in-neural-networks-a3c2ba5a5643 
- https://www.jeremyjordan.me/neural-networks-training/
- http://colah.github.io/posts/2015-08-Backprop/
- Back-propagation - Math Simplified. https://github.com/DebPanigrahi/Machine-Learning/blob/master/back_prop.ipynb
- https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/
- https://venturebeat.com/2020/12/16/at-neurips-2020-researchers-proposed-faster-more-efficient-alternatives-to-backpropagation/amp/

- #PAPER On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima (Shirish Keshkar 2017): https://arxiv.org/abs/1609.04836
- #PAPER Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour (Goyal 2018): https://arxiv.org/abs/1706.02677
- #PAPER Decoupled Weight Decay Regularization (Loshchilov 2018): https://arxiv.org/abs/1711.05101
	- AdamW optimizer
	- #CODE https://www.tensorflow.org/addons/api_docs/python/tfa/optimizers/AdamW
	- https://www.fast.ai/2018/07/02/adam-weight-decay/
- #PAPER Deep Double Descent: Where Bigger Models and More Data Hurt (Nakkiran 2019): https://arxiv.org/abs/1912.02292
	- https://openai.com/blog/deep-double-descent/
	- https://medium.com/mlearning-ai/double-descent-8f92dfdc442f
- #PAPER Reconciling modern machine learning practice and the bias-variance trade-off (Belkin 2019): https://arxiv.org/abs/1812.11118
	- Paper explained: https://www.youtube.com/watch?v=ZAW9EyNo2fw
- #PAPER Deep Double Descent: Where Bigger Models and More Data Hurt (Nakkiran 2020): https://arxiv.org/abs/1912.02292
	- https://openai.com/blog/deep-double-descent/
- #PAPER Descending through a Crowded Valley -- Benchmarking Deep Learning Optimizers (Schmidt 2020): https://arxiv.org/abs/2007.01547
	- Paper explained: https://www.youtube.com/watch?v=DiNzQP7kK-s
- #PAPER Early Stopping in Deep Networks: Double Descent and How to Eliminate it (Heckel 2020): https://arxiv.org/abs/2007.10099
	- contrary to model-wise double descent, epoch-wise double descent is not a phenomena tied o over-parameterization
	- both under- and overparameterized models can have epoch-wise double descent 
	- #CODE https://github.com/MLI-lab/early_stopping_double_descent


## Efficiency and performance
- #PAPER Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better (Menghani 2021): https://arxiv.org/abs/2106.08962
	- https://analyticsindiamag.com/how-to-build-smaller-faster-better-deep-learning-models/

## Attention
See: 
[Transformers#For NLP](Transformers#For%20NLP.md)
[CNNs#Visual attention](CNNs#Visual%20attention.md)

- #COURSE Attention and Memory in Deep Learning (DeepMind x UCL | Deep Learning Lectures | 8/12): https://www.youtube.com/watch?v=AIiwuClvH6k

## Deep learning for multi-dimensional data
See:
[Video segmentation and prediction](AI/Computer%20Vision/Video%20segmentation%20and%20prediction.md)
[Encoder-decoder networks](AI/Deep%20learning/Encoder-decoder%20networks.md)
[Transformers](Transformers.md)
[Generative modelling](Generative%20modelling.md)

- #PAPER Demystifying Deep Learning in Predictive Spatio-Temporal Analytics: An Information-Theoretic Framework (Tan 2020): https://arxiv.org/abs/2009.06304

## Deep learning for tabular data
- An Introduction to Deep Learning for Tabular Data: https://www.fast.ai/2018/04/29/categorical-embeddings/
- Applying Deep Learning on Tabular Data Using TensorFlow 2.0: https://pdf.co/blog/deep-learning-on-tabular-data-using-tensorflow-20
- #CODE [[#^pytorchtab]]
	
- #PAPER Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data (Popov 2019): https://arxiv.org/abs/1909.06312
- #PAPER TabNet: Attentive Interpretable Tabular Learning (Arik 2020): https://arxiv.org/abs/1908.07442
- #PAPER Converting tabular data into images for deep learning with convolutional neural networks (Zhu 2021): https://www.nature.com/articles/s41598-021-90923-y
- #PAPER Tabular Data: Deep Learning is Not All You Need (Shwartz-Ziv 2021): https://arxiv.org/abs/2106.03253
- #PAPER XBNet : An Extremely Boosted Neural Network (Sarkar 2021): https://arxiv.org/abs/2106.05239
	- #CODE XBNet: https://github.com/tusharsarkar3/XBNet
	- Boosted neural network for tabular data
	- https://analyticsindiamag.com/guide-to-xbnet-an-extremely-boosted-neural-network/
- #PAPER Revisiting Deep Learning Models for Tabular Data (Gorishniy 2021): https://arxiv.org/abs/2106.11959
	- #CODE RDTL (Yandex): https://github.com/yandex-research/rtdl
	- https://yandex-research.github.io/rtdl/
- #PAPER TABBIE: Pretrained Representations of Tabular Data (Lida 2021): https://arxiv.org/abs/2105.02584v1


# Architectures and types of models
- The neural network zoo: http://www.asimovinstitute.org/neural-network-zoo/
- Deep Learning Tips and Tricks cheatsheet: https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-deep-learning-tips-and-tricks
- A Visual and Interactive Guide to the Basics of NNs: https://jalammar.github.io/visual-interactive-guide-basics-neural-networks/
- A Visual And Interactive Look at Basic Neural Network Math: https://jalammar.github.io/feedforward-neural-networks-visual-interactive/

## [MLPs](MLPs.md)

## [Deep belief network](Deep%20belief%20network.md)

## [Autoencoders](Autoencoders.md)

## [CNNs](CNNs.md)

## [RNNs](RNNs.md)

## [CapsNets](CapsNets.md)

## [GANs](GANs.md)

## [Bayesian neural networks](Bayesian%20neural%20networks.md)

## [GNNs](GNNs.md)

## [Residual and dense neural networks](Residual%20and%20dense%20neural%20networks.md)

## [Neural ODEs](Neural%20ODEs.md)

## [Fourier Neural Operators](Fourier%20Neural%20Operators.md)

## [Multimodal learning](Multimodal%20learning.md)

## [Geometric deep learning](Geometric%20deep%20learning.md)

