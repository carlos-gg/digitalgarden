---
title: "Deep Learning (DL)"
---

> Deep learning (DL), also known as deep structured learning, is part of a broader family of [[AI/Machine Learning]] methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised. DL uses huge neural networks with many layers of processing units, taking advantage of advances in computing power and improved training techniques to learn complex patterns in large amounts of data

## Resources
- https://github.com/ChristosChristofidis/awesome-deep-learning
- https://github.com/endymecy/awesome-deeplearning-resources
- https://en.wikipedia.org/wiki/Deep_learning
- [Deep Learning Curriculum](https://github.com/jacobhilton/deep_learning_curriculum)
- https://jalammar.github.io/visual-interactive-guide-basics-neural-networks/
- [A Quick Introduction to Neural Networks](https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/)
- [Deep Neural Nets: 33 years ago and 33 years from now (Andrej Karpathy)](http://karpathy.github.io/2022/03/14/lecun1989/)
- [Deep learning's diminish returns (Thompson)](https://spectrum.ieee.org/deep-learning-computational-cost)
	- https://towardsdatascience.com/the-future-of-deep-learning-7e8574ad6ae3
- [Deep Learning Is Hitting a Wall](https://nautil.us/deep-learning-is-hitting-a-wall-14467/)
- [A Brief History of Neural Nets and Deep Learning (2020)](http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning/)
- [Time Benchmark of models](https://dawn.cs.stanford.edu/benchmark/)
- [A Recipe for Training Neural Networks](http://karpathy.github.io/2019/04/25/recipe/)
- [Computer Scientists Prove Why Bigger Neural Networks Do Better](https://www.quantamagazine.org/computer-scientists-prove-why-bigger-neural-networks-do-better-20220210/)


### DL news aggregators
- [DeepAI](https://deepai.org/)
- [Papers with code](https://paperswithcode.com/)
- [Deep learning monitor](https://deeplearn.org/)

### Cheatsheets
- https://github.com/afshinea/stanford-cs-230-deep-learning/blob/master/super-cheatsheet-deep-learning.pdf

### When to use and not to use deep learning
- [When and When Not to Use Deep Learning](https://blog.dataiku.com/when-and-when-not-to-use-deep-learning)
- [You can probably use deep learning even if your data isn't that big](http://beamandrew.github.io/deeplearning/2017/06/04/deep_learning_works.html)
- [When not to use deep learning](http://hyperparameter.space/blog/when-not-to-use-deep-learning/)
- [Using ANNs on small data – Deep Learning vs. Xgboost](http://maxberggren.se/2017/06/18/deep-learning-vs-xgboost/)
- [The limitations of deep learning](https://blog.keras.io/the-limitations-of-deep-learning.html)


## Books
- #BOOK [Deep Learning with R, 2nd Edition (Kalinowski 2022)](https://blogs.rstudio.com/ai/posts/2022-05-31-deep-learning-with-r-2e/)
- #BOOK [Deep Learning Interviews: Hundreds of fully solved job interview questions from a wide range of key topics in AI (Kashani 2022)](https://arxiv.org/abs/2201.00650)
- #BOOK [Physics-based Deep Learning Book (Thuerey 2021)](https://physicsbaseddeeplearning.org/intro.html) ^PBDL
- #BOOK [The Principles of DL Theory: An Effective Theory Approach to Understanding Neural Networks (Roberts 2022)](https://deeplearningtheory.com/PDLT.pdf)
	- https://ai.facebook.com/blog/advancing-ai-theory-with-a-first-principles-understanding-of-deep-neural-networks/
	- #PAPER [The Principles of Deep Learning Theory (Roberts 2021)](https://arxiv.org/abs/2106.10165)
	- [Paper explained](https://www.youtube.com/watch?v=m2bXL5Z5CBM)
- #BOOK [Deep Learning Book (Goodfellow, 2016 MIT)](https://www.deeplearningbook.org/)
	- The Deep Learning textbook is a resource intended to help students and practitioners enter the field of machine learning in general and deep learning in particular
- #BOOK [DL tutorial (LISA Lab, U Montreal)](http://deeplearning.net/tutorial/)
- #BOOK [Deep Learning with Python (Chollet, 2021 MANNING)](https://www.manning.com/books/deep-learning-with-python-second-edition)
	- [1st edition](http://faculty.neu.edu.cn/yury/AAI/Textbook/Deep%20Learning%20with%20Python.pdf)
- #BOOK [Machine learning yearning (Andrew Ng, 2018)](https://freecomputerbooks.com/Machine-Learning-Yearning.html)
	- https://github.com/ajaymache/machine-learning-yearning
- #BOOK [Dive into Deep Learning (Zhang)](https://d2l.ai/index.html)
	- An interactive deep learning book for students, engineers, and researchers. Uses MXNet/Gluon, Pytorch and Tensorflow
	- [Jupyter notebooks for each section](https://en.d2l.ai/d2l-en.zip)
- #BOOK [Introduccion practica con Keras (Torres 2018)](https://torres.ai/deep-learning-inteligencia-artificial-keras/)
- #BOOK [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/index.html)


## Talks
- #TALK [The Future of Sparsity in Deep Learning (Trevor Gale, Phd student Stanford, 2021)](https://events.rainfocus.com/widget/nvidia/nvidiagtc/sessioncatalog/session/1631029840983001jvzq)
- #TALK Deep Learning (Yoshua Bengio, MLSS 2020): 
	- [Part I](https://www.youtube.com/watch?v=c_U4THknoHE)
	- [Part II](https://www.youtube.com/watch?v=PDPdIDihPvc)
- #TALK [Deep Learning Hardware: Past, Present, and Future (Yann LeCun, ISSCC 2019)](https://www.youtube.com/watch?v=YzD7Z2yRL7Y)
- #TALK [Keras, Deep Learning, and the Progress of AI (François Chollet, Lex Fridman Podcast, 2019)](https://www.youtube.com/watch?v=Bo8MY4JpiXE)
- #TALK [Deep Learning and the Future of Artificial Intelligence (Yann LeCun, 2018)](https://www.youtube.com/watch?v=RM-Jtc2ryfM&t=5s)
- #TALK [AI Breakthroughs & Obstacles to Progress, Mathematical and Otherwise (Yann LeCun, 2018)](https://www.youtube.com/watch?v=1_KhJv0Em5Y)
- #TALK [François Chollet at France is AI 2017: Deep Learning: current limits and future perspectives (Chollet 2017)](https://www.youtube.com/watch?v=MUF32XHqM34 )
- #TALK [Power & Limits of Deep Learning (Yann Lecun, 2017)](https://www.youtube.com/watch?v=0tEhw5t6rhc)
- #TALK [The Deep End of Deep Learning (Hugo Larochelle, TEDxBoston 2016)](https://www.youtube.com/watch?v=dz_jeuWx3j0)
- #TALK [How deep neural networks work (Brandon Rohrer)](https://www.youtube.com/watch?v=ILsA4nyG7I0)
	- Simple explanations of DL basics and nice graphics


## Courses
- #COURSE [Introduction to Deep Learning (COMP0090, UCL)](https://github.com/YipengHu/COMP0090 )
- #COURSE [Full Stack Deep Learning](https://fullstackdeeplearning.com/)
	- [Full Stack Deep Learning - Spring 2021](https://fullstackdeeplearning.com/spring2021/)
		- [Lecture 13: ML Teams and Startups](https://fullstackdeeplearning.com/spring2021/lecture-13/)
	- https://fall2019.fullstackdeeplearning.com/
		- https://github.com/full-stack-deep-learning/course-gitbook
- #COURSE [Deep Learning (NYU)](https://atcold.github.io/pytorch-Deep-Learning/)
	- https://github.com/Atcold/pytorch-Deep-Learning (pytorch)
- #COURSE [Deep Learning (CS230, Stanford)](http://cs230.stanford.edu/)
	- [Cheatsheets](https://github.com/afshinea/stanford-cs-230-deep-learning)
- #COURSE [Tensorflow for Deep Learning Research (CS20SI, Stanford)](http://web.stanford.edu/class/cs20si/syllabus.html)
- #COURSE [DeepMind x UCL | Deep Learning Lecture Series 2020](https://www.youtube.com/playlist?list=PLqYmG7hTraZCDxZ44o4p3N5Anz3lLRVZF)
- #COURSE [Introduction to Deep Learning (6.S191, MIT)](http://introtodeeplearning.com/)
	- [MIT Introduction to Deep Learning | 6.S191 | 2022](https://www.youtube.com/watch?v=7sB052Pz0sQ)
- #COURSE [MIT Deep Learning and Artificial Intelligence Lectures](https://deeplearning.mit.edu/)
	- [Youtube playlist](https://www.youtube.com/watch?v=0VH1Lim8gL8&list=PLrAXtmErZgOeiKm4sgNOknGvNjby9efdf)
	- [Deep Learning State of the Art (2020)](https://www.youtube.com/watch?v=0VH1Lim8gL8)
- #COURSE [Introduction to Deep Learning (MIT 6.S191)](http://introtodeeplearning.com/)
- #COURSE [Intro to Neural Networks and Machine Learning (CSC 321, UToronto)](http://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/)
- #COURSE [Deep Learning nanodegree (Udacity)](https://www.udacity.com/course/deep-learning-nanodegree--nd101)
	- https://github.com/udacity/deep-learning-v2-pytorch
	- https://www.udacity.com/course/deep-learning-pytorch--ud188
- #COURSE [Deep Learning with PyTorch: Zero to GANs (Jovian)](https://jovian.ai/learn/deep-learning-with-pytorch-zero-to-gans)
- #COURSE [Fast AI - Practical Deep Learning For Coders](http://course.fast.ai/)
	- Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD - the book and the course
	- https://github.com/fastai/fastbook
- #COURSE [Deep Learning course (U Paris-Saclay)](https://m2dsupsdlclass.github.io/lectures-labs/)
- #COURSE [Introduction to Machine Learning and Neural Networks (Uniandes)](https://albahnsen.com/courses/applied-deep-learning/)
	- https://github.com/albahnsen/AppliedDeepLearningClass
- #COURSE [Deep learning specialization (deeplearning.ai, Coursera, Andrew Ng)](https://www.coursera.org/specializations/deep-learning)
	- https://www.deeplearning.ai/deep-learning-specialization/
- #COURSE [Neural Networks (U Sherbrooke)](http://info.usherbrooke.ca/hlarochelle/neural_networks/description.html)
- #COURSE [The Neural Aesthetic (ITP-NYU)](http://ml4a.github.io/classes/itp-F18/)


## Code
State of ML frameworks: 
- https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/
- https://towardsdatascience.com/tensorflow-or-pytorch-146f5397278a

- #CODE [Tensorflow, keras](AI/DS%20and%20DataEng/Tensorflow,%20keras.md)
- #CODE [Pytorch](AI/DS%20and%20DataEng/Pytorch.md)
- #CODE [Ivy](https://github.com/unifyai/ivy)
	- The unified machine learning framework, enabling framework-agnostic functions, layers and libraries
	- [lets-unify.ai](https://lets-unify.ai/ "https://lets-unify.ai")
	- #PAPER [Ivy: Templated Deep Learning for Inter-Framework Portability (Lenton 2021)](https://arxiv.org/abs/2102.02886)
	- https://medium.com/@unifyai/why-unify-21b502f2015e
	- https://medium.com/@unifyai/standardization-7726c5113e4
- #CODE [Huggingface](https://huggingface.co/)
	- Build, train and deploy state of the art models powered by the reference open source in ML
	- [Datasets](https://github.com/huggingface/datasets)
	- [Datasets-viewer](https://github.com/huggingface/datasets-viewer)
		- https://huggingface.co/datasets/viewer/
	- [Transformers](https://github.com/huggingface/transformers)
- #CODE [Triton](https://github.com/openai/triton)
	- language and compiler for writing highly efficient custom Deep-Learning primitives
	- https://openai.com/blog/triton/
	- https://www.infoq.com/news/2021/08/openAI-triton/
	- Triton uses Python as its base. The developer writes code in Python using Triton’s libraries, which are then JIT-compiled to run on the GPU. This allows integration with the rest of the Python ecosystem, currently the biggest destination for developing machine-learning solutions
- #CODE [Oneflow](https://github.com/Oneflow-Inc/oneflow)
	- OneFlow is a performance-centered and open-source deep learning framework
	- http://www.oneflow.org/
- #CODE [MindSpore (Huawei)](https://github.com/mindspore-ai/mindspore) ^huaweimindpore
	- https://towardsdatascience.com/program-your-first-neural-network-with-huawei-mindspore-1fc50023e90d
	- https://towardsdatascience.com/huaweis-mindspore-a-new-competitor-for-tensorflow-and-pytorch-d319deff2aec
	- https://www.mindspore.cn/en
- #CODE [Tensorlayer - Deep Learning and Reinforcement Learning Library for Scientists and Engineers](https://github.com/tensorlayer/tensorlayer)
	- http://tensorlayer.org/
- #CODE [Elegy - Neural Networks framework based on Jax and inspired by Keras](https://github.com/poets-ai/elegy)
	- https://poets-ai.github.io/elegy/
	- See [Mathematical Optimization](AI/Math%20and%20Statistics/Mathematical%20Optimization.md) JAX
- #CODE [Paddle (Baidu)](https://github.com/PaddlePaddle/Paddle)
	- http://www.paddlepaddle.org/
	- PArallel Distributed Deep LEarning: Machine Learning Framework from Industrial Practice
- #CODE [Mxnet (Apache)](https://github.com/apache/incubator-mxnet)
	- http://mxnet.io/
	- [Towards Next Generation Deep Learning Framework](https://mli.github.io/cvpr17/)
- #CODE [Microsoft Cognitive Toolkit (CNTK)](https://github.com/Microsoft/CNTK)
	- https://www.microsoft.com/en-us/research/product/cognitive-toolkit/
	- Microsoft Cognitive Toolkit: A free, easy-to-use, open-source, commercial-grade toolkit that trains deep learning algorithms to learn like the human brain.
	- #TALK https://www.youtube.com/watch?v=9gDDO5ldT-4&feature=youtu.be
- #CODE [Neupy - NeuPy is a Tensorflow based python library for prototyping and building neural networks](https://github.com/itdxer/neupy)
	- http://neupy.com/pages/home.html
- #CODE Chainer - Chainer is a Python-based deep learning framework aiming at flexibility
	- https://github.com/chainer/chainer
- #CODE [Neural Network Console (Sony)](https://dl.sony.com/)
- #CODE [PySyft](https://github.com/OpenMined/PySyft)
	- PySyft is a Python library for secure and private Deep Learning. 
	- PySyft decouples private data from model training, using Federated Learning, Differential Privacy, and Encrypted Computation (like Multi-Party Computation (MPC) and Homomorphic Encryption (HE)) within the main Deep Learning frameworks like PyTorch and TensorFlow.
	- #PAPER [A generic framework for privacy preserving deep learning](https://arxiv.org/abs/1811.04017)
- #CODE [Deep cognition](https://deepcognition.ai/)

### DL for science
- #CODE [Gt4sd-core (IBM)](https://github.com/GT4SD/gt4sd-core) 
	- GT4SD, an open-source library to accelerate hypothesis generation in the scientific discovery process 
	- https://gt4sd.github.io/gt4sd-core/
	- https://research.ibm.com/blog/generative-models-toolkit-for-scientific-discovery
	- https://thenewstack.io/ibms-open-source-gt4sd-generates-ideas-for-scientists/
- #CODE [Deep Search](https://github.com/DS4SD)
	- https://ds4sd.github.io/
	- Deep Search extracts and structures data from documents in four steps: Parse, Interpret, Index, and Integrate
	- [Handling Scientific Articles with Deep Search](https://opensource.science/handling-scientific-articles-with-deep-search-d3d7adebd3)


## References
- #PAPER [Deep learning in NNs: An overview (Schmidhuber 2015)](https://www.sciencedirect.com/science/article/pii/S0893608014002135)
- #PAPER [Deep learning (LeCun 2015)](https://www.nature.com/articles/nature14539) ^dllecun15
	- https://www.researchgate.net/profile/Y_Bengio/publication/277411157_Deep_Learning/links/55e0cdf908ae2fac471ccf0f/Deep-Learning.pdf
- #PAPER [Deep Neural Decision Forests (Kontschieder 2016)](https://www.ijcai.org/Proceedings/16/Papers/628.pdf)
	- #CODE https://keras.io/examples/structured_data/deep_neural_decision_forests/
- #PAPER [On the Origin of Deep Learning (Wang 2017)](https://arxiv.org/abs/1702.07800v4 )
- #PAPER [Representation Learning on Large and Small Data (Chou 2017)](https://arxiv.org/abs/1707.09873v1)
- #PAPER [Deep Learning in Neural Networks: An Overview (Schmidhuber, 2018)](https://arxiv.org/abs/1404.7828)
- #PAPER [Deep Learning as a Mixed Convex-Combinatorial Optimization Problem (Friesen 2018)](https://arxiv.org/abs/1710.11573)
- #PAPER [Using Deep Neural Networks for Inverse Problems in Imaging: Beyond Analytical Methods (Lucas, 2018)](https://ieeexplore.ieee.org/document/8253590)
	- http://decsai.ugr.es/vip/files/journals/08253590.pdf
- #PAPER [Neural Tangent Kernel: Convergence and Generalization in Neural Networks (Jacot 2018)](https://arxiv.org/abs/1806.07572#)
	- https://www.quantamagazine.org/a-new-link-to-an-old-model-could-crack-the-mystery-of-deep-learning-20211011/
- #PAPER [Neural circuit policies enabling auditable autonomy (Lechner 2020)](https://www.nature.com/articles/s42256-020-00237-3)
	- #CODE https://github.com/mlech26l/keras-ncp
	- https://www.csail.mit.edu/news/new-deep-learning-models-require-fewer-neurons
	- https://www.marktechpost.com/2021/10/19/mit-csail-tu-wien-and-ist-researchers-introduce-deep-learning-models-that-require-fewer-neurons/
- #PAPER [Implicitly Defined Layers in Neural Networks (Zhang 2020)](https://arxiv.org/abs/2003.01822)
- #PAPER [A Mathematical Principle of Deep Learning: Learn the Geodesic Curve in the Wasserstein Space (Gai 2021)](https://arxiv.org/abs/2102.09235)
- #PAPER [Why is AI hard and Physics simple? (Roberts 2021)](https://arxiv.org/abs/2104.00008)
- #PAPER [Deep Learning for AI (By Yoshua Bengio, Yann Lecun, Geoffrey Hinton, Turing lecture, 2021)](https://dl.acm.org/doi/10.1145/3448250)
	- https://cacm.acm.org/magazines/2021/7/253464-deep-learning-for-ai/fulltex
- #PAPER [Self-Tuning for Data-Efficient Deep Learning (Wang 2021)](https://arxiv.org/abs/2102.12903)
	- #CODE https://github.com/thuml/Self-Tuning
	- #TALK https://recorder-v3.slideslive.com/#/share?share=40334&s=f7988e61-bece-4a7a-a6ba-3e1a2b49b37b
- #PAPER [Neural circuit policies enabling auditable autonomy (Lechner 2021)](https://www.nature.com/articles/s42256-020-00237-3)
	- #CODE https://github.com/mlech26l/keras-ncp
- #PAPER [Controlling Neural Networks with Rule Representations (Seo 2021)](https://arxiv.org/abs/2106.07804)
	- https://ai.googleblog.com/2022/01/controlling-neural-networks-with-rule.html
- #PAPER [Deep physical neural networks trained with backpropagation (Wrigth 2022)](https://www.nature.com/articles/s41586-021-04223-6)
- #PAPER [Ensemble deep learning: A review (Ganaie 2022)](https://arxiv.org/pdf/2104.02395)            
- #PAPER [projUNN: efficient method for training deep networks with unitary matrices (Kiani 2022)](https://arxiv.org/pdf/2203.05483)            
	- https://www.marktechpost.com/2022/04/09/researchers-including-yann-lecun-propose-projunn-an-efficient-method-for-training-deep-neural-networks-with-unitary-matrices/
- #PAPER [LilNetX: Lightweight Networks with EXtreme Model Compression and Structured Sparsification (Girish 2022)](https://arxiv.org/pdf/2204.02965)            
	- https://www.marktechpost.com/2022/04/12/researchers-propose-a-novel-framework-lilnetx-for-training-deep-neural-network-with-extreme-model-compression-and-structured-sparsification/

### Generalization
- http://www.inference.vc/everything-that-works-works-because-its-bayesian-2/

- #PAPER [Understanding deep learning requires re-thinking generalization (Zhang 2016)](https://arxiv.org/abs/1611.03530)
	- https://blog.acolyer.org/2017/05/11/understanding-deep-learning-requires-re-thinking-generalization/
	- https://www.quora.com/Why-is-the-paper-%E2%80%9CUnderstanding-Deep-Learning-Requires-Rethinking-Generalization%E2%80%9D-important
- #PAPER [A Closer Look at Memorization in Deep Networks (Arpit 2017)](https://arxiv.org/abs/1706.05394)
- #PAPER [Deep nets don’t learn via memorization (Krueger 2017)](https://openreview.net/pdf?id=rJv6ZgHYg)
- #PAPER [Rethinking generalization requires revisiting old ideas: statistical mechanics approaches and complex learning behavior (Martin 2017)](https://arxiv.org/abs/1710.09553)
- #PAPER [Ablation Studies in Artificial Neural Networks (Meyes 2019)](https://arxiv.org/abs/1901.08644)
- #PAPER [Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning (Allen-Zhu 2020)](https://arxiv.org/abs/2012.09816)
	- https://www.microsoft.com/en-us/research/blog/three-mysteries-in-deep-learning-ensemble-knowledge-distillation-and-self-distillation/
- #PAPER [The Deep Bootstrap Framework: Good Online Learners are Good Offline Generalizers (Nakkiran 2021)](https://arxiv.org/abs/2010.08127)
	- https://ai.googleblog.com/2021/03/a-new-lens-on-understanding.html
- #PAPER [Predicting trends in the quality of state-of-the-art neural networks without access to training or testing data (Martin 2021)](https://www.nature.com/articles/s41467-021-24025-8)
- #PAPER [Stochastic Training is Not Necessary for Generalization (Geiping 2021)](https://arxiv.org/abs/2109.14119)
- #PAPER [Underspecification Presents Challenges for Credibility in Modern Machine Learning (D'Amour 2021)](https://arxiv.org/abs/2011.03395)
	- https://ai.googleblog.com/2021/10/how-underspecification-presents.html
- #PAPER [Learning in High Dimension Always Amounts to Extrapolation (Balestriero 2021)](https://arxiv.org/abs/2110.09485)
	- In order for NNs to succeed at solving a task, they have to operate in the “extrapolation” regime! But not all of them generalise as well as others. So this opens up new questions about the relationship between this specific notion of extrapolation and generalisation more generally.
- #PAPER [Incorporating Symmetry into Deep Dynamics Models for Improved Generalization (Wang 2021)](https://arxiv.org/abs/2002.03061)
	- #CODE https://github.com/Rose-STL-Lab/Equivariant-Net
- #PAPER [Grokking - Generatlization beyond overfitting on small algorithmic datasets (Power 2022)](https://arxiv.org/abs/2201.02177v1)
	- [Paper explained](https://www.youtube.com/watch?v=dND-7llwrpw)


### Regularization
- In general, techniques aimed at reducing overfitting and improve generalization
- [Overfit and underfit](https://www.tensorflow.org/tutorials/keras/overfit_and_underfit)
- [Regularization techniques for training deep neural networks](https://theaisummer.com/regularization/)
- https://towardsdatascience.com/regularization-in-deep-learning-l1-l2-and-dropout-377e75acc036
- https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-learning-with-weight-regularization/
- https://medium.com/intelligentmachines/convolutional-neural-network-and-regularization-techniques-with-tensorflow-and-keras-5a09e6e65dc7

#### Data augmentation
- https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html
- https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/
- #PAPER [A survey on Image Data Augmentation for Deep Learning (Shorten 2019)](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0)
- #PAPER [AutoAugment: Learning Augmentation Policies from Data (Cubuk 2019)](https://arxiv.org/pdf/1805.09501)

#### Dropout
- http://www.cs.toronto.edu/~hinton/absps/dropout.pdf
- https://pgaleone.eu/deep-learning/regularization/2017/01/10/anaysis-of-dropout/
- [12 Main Dropout Methods: Mathematical and Visual Explanation for DNNs, CNNs, and RNNs](https://towardsdatascience.com/12-main-dropout-methods-mathematical-and-visual-explanation-58cdc2112293)

- #PAPER [Dropout: A Simple Way to Prevent Neural Networks from Overfitting (Srivastava 2014)](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)
- #PAPER [Efficient Object Localization Using Convolutional Networks (Tompson 2015)](https://arxiv.org/abs/1411.4280v3)
	- Proposed spatial dropout
- #PAPER [Analysis on the Dropout Effect in Convolutional Neural Networks (Park 2017)](https://link.springer.com/chapter/10.1007/978-3-319-54184-6_12)
	- http://mipal.snu.ac.kr/images/1/16/Dropout_ACCV2016.pdf
- #PAPER [Effective and Efficient Dropout for Deep Convolutional Neural Networks (Cai 2020)](https://arxiv.org/abs/1904.03392)

### Stochastic depth
- #PAPER [Deep Networks with Stochastic Depth (Huang 2016)](https://arxiv.org/pdf/1603.09382)
	- Stochastic depth is a regularization technique that randomly drops a set of layers. During inference, the layers are kept as they are. It is very much similar to Dropout but only that it operates on a block of layers rather than individual nodes present inside a layer

#### Normalization
- Normalization techniques also improve generalization error, providing some regularization
- [Normalization Techniques in Deep Neural Networks](https://medium.com/techspace-usict/normalization-techniques-in-deep-neural-networks-9121bf100d8)
- [Different Types of Normalization in Tensorflow](https://towardsdatascience.com/different-types-of-normalization-in-tensorflow-dac60396efb0)
- [Normalization in Deep Learning](https://arthurdouillard.com/post/normalization/)
- https://sebastianraschka.com/faq/docs/scale-training-test.html 
- Data normalization/standardization can be used as an alternative (before training) to synch batchnorm (multi-gpu training)
- [Spectral normalization](https://sthalles.github.io/advanced_gans/)

- #PAPER [Normalization Techniques in Training DNNs: Methodology, Analysis and Application (Huang 2020)](https://arxiv.org/abs/2009.12836)

##### BatchNorm
- #PAPER [ Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (Ioffe 2015)](https://arxiv.org/abs/1502.03167)
	- #TALK https://www.youtube.com/watch?v=ZOabsYbmBRM&feature=youtu.be
	- http://stackoverflow.com/questions/34716454/where-do-i-call-the-batchnormalization-function-in-keras
	- Slower convergence w/o BN, BN can be applied on top of standardization 
	- Synch BatchNorm appears in TF 2.2, for multi-gpu training 
		- https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/SyncBatchNormalization 
- #PAPER [Rethinking the Usage of Batch Normalization and Dropout (Chen 2019)](https://arxiv.org/abs/1905.05928)

### Activations
- https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html
- https://mlfromscratch.com/activation-functions-explained/
- [RELU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))
	- https://www.quora.com/What-is-special-about-rectifier-neural-units-used-in-NN-learning
- http://stats.stackexchange.com/questions/126238/what-are-the-advantages-of-relu-over-sigmoid-function-in-deep-neural-network
- https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions

### Loss/Cost functions
- Cross entropy
	- http://neuralnetworksanddeeplearning.com/chap3.html
	- https://en.wikipedia.org/wiki/Cross_entropy
	- http://www.kdnuggets.com/2017/02/gentlest-introduction-tensorflow-part-4.html
- Perceptual loss, image reconstruction
	- https://arxiv.org/pdf/1511.06409.pdf (Learning to Generate Images With Perceptual Similarity Metrics) 
	- #PAPER [Loss Functions for Image Restoration with Neural Networks (Zhao 2018)](https://arxiv.org/abs/1511.08861)
	- https://medium.com/@sanari85/rediscovery-of-ssim-index-in-image-reconstruction-ssim-as-a-loss-function-a1ffef7d2be 
		- We use three different metric for comparing each different methods such as DSSIM, MSE, and MAE. Structural dissimilarity(DSSIM) is an image distance metric, that corresponds better to the human perception than MAE or RMSE. Mean Squared Error (MSE) measures the average of the squares of the errors that is, the average squared difference between the estimated values and the actual value. Mean Absolute Error (MAE) is the average distance between each pixel point. https://arxiv.org/abs/2001.05372
- [Deep learning image enhancement insights on loss function engineering](https://towardsdatascience.com/deep-learning-image-enhancement-insights-on-loss-function-engineering-f57ccbb585d7)
- Mean squared logarithmic error 
	- https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/mean-squared-logarithmic-error-(msle) 
	- https://medium.com/@olegrybkin_20684/the-reasonable-ineffectiveness-of-mse-pixel-loss-for-future-prediction-and-what-to-do-about-it-4dca8152355d 

### Optimizers and backpropagation
- [How to use Learning Curves to Diagnose Machine Learning Model Performance](https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/)
- https://www.quora.com/Intuitively-how-does-mini-batch-size-affect-the-performance-of-stochastic-gradient-descent
- [Keras optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/ )
- [Adam](http://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)
- [An overview of gradient descent optimization algorithms (2016)](https://ruder.io/optimizing-gradient-descent/index.html#otherrecentoptimizers )
- https://hackernoon.com/some-state-of-the-art-optimizers-in-neural-networks-a3c2ba5a5643 
- https://www.jeremyjordan.me/neural-networks-training/
- http://colah.github.io/posts/2015-08-Backprop/
- [Back-propagation - Math Simplified](https://github.com/DebPanigrahi/Machine-Learning/blob/master/back_prop.ipynb)
- https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/
- https://venturebeat.com/2020/12/16/at-neurips-2020-researchers-proposed-faster-more-efficient-alternatives-to-backpropagation/amp/

- #PAPER [On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima (Shirish Keshkar 2017)](https://arxiv.org/abs/1609.04836)
- #PAPER [Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour (Goyal 2018)](https://arxiv.org/abs/1706.02677)
- #PAPER [Decoupled Weight Decay Regularization (Loshchilov 2018)](https://arxiv.org/abs/1711.05101)
	- AdamW optimizer
	- #CODE https://www.tensorflow.org/addons/api_docs/python/tfa/optimizers/AdamW
	- https://www.fast.ai/2018/07/02/adam-weight-decay/
- #PAPER [Deep Double Descent: Where Bigger Models and More Data Hurt (Nakkiran 2019)](https://arxiv.org/abs/1912.02292)
	- https://openai.com/blog/deep-double-descent/
	- https://medium.com/mlearning-ai/double-descent-8f92dfdc442f
- #PAPER [Reconciling modern machine learning practice and the bias-variance trade-off (Belkin 2019)](https://arxiv.org/abs/1812.11118)
	- [Paper explained](https://www.youtube.com/watch?v=ZAW9EyNo2fw)
- #PAPER [Deep Double Descent: Where Bigger Models and More Data Hurt (Nakkiran 2020)](https://arxiv.org/abs/1912.02292)
	- https://openai.com/blog/deep-double-descent/
- #PAPER [Descending through a Crowded Valley -- Benchmarking Deep Learning Optimizers (Schmidt 2020)](https://arxiv.org/abs/2007.01547)
	- [Paper explained](https://www.youtube.com/watch?v=DiNzQP7kK-s)
- #PAPER [Early Stopping in Deep Networks: Double Descent and How to Eliminate it (Heckel 2020)](https://arxiv.org/abs/2007.10099)
	- contrary to model-wise double descent, epoch-wise double descent is not a phenomena tied o over-parameterization
	- both under- and overparameterized models can have epoch-wise double descent 
	- #CODE https://github.com/MLI-lab/early_stopping_double_descent


### Efficiency and performance
- #PAPER [Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better (Menghani 2021)](https://arxiv.org/abs/2106.08962)
	- https://analyticsindiamag.com/how-to-build-smaller-faster-better-deep-learning-models/

### Scaling and distributed DL
See [[AI/DS and DataEng/Distributed DL]]

### Attention
See "For NLP" section in [[AI/Deep learning/Transformers]] and "Channel/Visual attention" section in [[/AI/Deep learning/CNNs]]

- #COURSE [Attention and Memory in Deep Learning (DeepMind x UCL | Deep Learning Lectures | 8/12)](https://www.youtube.com/watch?v=AIiwuClvH6k)


### Explainability methods for Neural Networks
See [[AI/Deep learning/Explainability methods for NNs]]

## Applications

### Deep learning for multi-dimensional data
See [[AI/Computer Vision/Video segmentation and prediction]], [[AI/Deep learning/Encoder-decoder networks]], [[AI/Deep learning/Transformers]] and [[AI/Deep learning/Generative modelling]]
- #PAPER [Demystifying Deep Learning in Predictive Spatio-Temporal Analytics: An Information-Theoretic Framework (Tan 2020)](https://arxiv.org/abs/2009.06304)

### Deep learning for tabular data
- [An Introduction to Deep Learning for Tabular Data](https://www.fast.ai/2018/04/29/categorical-embeddings/)
- [Applying Deep Learning on Tabular Data Using TensorFlow 2.0](https://pdf.co/blog/deep-learning-on-tabular-data-using-tensorflow-20)
- [A short chronology of deep learning for tabular data (Sebastian Rschka)](https://sebastianraschka.com/blog/2022/deep-learning-for-tabular-data.html)
- #CODE See Pytorch tabular in [[AI/DS and DataEng/Pytorch]] 
- #PAPER [Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data (Popov 2019)](https://arxiv.org/abs/1909.06312)
- #PAPER [TabNet: Attentive Interpretable Tabular Learning (Arik 2020)](https://arxiv.org/abs/1908.07442)
- #PAPER [Converting tabular data into images for deep learning with convolutional neural networks (Zhu 2021)](https://www.nature.com/articles/s41598-021-90923-y)
- #PAPER [Tabular Data: Deep Learning is Not All You Need (Shwartz-Ziv 2021)](https://arxiv.org/abs/2106.03253)
- #PAPER [XBNet: An Extremely Boosted Neural Network (Sarkar 2021)](https://arxiv.org/abs/2106.05239)
	- #CODE [XBNet](https://github.com/tusharsarkar3/XBNet)
	- Boosted neural network for tabular data
	- https://analyticsindiamag.com/guide-to-xbnet-an-extremely-boosted-neural-network/
- #PAPER [Revisiting Deep Learning Models for Tabular Data (Gorishniy 2021)](https://arxiv.org/abs/2106.11959)
	- #CODE [RDTL (Yandex)](https://github.com/yandex-research/rtdl)
	- https://yandex-research.github.io/rtdl/
- #PAPER [TABBIE: Pretrained Representations of Tabular Data (Lida 2021)](https://arxiv.org/abs/2105.02584v1)

### Deep learning for scientific discovery
See relevant code in [[AI/Deep learning/DL#DL for science]]. See [[AI/Deep learning/Neural ODEs]]
- #PAPER [A Survey of Deep Learning for Scientific Discovery (Raghu & Schmidt, 2020)](https://arxiv.org/abs/2003.11755) ^dlscience20
- #PAPER [DeepXDE: A deep learning library for solving differential equations (Lu 2020)](https://arxiv.org/abs/1907.04502)
	- #CODE https://github.com/lululxvi/deepxde
	- https://deepxde.readthedocs.io/en/latest/
- #PAPER [SciANN: A Keras/Tensorflow wrapper for scientific computations and physics-informed deep learning using artificial neural networks (Haghighat 2020)](https://arxiv.org/abs/2202.07575)
	 - #CODE https://github.com/sciann/sciann
- #PAPER [Learning an Accurate Physics Simulator via Adversarial Reinforcement Learning (Jiang 2021)](http://ai.googleblog.com/2021/06/learning-accurate-physics-simulator-via.html "Learning an Accurate Physics Simulator via Adversarial Reinforcement Learning")
- #PAPER [Data-driven discovery of Green’s functions with human-understandable deep learning (Boulle 2022)](https://www.nature.com/articles/s41598-022-08745-5)
	- https://phys.org/news/2022-04-rational-neural-network-advances-partial.html

### Multimodal learning
See [[AI/Deep learning/Multimodal learning]]

### DL for NLP, time series and sequence modelling
See [[AI/Time Series analysis]], [[AI/Forecasting]] and "Deep learning approaches" in [[AI/NLP]]


## Architectures and model families
- [The neural network zoo](http://www.asimovinstitute.org/neural-network-zoo/)
- [Deep Learning Tips and Tricks cheatsheet](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-deep-learning-tips-and-tricks)
- [A Visual and Interactive Guide to the Basics of NNs](https://jalammar.github.io/visual-interactive-guide-basics-neural-networks/)
- [A Visual And Interactive Look at Basic Neural Network Math](https://jalammar.github.io/feedforward-neural-networks-visual-interactive/)
- #CODE [Model Zoo](https://modelzoo.co/)
- #CODE [Deep Learning Models (Raschka)](https://github.com/rasbt/deeplearning-models)

### Geometric DL
See [[AI/Deep learning/Geometric deep learning]]

### MLPs
See [[AI/Deep learning/MLPs]]

### Deep belief network
See [[AI/Deep learning/Deep belief network]]

### Autoencoders
See [[AI/Deep learning/Autoencoders]]

### CNNs
See [[AI/Deep learning/CNNs]]

### RNNs
See [[AI/Deep learning/RNNs]]

### CapsNets
See [[AI/Deep learning/CapsNets]]

### GANs
See [[AI/Deep learning/GANs]]

### Diffusion models
See [[AI/Deep learning/Diffusion models]]

### GNNs
See [[AI/Deep learning/GNNs]]

### Residual and dense neural networks
See [[AI/Deep learning/Residual and dense neural networks]]

### Neural ODEs
See [[AI/Deep learning/Neural ODEs]]

### Fourier Neural Operators
See [[AI/Deep learning/Fourier Neural Operators]]

### Transformers
See [[AI/Deep learning/Transformers]]

### GFlowNets
See [[AI/Deep learning/GFlowNets]]

### Neural Cellular Automata
See [[AI/Deep learning/Neural Cellular Automata]]

### Neural processes
See [[AI/Deep learning/Neural processes]]

### Bayesian/probabilistic DL
See [[AI/Deep learning/Probabilistic deep learning]]