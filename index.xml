<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>CarlosGG's Knowledge Garden ü™¥ on</title><link>https://carlos-gg.github.io/digitalgarden/</link><description>Recent content in CarlosGG's Knowledge Garden ü™¥ on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://carlos-gg.github.io/digitalgarden/index.xml" rel="self" type="application/rss+xml"/><item><title>Active learning</title><link>https://carlos-gg.github.io/digitalgarden/AI/Active-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Active-learning/</guid><description>Resources https://en.wikipedia.org/wiki/Active_learning_(machine_learning) Active learning refers to algorithms that take an active role in the selection of which ex-amples are labeled.</description></item><item><title>AI for good (AI4G)</title><link>https://carlos-gg.github.io/digitalgarden/AI4G/AI4good/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI4G/AI4good/</guid><description>References #PAPER AI for social good: unlocking the opportunity for positive impact (Tomasev 2020) Resources (Legacy) Overview of the Data Science and AI for good movement, on Medium Public lecture slides: Beyond the Hype - how we can make AI work for humanity (Yoshua Bengio) Twitter : data4good hashtag DS for Good resources Report - AI for Good How to Use Data for Good to Impact Society Challenges and opportunities of Artificial Intelligence for Good.</description></item><item><title>AI4G - Bootcamps and fellowships</title><link>https://carlos-gg.github.io/digitalgarden/AI4G/Bootcamps-and-fellowships/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI4G/Bootcamps-and-fellowships/</guid><description>AI4Good lab https://www.ai4goodlab.com/ We train and mentor women to succeed in AI Data Science for Social Good https://www.</description></item><item><title>AI4G - Entrepreneurship</title><link>https://carlos-gg.github.io/digitalgarden/AI4G/Entrepreneurship/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI4G/Entrepreneurship/</guid><description>Google impact challenge https://www.google.org/opportunities/ Working together to apply AI for social good Google.org is issuing an open call to organizations around the world to submit their ideas for how they could use AI to help address societal challenges.</description></item><item><title>Anomaly and Outlier Detection</title><link>https://carlos-gg.github.io/digitalgarden/AI/Anomaly-and-Outlier-Detection/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Anomaly-and-Outlier-Detection/</guid><description>See Active learning for anomaly discovery
Resources Most of the outlier detection approaches belong to Unsupervised learning although it might be framed as a Semi-supervised learning problem.</description></item><item><title>Artificial Intelligence</title><link>https://carlos-gg.github.io/digitalgarden/AI/AI/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/AI/</guid><description>Resources The expression artificial intelligence is an umbrella term encompassing a suite of technologies that can perform complex tasks when acting in conditions of uncertainty, including visual perception, speech recognition, natural language processing, reasoning, learning from data, and a range of optimisation problems.</description></item><item><title>Autoencoders</title><link>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/Autoencoders/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/Autoencoders/</guid><description>Resources https://en.wikipedia.org/wiki/Autoencoder Dimensionality Reduction The classical approach for unsupervised learning using neural networks. The basic version consists of a Multilayer Perceptron (MLP) where the input and output layer have the same size and a smaller hidden layer is trained to recover the input.</description></item><item><title>Automated planning</title><link>https://carlos-gg.github.io/digitalgarden/AI/Automated-planning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Automated-planning/</guid><description>Resources AI Planning is a branch of artificial intelligence that concerns the realization of strategies or action sequences, typically for execution by intelligent agents, autonomous robots and unmanned vehicles.</description></item><item><title>AutoML</title><link>https://carlos-gg.github.io/digitalgarden/AI/AutoML/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/AutoML/</guid><description>See Model selection and tuning
Resources https://en.wikipedia.org/wiki/Automated_machine_learning Automated machine learning (AutoML) is the process of automating the process of applying machine learning to real-world problems.</description></item><item><title>Background subtraction</title><link>https://carlos-gg.github.io/digitalgarden/AI/Computer-Vision/Background-subtraction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Computer-Vision/Background-subtraction/</guid><description>Resources https://en.wikipedia.org/wiki/Foreground_detection https://github.com/murari023/awesome-background-subtraction Foreground detection is one of the major tasks in the field of computer vision and image processing whose aim is to detect changes in image sequences.</description></item><item><title>Bayesian modelling</title><link>https://carlos-gg.github.io/digitalgarden/AI/Bayesian-modelling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Bayesian-modelling/</guid><description>See Monte Carlo methods and Probabilistic deep learning
Resources https://en.wikipedia.org/wiki/Bayesian_statistics https://en.wikipedia.org/wiki/Bayesian_inference http://brohrer.github.io/how_bayesian_inference_works.html http://willwolf.io/en/2017/02/07/bayesian-inference-via-simulated-annealing/ #TALK Bayesian Inference, Shakir Mohamed, MLSS 2020: Part I Part II Bayesian vs frequentist discussion http://jakevdp.</description></item><item><title>Capsule Neural networks (CapsNets)</title><link>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/CapsNets/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/CapsNets/</guid><description>Resources https://en.wikipedia.org/wiki/Capsule_neural_network A Capsule Neural Network (CapsNet) is a machine learning system that is a type of artificial neural network (ANN) that can be used to better model hierarchical relationships.</description></item><item><title>Causality</title><link>https://carlos-gg.github.io/digitalgarden/AI/Causality/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Causality/</guid><description>Resources To Build Truly Intelligent Machines, Teach Them Cause and Effect Representing uncertain knowledge Reasoning over time Making decisions Causal Analysis Introduction - Examples in Python and PyMC Granger causality The Granger causality test is a statistical hypothesis test for determining whether one time series is useful in forecasting another, first proposed in 1969 Granger causality is a fundamental technique for causal inference in time series data, commonly used in the social and biological sciences PCMCI: https://jakobrunge.</description></item><item><title>Class imbalance</title><link>https://carlos-gg.github.io/digitalgarden/AI/Supervised-Learning/Class-imbalance/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Supervised-Learning/Class-imbalance/</guid><description>Resources https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis http://www.marcoaltini.com/blog/dealing-with-imbalanced-data-undersampling-oversampling-and-proper-cross-validation http://www.alfredo.motta.name/cross-validation-done-wrong/ http://www.win-vector.com/blog/2015/02/does-balancing-classes-improve-classifier-performance/ http://www.chioka.in/class-imbalance-problem/ http://www.kdnuggets.com/2017/06/7-techniques-handle-imbalanced-data.html https://svds.com/learning-imbalanced-classes/ Conventional algorithms are often biased towards the majority class because their loss functions attempt to optimize quantities such as error rate, not taking the data distribution into consideration.</description></item><item><title>Classification</title><link>https://carlos-gg.github.io/digitalgarden/AI/Supervised-Learning/Classification/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Supervised-Learning/Classification/</guid><description>Resources https://github.com/jmportilla/Udemy---Machine-Learning/blob/master/Multi-Class%20Classification.ipynb Comparison of classifiers https://www.quora.com/What-are-the-advantages-of-different-classification-algorithms http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html https://medium.com/@maheshkkumar/implementing-a-binary-classifier-in-python-b69d08d8da21#.goynlh7ah Metrics https://www.neuraldesigner.com/blog/methods-binary-classification http://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/ http://docs.aws.amazon.com/machine-learning/latest/dg/binary-model-insights.html Naive Bayes https://blancosilva.</description></item><item><title>Clustering</title><link>https://carlos-gg.github.io/digitalgarden/AI/Unsupervised-learning/Clustering/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Unsupervised-learning/Clustering/</guid><description>Resources Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters).</description></item><item><title>Computer Vision</title><link>https://carlos-gg.github.io/digitalgarden/AI/Computer-Vision/Computer-vision/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Computer-Vision/Computer-vision/</guid><description>See
CNNs &amp;ldquo;MLPs for vision and language&amp;rdquo; section in MLPs &amp;ldquo;For Computer Vision&amp;rdquo; section in Transformers &amp;ldquo;Generative models for Image data&amp;rdquo; section in Generative modelling GANs Resources https://github.</description></item><item><title>Convolutional Neural Networks (CNNs)</title><link>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/CNNs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/CNNs/</guid><description>Resources A convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery.</description></item><item><title>Dask</title><link>https://carlos-gg.github.io/digitalgarden/AI/Data-Science-Data-Engineering/Dask/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Data-Science-Data-Engineering/Dask/</guid><description>Resources Parallel computing with Dask http://jcrist.github.io/introducing-dask-searchcv.html Talks #TALK Dask for ad hoc distributed computing (Pydata) #TALK Using Dask for Parallel Computing in Python #TALK Parallelizing Scientific Python with Dask | SciPy 2017 Tutorial | James Crist Code #CODE Dask flexible parallel computing library for analytics http://docs.</description></item><item><title>Data Engineering and Computer Science</title><link>https://carlos-gg.github.io/digitalgarden/AI/Data-Science-Data-Engineering/Data-engineering-and-computer-science/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Data-Science-Data-Engineering/Data-engineering-and-computer-science/</guid><description>Resources Data engineering role is ensuring uninterrupted flow of data between servers and applications https://github.com/ossu/computer-science What is Data Engineering and Why Is It So Important?</description></item><item><title>Data Science</title><link>https://carlos-gg.github.io/digitalgarden/AI/Data-Science-Data-Engineering/Data-Science/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Data-Science-Data-Engineering/Data-Science/</guid><description>Resources https://en.wikipedia.org/wiki/Data_science https://github.com/bulutyazilim/awesome-datascience Reproducible Data Analysis in Jupyter (Vanderplas) Cookiecutter Data Science An Executive&amp;rsquo;s Guide To Understanding Cloud-based ML Services Why data-driven science is more than just a buzzword Cheatsheets https://github.</description></item><item><title>Deep belief network</title><link>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/Deep-belief-network/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/Deep-belief-network/</guid><description>Resources In machine learning, a deep belief network (DBN) is a generative graphical model, or alternatively a type of deep neural network, composed of multiple layers of latent variables(&amp;ldquo;hidden units&amp;rdquo;), with connections between the layers but not between units within each layer.</description></item><item><title>Deep Learning (DL)</title><link>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/Deep-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/Deep-learning/</guid><description>See:
Multimodal learning Geometric deep learning Probabilistic deep learning Time Series analysis and Forecasting Resources DL is a branch of Machine Learning and AI based on a set of algorithms that attempt to model high level abstractions in data by using a deep graph with multiple processing layers, composed of multiple linear and non-linear transformations.</description></item><item><title>Diffusion models</title><link>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/Diffusion-models/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/Diffusion-models/</guid><description>Resources Diffusion models are autoencoders (Dieleman | Deepmind) High Fidelity Image Generation Using Diffusion Models Introduction to deep generative modeling: Diffusion-based Deep Generative Models References #PAPER Improved Denoising Diffusion Probabilistic Models (Nichol 2021) #PAPER Cascaded Diffusion Models for High Fidelity Image Generation (Ho 2021) #PAPER Diffusion Models Beat GANs on Image Synthesis (Dhariwal 2021) #CODE https://github.</description></item><item><title>Dimensionality reduction and low-rank modeling</title><link>https://carlos-gg.github.io/digitalgarden/AI/Unsupervised-learning/Dimensionality-reduction-and-low-rank-modeling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Unsupervised-learning/Dimensionality-reduction-and-low-rank-modeling/</guid><description>Resources The Beginner&amp;rsquo;s Guide to Dimensionality Reduction Distances, Neighborhoods, or Dimensions? Projection Literacy for the Analysis of Multivariate Data Decomposing signals in components (matrix factorization problems) Projection techniques transform high-dimensional data to a lower-dimensional space while preserving its main structure.</description></item><item><title>Distances</title><link>https://carlos-gg.github.io/digitalgarden/AI/Math-and-Statistics/Distances/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Math-and-Statistics/Distances/</guid><description>Resources Anscombe dataset Distance = 1 - Similarity Having a set of points (space), a distance d is a function d(x,y) that takes 2 point in the space and produces a real number.</description></item><item><title>Distributed Deep learning</title><link>https://carlos-gg.github.io/digitalgarden/AI/Data-Science-Data-Engineering/Distributed-DL/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Data-Science-Data-Engineering/Distributed-DL/</guid><description>Resources https://d2l.ai/chapter_computational-performance/multiple-gpus.html https://jhui.github.io/2017/03/07/TensorFlow-GPU/ https://www.logicalclocks.com/blog/goodbye-horovod-hello-collectiveallreduce Twelve ways to fool the masses when reporting performance of deep learning workloads Distributed Deep Learning 101: Introduction #TALK ALCF Datascience frameworks: Tensorflow, PyTorch, Keras, and Horovod #TALK Scaling Deep Learning for Scientific Workloads on the #1 Summit Supercomputer Code See Tensorflow, keras, Distributed training section</description></item><item><title>Encoder-decoder networks</title><link>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/Encoder-decoder-networks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/Encoder-decoder-networks/</guid><description>Resources Very common models for semantic segmentation tasks Deep learning architectures composed of two paths, an encoding and a decoding one.</description></item><item><title>Ensemble learning</title><link>https://carlos-gg.github.io/digitalgarden/AI/Supervised-Learning/Ensemble-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Supervised-Learning/Ensemble-learning/</guid><description>Resources https://en.wikipedia.org/wiki/Ensemble_learning In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.</description></item><item><title>Fair AI</title><link>https://carlos-gg.github.io/digitalgarden/AI/FairAI/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/FairAI/</guid><description>Resources The term ‚Äò‚Äòfair AI‚Äô‚Äô refers to probabilistic decision support that prevents disparate harm (or benefit) to different subgroups Timnit Gebru Launches Independent AI Research Institute On Anniversary of Ouster from Google Socially acceptable and fair AI References #PAPER Fair AI : Challenges and Opportunities (Feuerriegel 2020)</description></item><item><title>Feature learning</title><link>https://carlos-gg.github.io/digitalgarden/AI/Feature-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Feature-learning/</guid><description>Resources In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data.</description></item><item><title>Feature selection</title><link>https://carlos-gg.github.io/digitalgarden/AI/Supervised-Learning/Feature-selection/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Supervised-Learning/Feature-selection/</guid><description>See &amp;ldquo;Regularized regression&amp;rdquo; section in Regression
Resources https://en.wikipedia.org/wiki/Feature_selection http://machinelearningmastery.com/an-introduction-to-feature-selection/ http://scikit-learn.org/stable/modules/feature_selection.html Removing features with low variance Univariate feature selection Recursive feature elimination Regularization: http://scikit-learn.</description></item><item><title>Forecasting</title><link>https://carlos-gg.github.io/digitalgarden/AI/Forecasting/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Forecasting/</guid><description>See:
Time Series analysis Regression RNNs CNNs#Sequence time series modelling See For NLP section in Transformers Resources https://en.</description></item><item><title>Fourier Neural Operator</title><link>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/Fourier-Neural-Operators/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/Fourier-Neural-Operators/</guid><description>References #PAPER Fourier Neural Operator for Parametric Partial Differential Equations (Li 2020) #CODE https://github.com/zongyi-li/fourier_neural_operator https://zongyi-li.github.io/blog/2020/fourier-pde/ https://www.technologyreview.com/2020/10/30/1011435/ai-fourier-neural-network-cracks-navier-stokes-and-partial-differential-equations/ Function approximation in Fourier space instead of a the Euclidian (with conventional convolutions) Paper explained</description></item><item><title>Gaussian Process</title><link>https://carlos-gg.github.io/digitalgarden/AI/Supervised-Learning/Gaussian-Process/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Supervised-Learning/Gaussian-Process/</guid><description>Resources In probability theory and statistics, a Gaussian process is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution, i.</description></item><item><title>Generative Adversarial Networks (GANs)</title><link>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/GANs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/GANs/</guid><description>See:
Generative modelling Resources A GAN consists of two networks; a generator (G) and a discriminator (D), given a set of training examples, G will generate outputs and D will classify them as either being from the same distribution as the training examples or not.</description></item><item><title>Generative modeling</title><link>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/Generative-modelling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/Generative-modelling/</guid><description>Resources Generative models Deep Generative Models Taxonomy of Generative Models Jakub Tomczak blog: Introduction to deep generative modeling: Why, Where and How Introduction to deep generative modeling: Energy-based Models Courses #COURSE Deep Generative Modeling: VAEs and GANs (MIT 6.</description></item><item><title>Geometric deep learning</title><link>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/Geometric-deep-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/Geometric-deep-learning/</guid><description>Talks and courses #TALK Geometric Deep Learning: The Erlangen Programme of ML (M Bronstein, ICLR 2021 Keynote) #COURSE Geometric Deep Learning References Review papers:</description></item><item><title>GFlowNets</title><link>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/GFlowNets/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/GFlowNets/</guid><description>Resources Generative Flow Networks (Bengio) Talks #TALK GFlowNets for generative active learning | Amazon Science #TALK Prof.</description></item><item><title>Gradient boosting</title><link>https://carlos-gg.github.io/digitalgarden/AI/Supervised-Learning/Gradient-boosting/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Supervised-Learning/Gradient-boosting/</guid><description>See Ensemble learning
Resources Outperforms Random Forests and AdaBoost. RF is easier to tune and less prone to overfitting http://arogozhnikov.</description></item><item><title>Graph neural networks (GNNs)</title><link>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/GNNs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/GNNs/</guid><description>Resources Graph Neural networks (GNNs) are being widely adopted for diverse applications and domains. This is in part due to their effectiveness on complex data structures, improved performance and scalability, and availability of approaches A Gentle Introduction to Graph Neural Networks Must read papers on GNNs Graph Neural Networks beyond Weisfeiler-Lehman and vanilla Message Passing Time Series Forecasting with Graph Convolutional Neural Network https://medium.</description></item><item><title>Horovod</title><link>https://carlos-gg.github.io/digitalgarden/AI/Data-Science-Data-Engineering/Horovod/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Data-Science-Data-Engineering/Horovod/</guid><description>References #PAPER Horovod: fast and easy distributed deep learning in TensorFlow (Sergeev 2018) #CODE https://github.com/horovod/horovod https://horovod.readthedocs.io/en/latest/keras.html https://horovod.readthedocs.io/en/stable/tensorflow.html https://eng.uber.com/horovod/ Horovod is a distributed training framework for TensorFlow, Keras, PyTorch, and MXNet.</description></item><item><title>Image and video captioning</title><link>https://carlos-gg.github.io/digitalgarden/AI/Computer-Vision/Image-and-video-captioning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Computer-Vision/Image-and-video-captioning/</guid><description>References Review papers:
#PAPER A Systematic Literature Review on Image Captioning (Staniute 2019)
#PAPER Survey of convolutional neural networks for image captioning (Kalra 2020)</description></item><item><title>Image-to-image translation</title><link>https://carlos-gg.github.io/digitalgarden/AI/Computer-Vision/Image-to-image-translation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Computer-Vision/Image-to-image-translation/</guid><description>Resources The task of Image-to-image translation is to learn the mapping from a given image (X) to a specific target image (Y), e.</description></item><item><title>Inpainting</title><link>https://carlos-gg.github.io/digitalgarden/AI/Computer-Vision/Inpainting/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Computer-Vision/Inpainting/</guid><description>Resources https://en.wikipedia.org/wiki/Inpainting Inpainting is a conservation process where damaged, deteriorating, or missing parts of an artwork are filled in to present a complete image https://www.</description></item><item><title>Jupyter</title><link>https://carlos-gg.github.io/digitalgarden/AI/Data-Science-Data-Engineering/Jupyter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Data-Science-Data-Engineering/Jupyter/</guid><description>Resources #CODE Jupyter #CODE Jupyterlab #CODE Jupyter(hub) #CODE Jupyterlite https://blog.jupyter.org/jupyterlite-jupyter-%EF%B8%8F-webassembly-%EF%B8%8F-python-f6e2e41ab3fa #CODE Stickyland Break the linear presentation of Jupyter Notebooks with sticky cells #CODE Papermill - Parameterize, execute, and analyze notebooks #CODE Beaker kernels and extensions Juypterbook - Books with Jupyter Executing notebooks from the command line $ jupyter nbconvert --to notebook --inplace --ExecutePreprocessor.</description></item><item><title>Learning to rank</title><link>https://carlos-gg.github.io/digitalgarden/AI/Learning-to-rank/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Learning-to-rank/</guid><description>Resources https://en.wikipedia.org/wiki/Learning_to_rank Learning to rank or machine-learned ranking (MLR) is the application of machine learning, typically supervised, semi-supervised or reinforcement learning, in the construction of ranking models for information retrieval systems.</description></item><item><title>Linear Algebra</title><link>https://carlos-gg.github.io/digitalgarden/AI/Math-and-Statistics/Linear-Algebra/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Math-and-Statistics/Linear-Algebra/</guid><description>Resources https://en.wikipedia.org/wiki/Linear_algebra The Matrix Cookbook (Brandt 2012) Stanford, Linear algebra refresher Stanford CS229, algebra and calculus refresher: https://github.</description></item><item><title>Machine Learning</title><link>https://carlos-gg.github.io/digitalgarden/AI/Machine-Learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Machine-Learning/</guid><description>Resources Machine learning identifies patterns using statistical learning and computers by unearthing boundaries in data sets. Awesome ML Machine Learning Research Articles Rules of ML (Google) Jason&amp;rsquo;s Machine Learning 101 (Google) https://docs.</description></item><item><title>Math and Statistics</title><link>https://carlos-gg.github.io/digitalgarden/AI/Math-and-Statistics/Math-and-Statistics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Math-and-Statistics/Math-and-Statistics/</guid><description>Resources https://en.wikipedia.org/wiki/Portal:Mathematics https://en.wikipedia.org/wiki/Mathematics Statistics cheatsheet https://github.com/rouseguy/intro2stats Stanford-cs-229 ML, probability and stats refresher https://www.khanacademy.org/math/statistics-probability http://christopherroach.com/articles/statistics-for-hackers/ Trigonometry refresher Books #BOOK Essential Mathematics and Statistics for Science (Currell 2009, WILEY) http://www.</description></item><item><title>Mathematical Optimization</title><link>https://carlos-gg.github.io/digitalgarden/AI/Math-and-Statistics/Mathematical-Optimization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Math-and-Statistics/Mathematical-Optimization/</guid><description>Resources https://en.wikipedia.org/wiki/Mathematical_optimization A birds-eye view of optimization algorithms (Pedregosa) http://people.duke.edu/~ccc14/sta-663/BlackBoxOptimization.html http://www.benfrederickson.com/numerical-optimization/ (notebook kind of post with python, d3) http://www.</description></item><item><title>MLOps</title><link>https://carlos-gg.github.io/digitalgarden/AI/Data-Science-Data-Engineering/MLops/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Data-Science-Data-Engineering/MLops/</guid><description>Resources https://en.wikipedia.org/wiki/MLOps Set of practices that aims to deploy and maintain machine learning models in production reliably and efficiently. The word is a compound of &amp;ldquo;machine learning&amp;rdquo; and the continuous development practice of DevOps in the software field https://github.</description></item><item><title>Model selection and tuning</title><link>https://carlos-gg.github.io/digitalgarden/AI/Supervised-Learning/Model-selection-and-tuning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Supervised-Learning/Model-selection-and-tuning/</guid><description>See:
AutoML Data engineering and computer science Resources Model selection and evaluation Code See MLOps</description></item><item><title>Monte Carlo methods</title><link>https://carlos-gg.github.io/digitalgarden/AI/Math-and-Statistics/Monte-Carlo-methods/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Math-and-Statistics/Monte-Carlo-methods/</guid><description>Resources https://en.wikipedia.org/wiki/Monte_Carlo_method Sequential Monte Carlo (SMC or particle filter) Particle filters or Sequential Monte Carlo (SMC) methods are a set of Monte Carlo algorithms used to solve filtering problems arising in signal processing and Bayesian statistical inference.</description></item><item><title>Multilayer perceptrons (MLPs)</title><link>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/MLPs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/MLPs/</guid><description>Resources A multilayer perceptron (MLP) is a feed forward artificial neural network model that maps sets of input data onto a set of appropriate outputs.</description></item><item><title>Multimodal learning</title><link>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/Multimodal-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/Multimodal-learning/</guid><description>Resources General-purpose neural networks capable of handling diverse inputs and output tasks Multimodal Deep Learning Code #CODE Pykale (in pytorch) References Review papers:</description></item><item><title>Natural Language Processing (NLP)</title><link>https://carlos-gg.github.io/digitalgarden/AI/NLP/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/NLP/</guid><description>Resources NLP is a branch of data science that consists of systematic processes for analyzing, understanding, and deriving information from the text data in a smart and efficient manner.</description></item><item><title>Neural Ordinary Differential Equations</title><link>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/Neural-ODEs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/Neural-ODEs/</guid><description>Resources https://github.com/Zymrael/awesome-neural-ode Understanding Neural ODE&amp;rsquo;s Neural Ordinary Differential Equations and Dynamics Models ODEs are often used to describe the time derivatives of a physical situation, referred to as the dynamics.</description></item><item><title>Normalizing flows</title><link>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/Normalizing-flows/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/Normalizing-flows/</guid><description>Resources Normalizing flow models are generative models, i.e. they infer the underlying probability distribution of an observed dataset. With that distribution we can do a number of interesting things, namely sample new realistic points and query probability densities.</description></item><item><title>Object classification, image recognition</title><link>https://carlos-gg.github.io/digitalgarden/AI/Computer-Vision/Object-classification-image-recognition/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Computer-Vision/Object-classification-image-recognition/</guid><description>See:
CNNs Object detection Semantic segmentation Residual and dense neural networks Resources https://cv-tricks.com/cnn/understand-resnet-alexnet-vgg-inception/ https://blog.</description></item><item><title>Object detection</title><link>https://carlos-gg.github.io/digitalgarden/AI/Computer-Vision/Object-detection/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Computer-Vision/Object-detection/</guid><description>See Semantic segmentation
Code https://github.com/microsoft/computervision-recipes/tree/master/scenarios/detection #CODE MMdetection OpenMMLab Detection Toolbox and Benchmark (pytorch) https://mmdetection.readthedocs.io/ #CODE TensorFlow object detection API Repository Model zoo https://medium.</description></item><item><title>One, few-shot learning</title><link>https://carlos-gg.github.io/digitalgarden/AI/One-few-shot-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/One-few-shot-learning/</guid><description>Resources https://en.wikipedia.org/wiki/One-shot_learning One-shot learning is an object categorization problem, found mostly in computer vision. Whereas most machine learning based object categorization algorithms require training on hundreds or thousands of samples/images and very large datasets, one-shot learning aims to learn information about object categories from one, or only a few, training samples/images.</description></item><item><title>Probabilistic deep learning</title><link>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/Probabilistic-deep-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/Probabilistic-deep-learning/</guid><description>See:
Bayesian modelling GFlowNets Resources A Comprehensive Introduction to Bayesian Deep Learning Bayesian Neural Network tutorial Bayesian Deep Learning - NeurIPS Workshop Deep Learning Is Not Good Enough, We Need Bayesian Deep Learning for Safe AI Making Your Neural Network Say ‚ÄúI Don‚Äôt Know‚Äù‚Ää‚Äî‚ÄäBayesian NNs using Pyro and PyTorch Building a Bayesian deep learning classifier Physics - a Gateway to Bayesian Deep Learning Bayesian deep learning with Fastai : how not to be uncertain about your uncertainty!</description></item><item><title>Probability Theory</title><link>https://carlos-gg.github.io/digitalgarden/AI/Math-and-Statistics/Probability-Theory/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Math-and-Statistics/Probability-Theory/</guid><description>Resources https://en.wikipedia.org/wiki/Probability_theory https://stanford.edu/~shervine/teaching/cme-106/key-concepts A visual introduction to probability and statistics https://leanpub.com/LittleInferenceBook/read#leanpub-auto-probability Probability forms the foundation for almost all treatments of statistical inference.</description></item><item><title>Problem Solving and Search</title><link>https://carlos-gg.github.io/digitalgarden/AI/Problem-Solving-and-Search/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Problem-Solving-and-Search/</guid><description>Resources Solving problems by searching Problem solving and search Constraint satisfaction problems: Constraint satisfaction problems (CSPs) are mathematical questions defined as a set of objects whose state must satisfy a number of constraints or limitations.</description></item><item><title>Pytorch</title><link>https://carlos-gg.github.io/digitalgarden/AI/Data-Science-Data-Engineering/Pytorch/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Data-Science-Data-Engineering/Pytorch/</guid><description>References #PAPER PyTorch: An Imperative Style, High-Performance Deep Learning Library (Paszke 2019) Deep learning frameworks have often focused on either usability or speed, but not both.</description></item><item><title>Random forest</title><link>https://carlos-gg.github.io/digitalgarden/AI/Supervised-Learning/Random-forest/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Supervised-Learning/Random-forest/</guid><description>See Ensemble learning
Resources https://github.com/kjw0612/awesome-random-forest https://sebastianraschka.com/faq/docs/bagging-boosting-rf.html Bagging and random forests are ‚Äúbagging‚Äù algorithms that aim to reduce the complexity of models that overfit the training data.</description></item><item><title>Recurrent Neural Networks (RNNs)</title><link>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/RNNs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/RNNs/</guid><description>Resources https://en.wikipedia.org/wiki/Recurrent_neural_network https://github.com/kjw0612/awesome-rnn Recurrent Neural Networks cheatsheet Tensorflow, DL and RNNs without a PhD http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/ http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html http://karpathy.github.io/2015/05/21/rnn-effectiveness/ http://www.</description></item><item><title>Regression</title><link>https://carlos-gg.github.io/digitalgarden/AI/Supervised-Learning/Regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Supervised-Learning/Regression/</guid><description>See:
Time Series analysis RNNs &amp;ldquo;Sequence time series modelling&amp;rdquo; section in CNNs Resources https://www.analytics. idhya.com/blog/2015/08/comprehensive-guide-regression/ https://towardsdatascience.</description></item><item><title>Reinforcement learning</title><link>https://carlos-gg.github.io/digitalgarden/AI/Reinforcement-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Reinforcement-learning/</guid><description>Resources Reinforcement learning is the task of learning what actions to take, given a certain situation/environment, so as to maximize a reward signal.</description></item><item><title>Residual and dense neural networks</title><link>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/Residual-and-dense-neural-networks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/Residual-and-dense-neural-networks/</guid><description>Resources https://en.wikipedia.org/wiki/Residual_neural_network Training and investigating Residual Nets References #PAPER Deep Residual Learning for Image Recognition, Resnet-50 (He 2015) ^resnet #CODE https://github.</description></item><item><title>Self-supervised learning</title><link>https://carlos-gg.github.io/digitalgarden/AI/Self-supervised-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Self-supervised-learning/</guid><description>Resources https://github.com/jason718/awesome-self-supervised-learning https://hackernoon.com/self-supervised-learning-gets-us-closer-to-autonomous-learning-be77e6c86b5a Self-Supervised Representation Learning Self-Supervised Vision Models (2021, Dr. Ishan Misra - FAIR) Self-supervised learning: The dark matter of intelligence The general technique of self-supervised learning is to predict any unobserved or hidden part (or property) of the input from any observed or unhidden part of the input Blog post explained Code #CODE VISSL FAIR&amp;rsquo;s library of extensible, modular and scalable components for SOTA Self-Supervised Learning with images #CODE Solo-learn A library of self-supervised methods for unsupervised visual representation learning powered by PyTorch Lightning Methods available: Barlow Twins, BYOL, DeepCluster V2, DINO, MoCo V2+, NNCLR, ReSSL, SimCLR + Supervised Contrastive Learning, SimSiam, Swav, VICReg, W-MSE https://arxiv.</description></item><item><title>Semantic segmentation</title><link>https://carlos-gg.github.io/digitalgarden/AI/Computer-Vision/Semantic-segmentation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Computer-Vision/Semantic-segmentation/</guid><description>See:
Encoder-decoder networks for image segmentation Object detection Resources https://en.wikipedia.org/wiki/Image_segmentation https://github.com/mrgloom/awesome-semantic-segmentation https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4 Overview of semantic image segmentation Code #CODE DeepLab2 DeepLab2 is a TensorFlow library for deep labeling, aiming to provide a unified and state-of-the-art TensorFlow codebase for dense pixel labeling tasks.</description></item><item><title>Semi-supervised learning</title><link>https://carlos-gg.github.io/digitalgarden/AI/Semi-supervised-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Semi-supervised-learning/</guid><description>Resources https://en.wikipedia.org/wiki/Semi-supervised_learning Semi-supervised learning is an approach to machine learning that combines a small amount of labeled data with a large amount of unlabeled data during training.</description></item><item><title>Super-resolution</title><link>https://carlos-gg.github.io/digitalgarden/AI/Computer-Vision/Super-resolution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Computer-Vision/Super-resolution/</guid><description>See Image-to-image translation
Resources Papers and related resources, mainly state-of-the-art and novel works in ICCV, ECCV and CVPR about image super-resolution and video super-resolution https://github.</description></item><item><title>Supervised Learning</title><link>https://carlos-gg.github.io/digitalgarden/AI/Supervised-Learning/Supervised-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Supervised-Learning/Supervised-learning/</guid><description>Resources https://en.wikipedia.org/wiki/Supervised_learning Supervised Learning cheatsheet http://scikit-learn.org/stable/supervised_learning.html Metrics: http://docs.aws.amazon.com/machine-learning/latest/dg/binary-model-insights.html ROC curves, AUC http://www.dataschool.io/roc-curves-and-auc-explained/ http://corysimon.github.io/articles/what-is-an-roc-curve/ Classification See Classification</description></item><item><title>Tensorflow, Keras</title><link>https://carlos-gg.github.io/digitalgarden/AI/Data-Science-Data-Engineering/Tensorflow-keras/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Data-Science-Data-Engineering/Tensorflow-keras/</guid><description>Code #CODE Tensorflow (Google) http://playground.tensorflow.org/ https://cloud.google.com/blog/big-data/2016/07/understanding-neural-networks-with-tensorflow-playground https://blog.metaflow.fr/tensorflow-how-to-freeze-a-model-and-serve-it-with-a-python-api-d4f3596b3adc#.dg41ldof5 https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/#0 Tensorflow 2.0: models migration and new design http://planspace.org/20170404-how_not_to_program_the_tensorflow_graph/ #CODE TF Similarity https://blog.</description></item><item><title>Time series analysis</title><link>https://carlos-gg.github.io/digitalgarden/AI/Time-Series-analysis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Time-Series-analysis/</guid><description>See:
Forecasting RNNs &amp;ldquo;Sequence (time series) modelling&amp;rdquo; section in CNNs &amp;ldquo;Deep learning for tabular data&amp;rdquo; section in Deep learning Resources https://en.</description></item><item><title>Transfer learning</title><link>https://carlos-gg.github.io/digitalgarden/AI/Transfer-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Transfer-learning/</guid><description>Resources https://en.wikipedia.org/wiki/Transfer_learning Transfer learning (TL) is a research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem https://github.</description></item><item><title>Transformers</title><link>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/Transformers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Deep-learning/Transformers/</guid><description>Resources https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/ (from RNNs with attention to Transformers) https://analyticsindiamag.com/a-complete-learning-path-to-transformers/ https://analyticsindiamag.com/transformers-for-vision-7-works-that-indicate-fusion-is-the-future-of-ai/ Code #CODE Transformers #CODE Xformers #CODE Transformers: from NLP to CV For NLP #PAPER Attention is all you need (Vaswani 2017) https://ai.</description></item><item><title>Unsupervised learning</title><link>https://carlos-gg.github.io/digitalgarden/AI/Unsupervised-learning/Unsupervised-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Unsupervised-learning/Unsupervised-learning/</guid><description>Resources https://en.wikipedia.org/wiki/Unsupervised_learning Unsupervised Learning cheatsheet Unsupervised machine learning is the machine learning task of inferring a function to describe hidden structure from &amp;ldquo;unlabeled&amp;rdquo; data (a classification or categorization is not included in the observations).</description></item><item><title>Video Frame Interpolation</title><link>https://carlos-gg.github.io/digitalgarden/AI/Computer-Vision/Video-Frame-Interpolation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Computer-Vision/Video-Frame-Interpolation/</guid><description>Resources The goal of Video Frame Interpolation is to synthesize several frames in the middle of two adjacent frames of the original video.</description></item><item><title>Video segmentation and prediction</title><link>https://carlos-gg.github.io/digitalgarden/AI/Computer-Vision/Video-segmentation-and-prediction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Computer-Vision/Video-segmentation-and-prediction/</guid><description>See:
Encoder-decoder networks &amp;ldquo;Deep learning for multi-dimensional data&amp;rdquo; section in Deep learning RNNs Resources Spatiotemporal classification and regression Hybrid convolutional and recurrent networks, 3dconv and related approaches https://github.</description></item><item><title>Visualization</title><link>https://carlos-gg.github.io/digitalgarden/AI/Data-Science-Data-Engineering/Visualization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Data-Science-Data-Engineering/Visualization/</guid><description>Resources https://github.com/fasouto/awesome-dataviz http://keshif.me/demo/VisTools The visualization universe http://visualizationuniverse.com/charts/ Dataviz project UW Interactive Data Lab Flowing Data Tutorials http://www.</description></item><item><title>Weakly supervised learning</title><link>https://carlos-gg.github.io/digitalgarden/AI/Weakly-supervised-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Weakly-supervised-learning/</guid><description>See:
Transfer learning Active learning Semi-supervised learning Resources Weak supervision is a branch of machine learning where noisy, limited, or imprecise sources are used to provide supervision signal for labeling large amounts of training data in a supervised learning setting.</description></item><item><title>XAI</title><link>https://carlos-gg.github.io/digitalgarden/AI/XAI/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/XAI/</guid><description>Resources Explainable AI (XAI), or Interpretable AI, is artificial intelligence (AI) in which the results of the solution can be understood by humans https://en.</description></item><item><title>Xarray</title><link>https://carlos-gg.github.io/digitalgarden/AI/Data-Science-Data-Engineering/Xarray/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://carlos-gg.github.io/digitalgarden/AI/Data-Science-Data-Engineering/Xarray/</guid><description>Code #CODE Xarray - N-D labeled arrays and datasets in Python #PAPER Xarray - N-D labeled Arrays and Datasets in Python (Hoyer 2017) http://xarray.</description></item></channel></rss>